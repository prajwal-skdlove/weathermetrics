{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data for a station and organize for precipitation model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths\n",
    "#Need to set up based on who is running\n",
    "usr = \"JH\"\n",
    "if usr == \"PK\":\n",
    "    ipd01 = \"\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\2024\\\\\"\n",
    "    ipd02 = \"D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\\"\n",
    "    \n",
    "if usr == \"JH\":\n",
    "    ipd01 = \"\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\2024\\\\\"\n",
    "    ipd02 = \"\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def date_to_integer(date_string, date_format='%Y-%m-%d'):\n",
    "    \"\"\"\n",
    "    Converts a date string to an integer representation.\n",
    "\n",
    "    Args:\n",
    "        date_string (str): The date string to convert.\n",
    "        date_format (str, optional): The format of the date string. Defaults to '%Y-%m-%d'.\n",
    "\n",
    "    Returns:\n",
    "        int: The integer representation of the date.\n",
    "    \"\"\"\n",
    "    date_object = datetime.datetime.strptime(date_string, date_format).date()\n",
    "    return int(date_object.strftime('%Y%m%d'))\n",
    "\n",
    "# Example usage\n",
    "# date_str = '2025-04-06'\n",
    "# date_int = date_to_integer(date_str)\n",
    "# print(date_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_make_matrix_pr04(stid_keys,nstations,ipd,nhours,ndays,nyears_row,nyears_col):\n",
    "\n",
    "    #Create a matrix for precipitation data that feeds\n",
    "    #The S4 model\n",
    "    # 1. Read in all data files for multiple stations\n",
    "    # 2. Find the common starting date that will be used to align the matrices\n",
    "    # 3. Keep only the values that fit the common starting date\n",
    "    # 4. Combine the data into one large matrix for the StationID \n",
    "\n",
    "    # 1. Read in data files and get start date v_date... for each\n",
    "    # Check that data exist for all metrics\n",
    "\n",
    "    #Create two objects of length 12\n",
    "    #data_list which contains the metrics for stations 1-4, the first is the target\n",
    "    #data_start_date which contains the most recent date for each file\n",
    "    vc_first_rcd = [0,3,6,9]\n",
    "    num_stations = len(vc_first_rcd)\n",
    "    vc_st_id = stid_keys.iloc[:,0]\n",
    "    vc_path = stid_keys.iloc[:,3]\n",
    "    vc_fnames = stid_keys.iloc[:,2]\n",
    "\n",
    "    data_list = []\n",
    "    data_start_date = []\n",
    "    \n",
    "    for i in range(0, num_stations):\n",
    "            #Air temperature (one variable recorded once an hour a day)\n",
    "            infile = vc_path.iloc[vc_first_rcd[i]] + \"\\\\\" + vc_fnames.iloc[vc_first_rcd[i]]\n",
    "            df_at = pd.read_csv(infile)\n",
    "            data_list.append(df_at)\n",
    "            dt_first = datetime.strptime(df_at.iloc[0,2], \"%Y-%m-%d\").date()\n",
    "            data_start_date.append(dt_first)\n",
    "            #print(dt_first)\n",
    "            \n",
    "            #Precipitation (one variable recorded once an hour a day)\n",
    "            infile = vc_path.iloc[vc_first_rcd[i]] + \"\\\\\" + vc_fnames.iloc[vc_first_rcd[i]+1] \n",
    "            df_pr = pd.read_csv(infile)\n",
    "            data_list.append(df_pr)\n",
    "            dt_first = datetime.strptime(df_pr.iloc[0,2], \"%Y-%m-%d\").date()\n",
    "            data_start_date.append(dt_first)\n",
    "            #print(dt_first)\n",
    "            \n",
    "            #Relative Humidity (3 variables recorded once a day)\n",
    "            infile = vc_path.iloc[vc_first_rcd[i]] + \"\\\\\" + vc_fnames.iloc[vc_first_rcd[i]+2]\n",
    "            df_rh = pd.read_csv(infile)\n",
    "            data_list.append(df_rh)\n",
    "            dt_first = datetime.strptime(df_rh.iloc[0,4], \"%Y-%m-%d\").date()\n",
    "            data_start_date.append(dt_first)\n",
    "            #print(dt_first)\n",
    " \n",
    "     # 2. Find the offsets needed to all datasets start at same starting date\n",
    "    v_date_minimum = min(data_start_date)\n",
    "    print(v_date_minimum)\n",
    "\n",
    "    #Identify offsets to use so all datadata vectors are appropriately aligned \n",
    "    upper_range = len(data_start_date)\n",
    "    \n",
    "    date_offsets_list = []\n",
    "    for i in range(0, upper_range):\n",
    "        date_offset = data_start_date[i] - v_date_minimum\n",
    "        #Temporarily set to 0, revise later\n",
    "        #date_offset = 0\n",
    "        date_offsets_list.append(date_offset)\n",
    "    \n",
    "    # 3. Get vectors of data for each measure\n",
    "    #These vectors all start on the same date\n",
    "\n",
    "    data_vectors_list = []\n",
    "    for i in vc_first_rcd:\n",
    "        tdt = data_list[i]\n",
    "        tof = date_offsets_list[i]\n",
    "        \n",
    "        i_start = tof.days\n",
    "        i_end = len(tdt.iloc[:,1]) - i_start\n",
    "        vc_at1 = tdt.iloc[i_start:i_end,1]\n",
    "        data_vectors_list.append(vc_at1)\n",
    "\n",
    "        tdt = data_list[i+1]\n",
    "        tof = date_offsets_list[i+1]\n",
    "        i_start = tof.days\n",
    "        i_end = len(tdt.iloc[:,1]) - i_start\n",
    "        vc_pr1 = tdt.iloc[i_start:i_end,1]\n",
    "        data_vectors_list.append(vc_pr1)\n",
    "            \n",
    "        tdt = data_list[i+2]\n",
    "        tof = date_offsets_list[i+2]\n",
    "        i_start = tof.days\n",
    "        i_end = len(tdt.iloc[:,1]) - i_start        \n",
    "        vc_rh1 = tdt.iloc[i_start:i_end,1]\n",
    "        data_vectors_list.append(vc_rh1)\n",
    "\n",
    "        tdt = data_list[i+2]\n",
    "        tof = date_offsets_list[i+2]\n",
    "        i_start = tof.days\n",
    "        i_end = len(tdt.iloc[:,1]) - i_start        \n",
    "        vc_rh2 = tdt.iloc[i_start:i_end,2]\n",
    "        data_vectors_list.append(vc_rh2)\n",
    "\n",
    "        tdt = data_list[i+2]\n",
    "        tof = date_offsets_list[i+2]\n",
    "        i_start = tof.days\n",
    "        i_end = len(tdt.iloc[:,1]) - i_start        \n",
    "        vc_rh3 = tdt.iloc[i_start:i_end,3]\n",
    "        data_vectors_list.append(vc_rh3)\n",
    "    \n",
    "    # 4. Interleave vectors so we get a complete vector of all metrics\n",
    "    #Start with the once a day vectors and loop over the number of days as defined by once a day vector\n",
    "    #These set how many years of data are needed : nyears_row + nyears_col\n",
    "    nyears_data_limit = nyears_row + nyears_col + 1\n",
    "    nrows = nhours*ndays*nyears_row\n",
    "    ncols = nhours*ndays*nyears_col \n",
    "        \n",
    "    # set the number of metrics created each day\n",
    "    # Metrics measured at hourly intervals are put into day metrics\n",
    "    # And ordered from most recent hour to most distant hour from left to right\n",
    "    # Day metrics are also ordered most recent day to most distant day left to right\n",
    "    ndaily_metrics = 2*nhours + 3*1\n",
    "\n",
    "    #Set the total number of days of data needed for the model\n",
    "    num_days = nyears_data_limit*ndays\n",
    "    #print(f\"Number of days: {num_days}\")\n",
    "\n",
    "    is_first = 1\n",
    "\n",
    "    #Construct the vector that holds all data values (valid and missing)\n",
    "    #Assumes that all vectors in data_vectors_list match on date start\n",
    "    #data are organized by day, target station, other stations\n",
    "    for i_day in range(num_days):\n",
    "        i_day_hour_end = (i_day + 1)*nhours - 1\n",
    "        i_day_hour_start = i_day_hour_end - (nhours -1)\n",
    "        \n",
    "        #Target Station\n",
    "        va2 = data_vectors_list[0][(i_day_hour_start):(i_day_hour_end)]  #AT\n",
    "        va1 = data_vectors_list[1][(i_day_hour_start):(i_day_hour_end)]  #PR\n",
    "        va3 = data_vectors_list[2][i_day:i_day]                          #RH1\n",
    "        va4 = data_vectors_list[3][i_day:i_day]                          #RH2\n",
    "        va5 = data_vectors_list[4][i_day:i_day]                          #RH3\n",
    "        #Station 1\n",
    "        vb2 = data_vectors_list[5][(i_day_hour_start):(i_day_hour_end)]  #AT\n",
    "        vb1 = data_vectors_list[6][(i_day_hour_start):(i_day_hour_end)]  #PR\n",
    "        vb3 = data_vectors_list[7][i_day:i_day]                          #RH1\n",
    "        vb4 = data_vectors_list[8][i_day:i_day]                          #RH2\n",
    "        vb5 = data_vectors_list[9][i_day:i_day]                          #RH3\n",
    "        #Station 2\n",
    "        vc2 = data_vectors_list[10][(i_day_hour_start):(i_day_hour_end)]  #AT\n",
    "        vc1 = data_vectors_list[11][(i_day_hour_start):(i_day_hour_end)]  #PR\n",
    "        vc3 = data_vectors_list[12][i_day:i_day]                          #RH1\n",
    "        vc4 = data_vectors_list[13][i_day:i_day]                          #RH2\n",
    "        vc5 = data_vectors_list[14][i_day:i_day]                          #RH3\n",
    "        #Station 3\n",
    "        vd2 = data_vectors_list[15][(i_day_hour_start):(i_day_hour_end)]  #AT\n",
    "        vd1 = data_vectors_list[16][(i_day_hour_start):(i_day_hour_end)]  #PR\n",
    "        vd3 = data_vectors_list[17][i_day:i_day]                          #RH1\n",
    "        vd4 = data_vectors_list[18][i_day:i_day]                          #RH2\n",
    "        vd5 = data_vectors_list[19][i_day:i_day]                          #RH3\n",
    "            \n",
    "\n",
    "        #vmt_full set = np.concatenate((vmt_one_day,va,vb,vc,vd,ve)) \n",
    "        #print(i_day)\n",
    "        if is_first == 0:\n",
    "            vmt_full_set = np.concatenate((vmt_full_set,va1,va2,va3,va4,va5,vb1,vb2,vb3,vb4,vb5,vc1,vc2,vc3,vc4,vc5,vd1,vd2,vd3,vd4,vd5))\n",
    "            #print(len(vmt_full_set))\n",
    "        else:\n",
    "            is_first = 0\n",
    "            vmt_full_set = np.concatenate((va1,va2,va3,va4,va5,vb1,vb2,vb3,vb4,vb5,vc1,vc2,vc3,vc4,vc5,vd1,vd2,vd3,vd4,vd5))\n",
    "            #print(len(vmt_full_set))\n",
    "\n",
    "    print(f\"Length of data vector: {len(vmt_full_set)}\")\n",
    "\n",
    "\n",
    "    #Organize the data vector into an array which will be returned to the function call\n",
    "    #Create an empty matrix in which data are organized\n",
    "    #nstations = 4\n",
    "    nrows = nyears_row*ndays\n",
    "    ncols = nstations*nyears_col*ndays*ndaily_metrics\n",
    "    nrow_days =  nyears_row*ndays\n",
    "    ncol_days =  nyears_col*ndays\n",
    "    data_limit = len(vmt_full_set)\n",
    "    #print(f\"Number of columns in matrix: {ncols}\")\n",
    "    #print(f\"Limit of data: {data_limit}\")\n",
    "\n",
    "    #Make template matrix to house data\n",
    "    template_matrix = np.zeros((nrows, ncols), dtype=np.float16)\n",
    "\n",
    "    # Fill the matrix\n",
    "    for i in range(nrow_days):\n",
    "        vc_start = i*(ndaily_metrics)\n",
    "        vc_end = ncols + vc_start\n",
    "            \n",
    "        if vc_end < data_limit:\n",
    "            template_matrix[i] = vmt_full_set[(vc_start):(vc_end)]\n",
    "\n",
    "            \n",
    "    return template_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_make_matrix_pr05(stid_keys,nstations,ipd,nhours,ndays,nyears_row,nyears_col,n_metrics):\n",
    "    #Create a matrix for precipitation data that feeds\n",
    "    #The S4 model\n",
    "    # 1. Read in all data files for multiple stations\n",
    "    # 2. Find the common starting date that will be used to align the matrices\n",
    "    # 3. Keep only the values that fit the common starting date\n",
    "    # 4. Combine the data into one large matrix for the StationID \n",
    "\n",
    "    # 1. Read in data files and get start date v_date... for each\n",
    "    # Check that data exist for all metrics\n",
    "\n",
    "    #Create two objects of length 12\n",
    "    #data_list which contains the metrics for stations 1-4, the first is the target\n",
    "    #data_start_date which contains the most recent date for each file\n",
    "    vc_first_rcd = [0,3,6,9]\n",
    "    num_stations = len(vc_first_rcd)\n",
    "    vc_st_id = stid_keys.iloc[:,0]\n",
    "    vc_path = stid_keys.iloc[:,3]\n",
    "    vc_fnames = stid_keys.iloc[:,2]\n",
    "\n",
    "    data_list = []\n",
    "    data_start_date = []\n",
    "    \n",
    "    for i in range(0, num_stations):\n",
    "            #Air temperature (one variable recorded once an hour a day)\n",
    "            infile = vc_path.iloc[vc_first_rcd[i]] + \"\\\\\" + vc_fnames.iloc[vc_first_rcd[i]]\n",
    "            df_at = pd.read_csv(infile)\n",
    "            data_list.append(df_at)\n",
    "            dt_first = datetime.strptime(df_at.iloc[0,2], \"%Y-%m-%d\").date()\n",
    "            data_start_date.append(dt_first)\n",
    "            #print(dt_first)\n",
    "            \n",
    "            #Precipitation (one variable recorded once an hour a day)\n",
    "            infile = vc_path.iloc[vc_first_rcd[i]] + \"\\\\\" + vc_fnames.iloc[vc_first_rcd[i]+1] \n",
    "            df_pr = pd.read_csv(infile)\n",
    "            data_list.append(df_pr)\n",
    "            dt_first = datetime.strptime(df_pr.iloc[0,2], \"%Y-%m-%d\").date()\n",
    "            data_start_date.append(dt_first)\n",
    "            #print(dt_first)\n",
    "            \n",
    "            #Relative Humidity (3 variables recorded once a day)\n",
    "            infile = vc_path.iloc[vc_first_rcd[i]] + \"\\\\\" + vc_fnames.iloc[vc_first_rcd[i]+2]\n",
    "            df_rh = pd.read_csv(infile)\n",
    "            data_list.append(df_rh)\n",
    "            dt_first = datetime.strptime(df_rh.iloc[0,4], \"%Y-%m-%d\").date()\n",
    "            data_start_date.append(dt_first)\n",
    "            #print(dt_first)\n",
    " \n",
    "     # 2. Find the offsets needed to all datasets start at same starting date\n",
    "    v_date_minimum = min(data_start_date)\n",
    "    #print(v_date_minimum)\n",
    "\n",
    "    #Identify offsets to use so all datadata vectors are appropriately aligned \n",
    "    upper_range = len(data_start_date)\n",
    "    \n",
    "    date_offsets_list = []\n",
    "    for i in range(0, upper_range):\n",
    "        date_offset = data_start_date[i] - v_date_minimum\n",
    "        #Temporarily set to 0, revise later\n",
    "        #date_offset = 0\n",
    "        date_offsets_list.append(date_offset)\n",
    "    \n",
    "    # 3. Get vectors of data for each measure\n",
    "    #These vectors all start on the same date\n",
    "\n",
    "    data_vectors_list = []\n",
    "    for i in vc_first_rcd:\n",
    "        tdt = data_list[i]\n",
    "        tof = date_offsets_list[i]\n",
    "        \n",
    "        i_start = tof.days\n",
    "        i_end = len(tdt.iloc[:,1]) - i_start\n",
    "        vc_at1 = tdt.iloc[i_start:i_end,1]\n",
    "        data_vectors_list.append(vc_at1)\n",
    "\n",
    "        tdt = data_list[i+1]\n",
    "        tof = date_offsets_list[i+1]\n",
    "        i_start = tof.days\n",
    "        i_end = len(tdt.iloc[:,1]) - i_start\n",
    "        vc_pr1 = tdt.iloc[i_start:i_end,1]\n",
    "        data_vectors_list.append(vc_pr1)\n",
    "            \n",
    "        tdt = data_list[i+2]\n",
    "        tof = date_offsets_list[i+2]\n",
    "        i_start = tof.days\n",
    "        i_end = len(tdt.iloc[:,1]) - i_start        \n",
    "        vc_rh1 = tdt.iloc[i_start:i_end,1]\n",
    "        #adjust from 1 a day to 24 a day measures\n",
    "        vc_rh1_24 = np.repeat(vc_rh1, nhours)\n",
    "        data_vectors_list.append(vc_rh1_24)\n",
    "    \n",
    "        tdt = data_list[i+2]\n",
    "        tof = date_offsets_list[i+2]\n",
    "        i_start = tof.days\n",
    "        i_end = len(tdt.iloc[:,1]) - i_start        \n",
    "        vc_rh2 = tdt.iloc[i_start:i_end,2]\n",
    "        #adjust from 1 a day to 24 a day measures\n",
    "        vc_rh2_24 = np.repeat(vc_rh2, nhours)\n",
    "        data_vectors_list.append(vc_rh2_24)\n",
    "\n",
    "        tdt = data_list[i+2]\n",
    "        tof = date_offsets_list[i+2]\n",
    "        i_start = tof.days\n",
    "        i_end = len(tdt.iloc[:,1]) - i_start        \n",
    "        vc_rh3 = tdt.iloc[i_start:i_end,3]\n",
    "        #adjust from 1 a day to 24 a day measures\n",
    "        vc_rh3_24 = np.repeat(vc_rh3, nhours)\n",
    "        data_vectors_list.append(vc_rh3_24)\n",
    "\n",
    "    for i in range(0,20):\n",
    "        print(f\"Length vector: {len(data_vectors_list[i])}\")\n",
    "        \n",
    "    # 4. Interleave vectors so we get a complete vector of all metrics\n",
    "    #Start with the once a day vectors and loop over the number of days as defined by once a day vector\n",
    "    #These set how many years of data are needed : nyears_row + nyears_col\n",
    "    nyears_data_limit = nyears_row + nyears_col + 1\n",
    "    nrows = nhours*ndays*nyears_row\n",
    "    ncols = nhours*ndays*nyears_col \n",
    "        \n",
    "    # Set the number of metrics created each day\n",
    "    # And ordered from most recent hour to most distant hour from left to right\n",
    "    # Day metrics are also ordered most recent day to most distant day left to right\n",
    "    ndaily_metrics = n_metrics*nhours\n",
    "\n",
    "    #Set the total number of days of data needed for the model\n",
    "    num_days = nyears_data_limit*ndays\n",
    "    num_days_hrs = nhours*num_days\n",
    "    #num_days_hrs = nhours*100\n",
    "\n",
    "    print(f\"Number of years total needed: {nyears_data_limit}\")\n",
    "    print(f\"Number of days total needed: {num_days}\")\n",
    "    print(f\"Number of days and hours: {num_days_hrs}\")\n",
    "    print(f\"Number of matrix columns per metric: {ncols}\")\n",
    "    print(f\"Number of matrix rows: {nrows}\")\n",
    "    #print(f\"tgt : {data_vectors_list[1][0:23]}\")\n",
    "    is_first = 1\n",
    "\n",
    "    #Construct the vector that holds all data values (valid and missing)\n",
    "    #Assumes that all vectors in data_vectors_list match on date start\n",
    "    #data are organized by day, target station, other stations\n",
    "    #for i_day in range(num_days_hrs):\n",
    "    for i_day in range(nrows):\n",
    "        #print(i_day)\n",
    "        i_day_hour_end_tgt = i_day + ncols\n",
    "        i_day_hour_start_tgt = i_day\n",
    "        \n",
    "        i_day_hour_end_pred = i_day_hour_end_tgt + 1\n",
    "        i_day_hour_start_pred = i_day_hour_start_tgt + 1\n",
    "        \n",
    "        #print(f\"start :{i_day_hour_start_tgt}\")\n",
    "        #print(f\"end :{i_day_hour_end_tgt}\")\n",
    "        \n",
    "        #Target Station\n",
    "        if nstations > 0:\n",
    "            va2 = data_vectors_list[0][(i_day_hour_start_pred):(i_day_hour_end_pred)]  #AT\n",
    "            va1 = data_vectors_list[1][(i_day_hour_start_tgt):(i_day_hour_end_tgt)]  #PR\n",
    "            va3 = data_vectors_list[2][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH1\n",
    "            va4 = data_vectors_list[3][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH2\n",
    "            va5 = data_vectors_list[4][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH3\n",
    "        #Station 1\n",
    "        if nstations > 1:\n",
    "            vb2 = data_vectors_list[5][(i_day_hour_start_pred):(i_day_hour_end_pred)]  #AT\n",
    "            vb1 = data_vectors_list[6][(i_day_hour_start_pred):(i_day_hour_end_pred)]  #PR\n",
    "            vb3 = data_vectors_list[7][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH1\n",
    "            vb4 = data_vectors_list[8][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH2\n",
    "            vb5 = data_vectors_list[9][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH3        \n",
    "        #Station 2\n",
    "        if nstations > 2:\n",
    "            vc2 = data_vectors_list[10][(i_day_hour_start_pred):(i_day_hour_end_pred)]  #AT\n",
    "            vc1 = data_vectors_list[11][(i_day_hour_start_pred):(i_day_hour_end_pred)]  #PR\n",
    "            vc3 = data_vectors_list[12][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH1\n",
    "            vc4 = data_vectors_list[13][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH2\n",
    "            vc5 = data_vectors_list[14][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH3        \n",
    "        #Station 3\n",
    "        if nstations > 3:\n",
    "            vd2 = data_vectors_list[15][(i_day_hour_start_pred):(i_day_hour_end_pred)]  #AT\n",
    "            vd1 = data_vectors_list[16][(i_day_hour_start_pred):(i_day_hour_end_pred)]  #PR\n",
    "            vd3 = data_vectors_list[17][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH1\n",
    "            vd4 = data_vectors_list[18][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH2\n",
    "            vd5 = data_vectors_list[19][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH3        \n",
    "\n",
    "        if is_first == 0:\n",
    "            if nstations == 1:\n",
    "                vc_row = np.concatenate((va1,va2,va3,va4,va5))\n",
    "            if nstations == 2:\n",
    "                vc_row = np.concatenate((va1,va2,va3,va4,va5,vb1,vb2,vb3,vb4,vb5))\n",
    "            if nstations == 3:\n",
    "                vc_row = np.concatenate((va1,va2,va3,va4,va5,vb1,vb2,vb3,vb4,vb5,vc1,vc2,vc3,vc4,vc5))\n",
    "            if nstations == 4:\n",
    "                vc_row = np.concatenate((va1,va2,va3,va4,va5,vb1,vb2,vb3,vb4,vb5,vc1,vc2,vc3,vc4,vc5,vd1,vd2,vd3,vd4,vd5))\n",
    "\n",
    "            vmt_full_set = np.vstack((vmt_full_set, vc_row))\n",
    "            print(vmt_full_set.shape)\n",
    "        else:\n",
    "            is_first = 0\n",
    "            \n",
    "            if nstations == 1:\n",
    "                vmt_full_set = np.concatenate((va1,va2,va3,va4,va5))\n",
    "            if nstations == 2:\n",
    "                vmt_full_set = np.concatenate((va1,va2,va3,va4,va5,vb1,vb2,vb3,vb4,vb5))\n",
    "            if nstations == 3:\n",
    "                vmt_full_set = np.concatenate((va1,va2,va3,va4,va5,vb1,vb2,vb3,vb4,vb5,vc1,vc2,vc3,vc4,vc5))\n",
    "            if nstations == 4:\n",
    "                vmt_full_set = np.concatenate((va1,va2,va3,va4,va5,vb1,vb2,vb3,vb4,vb5,vc1,vc2,vc3,vc4,vc5,vd1,vd2,vd3,vd4,vd5))\n",
    "\n",
    "    return vmt_full_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_make_matrix_pr06(stid_keys,nstations,ipd,nhours,ndays,nyears_row,nyears_col,n_metrics):\n",
    "    #Create a matrix for precipitation data that feeds\n",
    "    #The S4 model\n",
    "    # 1. Read in all data files for multiple stations\n",
    "    # 2. Find the common starting date that will be used to align the matrices\n",
    "    # 3. Keep only the values that fit the common starting date\n",
    "    # 4. Combine the data into one large matrix for the StationID \n",
    "\n",
    "    # 1. Read in data files and get start date v_date... for each\n",
    "    # Check that data exist for all metrics\n",
    "\n",
    "    #Create two objects of length 12\n",
    "    #data_list which contains the metrics for stations 1-4, the first is the target\n",
    "    #data_start_date which contains the most recent date for each file\n",
    "    vc_first_rcd = [0,3,6,9]\n",
    "    num_stations = len(vc_first_rcd)\n",
    "    vc_st_id = stid_keys.iloc[:,0]\n",
    "    vc_path = stid_keys.iloc[:,3]\n",
    "    vc_fnames = stid_keys.iloc[:,2]\n",
    "\n",
    "    data_list = []\n",
    "    data_start_date = []\n",
    "    \n",
    "    for i in range(0, num_stations):\n",
    "            #Air temperature (one variable recorded once an hour a day)\n",
    "            infile = vc_path.iloc[vc_first_rcd[i]] + \"\\\\\" + vc_fnames.iloc[vc_first_rcd[i]]\n",
    "            df_at = pd.read_csv(infile)\n",
    "            data_list.append(df_at)\n",
    "            dt_first = datetime.strptime(df_at.iloc[0,2], \"%Y-%m-%d\").date()\n",
    "            data_start_date.append(dt_first)\n",
    "            #print(dt_first)\n",
    "            \n",
    "            #Precipitation (one variable recorded once an hour a day)\n",
    "            infile = vc_path.iloc[vc_first_rcd[i]] + \"\\\\\" + vc_fnames.iloc[vc_first_rcd[i]+1] \n",
    "            df_pr = pd.read_csv(infile)\n",
    "            data_list.append(df_pr)\n",
    "            dt_first = datetime.strptime(df_pr.iloc[0,2], \"%Y-%m-%d\").date()\n",
    "            data_start_date.append(dt_first)\n",
    "            #print(dt_first)\n",
    "            \n",
    "            #Relative Humidity (3 variables recorded once a day)\n",
    "            infile = vc_path.iloc[vc_first_rcd[i]] + \"\\\\\" + vc_fnames.iloc[vc_first_rcd[i]+2]\n",
    "            df_rh = pd.read_csv(infile)\n",
    "            data_list.append(df_rh)\n",
    "            dt_first = datetime.strptime(df_rh.iloc[0,4], \"%Y-%m-%d\").date()\n",
    "            data_start_date.append(dt_first)\n",
    "            #print(dt_first)\n",
    " \n",
    "     # 2. Find the offsets needed to all datasets start at same starting date\n",
    "    v_date_minimum = min(data_start_date)\n",
    "    #print(v_date_minimum)\n",
    "\n",
    "    #Identify offsets to use so all datadata vectors are appropriately aligned \n",
    "    upper_range = len(data_start_date)\n",
    "    \n",
    "    date_offsets_list = []\n",
    "    for i in range(0, upper_range):\n",
    "        date_offset = data_start_date[i] - v_date_minimum\n",
    "        #Temporarily set to 0, revise later\n",
    "        #date_offset = 0\n",
    "        date_offsets_list.append(date_offset)\n",
    "    \n",
    "    # 3. Get vectors of data for each measure\n",
    "    #These vectors all start on the same date\n",
    "\n",
    "    data_vectors_list = []\n",
    "    for i in vc_first_rcd:\n",
    "        tdt = data_list[i]\n",
    "        tof = date_offsets_list[i]\n",
    "        \n",
    "        i_start = tof.days\n",
    "        i_end = len(tdt.iloc[:,1]) - i_start\n",
    "        vc_at1 = tdt.iloc[i_start:i_end,1]\n",
    "        data_vectors_list.append(vc_at1)\n",
    "\n",
    "        tdt = data_list[i+1]\n",
    "        tof = date_offsets_list[i+1]\n",
    "        i_start = tof.days\n",
    "        i_end = len(tdt.iloc[:,1]) - i_start\n",
    "        vc_pr1 = tdt.iloc[i_start:i_end,1]\n",
    "        data_vectors_list.append(vc_pr1)\n",
    "            \n",
    "        tdt = data_list[i+2]\n",
    "        tof = date_offsets_list[i+2]\n",
    "        i_start = tof.days\n",
    "        i_end = len(tdt.iloc[:,1]) - i_start        \n",
    "        vc_rh1 = tdt.iloc[i_start:i_end,1]\n",
    "        #adjust from 1 a day to 24 a day measures\n",
    "        vc_rh1_24 = np.repeat(vc_rh1, nhours)\n",
    "        data_vectors_list.append(vc_rh1_24)\n",
    "    \n",
    "        tdt = data_list[i+2]\n",
    "        tof = date_offsets_list[i+2]\n",
    "        i_start = tof.days\n",
    "        i_end = len(tdt.iloc[:,1]) - i_start        \n",
    "        vc_rh2 = tdt.iloc[i_start:i_end,2]\n",
    "        #adjust from 1 a day to 24 a day measures\n",
    "        vc_rh2_24 = np.repeat(vc_rh2, nhours)\n",
    "        data_vectors_list.append(vc_rh2_24)\n",
    "\n",
    "        tdt = data_list[i+2]\n",
    "        tof = date_offsets_list[i+2]\n",
    "        i_start = tof.days\n",
    "        i_end = len(tdt.iloc[:,1]) - i_start        \n",
    "        vc_rh3 = tdt.iloc[i_start:i_end,3]\n",
    "        #adjust from 1 a day to 24 a day measures\n",
    "        vc_rh3_24 = np.repeat(vc_rh3, nhours)\n",
    "        data_vectors_list.append(vc_rh3_24)\n",
    "\n",
    "    for i in range(0,20):\n",
    "        print(f\"Length vector: {len(data_vectors_list[i])}\")\n",
    "        \n",
    "    # 4. Interleave vectors so we get a complete vector of all metrics\n",
    "    #Start with the once a day vectors and loop over the number of days as defined by once a day vector\n",
    "    #These set how many years of data are needed : nyears_row + nyears_col\n",
    "    nyears_data_limit = nyears_row + nyears_col + 1\n",
    "    nrows = nhours*ndays*nyears_row\n",
    "    ncols = nhours*ndays*nyears_col \n",
    "        \n",
    "    # Set the number of metrics created each day\n",
    "    # And ordered from most recent hour to most distant hour from left to right\n",
    "    # Day metrics are also ordered most recent day to most distant day left to right\n",
    "    ndaily_metrics = n_metrics*nhours\n",
    "\n",
    "    #Set the total number of days of data needed for the model\n",
    "    num_days = nyears_data_limit*ndays\n",
    "    num_days_hrs = nhours*num_days\n",
    "    #num_days_hrs = nhours*100\n",
    "\n",
    "    print(f\"Number of years total needed: {nyears_data_limit}\")\n",
    "    print(f\"Number of days total needed: {num_days}\")\n",
    "    print(f\"Number of days and hours: {num_days_hrs}\")\n",
    "    print(f\"Number of matrix columns per metric: {ncols}\")\n",
    "    print(f\"Number of total matrix columns: {ncols*n_metrics}\")\n",
    "    print(f\"Number of matrix rows: {nrows}\")\n",
    "    #print(f\"tgt : {data_vectors_list[1][0:23]}\")\n",
    "    vmt_full_set = np.empty((nrows,ncols*n_metrics*nstations),np.float16)\n",
    "    is_first = 1\n",
    "\n",
    "    #Construct the vector that holds all data values (valid and missing)\n",
    "    #Assumes that all vectors in data_vectors_list match on date start\n",
    "    #data are organized by day, target station, other stations\n",
    "    #for i_day in range(num_days_hrs):\n",
    "    for i_day in range(nrows):\n",
    "        #print(i_day)\n",
    "        i_day_hour_end_tgt = i_day + ncols\n",
    "        i_day_hour_start_tgt = i_day\n",
    "        \n",
    "        i_day_hour_end_pred = i_day_hour_end_tgt + 1\n",
    "        i_day_hour_start_pred = i_day_hour_start_tgt + 1\n",
    "        \n",
    "        #print(f\"start :{i_day_hour_start_tgt}\")\n",
    "        #print(f\"end :{i_day_hour_end_tgt}\")\n",
    "        \n",
    "        #Target Station\n",
    "        if nstations > 0:\n",
    "            va2 = data_vectors_list[0][(i_day_hour_start_pred):(i_day_hour_end_pred)]  #AT\n",
    "            va1 = data_vectors_list[1][(i_day_hour_start_tgt):(i_day_hour_end_tgt)]  #PR\n",
    "            va3 = data_vectors_list[2][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH1\n",
    "            va4 = data_vectors_list[3][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH2\n",
    "            va5 = data_vectors_list[4][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH3\n",
    "        #Station 1\n",
    "        if nstations > 1:\n",
    "            vb2 = data_vectors_list[5][(i_day_hour_start_pred):(i_day_hour_end_pred)]  #AT\n",
    "            vb1 = data_vectors_list[6][(i_day_hour_start_pred):(i_day_hour_end_pred)]  #PR\n",
    "            vb3 = data_vectors_list[7][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH1\n",
    "            vb4 = data_vectors_list[8][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH2\n",
    "            vb5 = data_vectors_list[9][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH3        \n",
    "        #Station 2\n",
    "        if nstations > 2:\n",
    "            vc2 = data_vectors_list[10][(i_day_hour_start_pred):(i_day_hour_end_pred)]  #AT\n",
    "            vc1 = data_vectors_list[11][(i_day_hour_start_pred):(i_day_hour_end_pred)]  #PR\n",
    "            vc3 = data_vectors_list[12][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH1\n",
    "            vc4 = data_vectors_list[13][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH2\n",
    "            vc5 = data_vectors_list[14][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH3        \n",
    "        #Station 3\n",
    "        if nstations > 3:\n",
    "            vd2 = data_vectors_list[15][(i_day_hour_start_pred):(i_day_hour_end_pred)]  #AT\n",
    "            vd1 = data_vectors_list[16][(i_day_hour_start_pred):(i_day_hour_end_pred)]  #PR\n",
    "            vd3 = data_vectors_list[17][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH1\n",
    "            vd4 = data_vectors_list[18][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH2\n",
    "            vd5 = data_vectors_list[19][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH3        \n",
    "\n",
    "        if is_first == 0:\n",
    "            if nstations == 1:\n",
    "                vc_row = np.concatenate((va1,va2,va3,va4,va5))\n",
    "            if nstations == 2:\n",
    "                vc_row = np.concatenate((va1,va2,va3,va4,va5,vb1,vb2,vb3,vb4,vb5))\n",
    "            if nstations == 3:\n",
    "                vc_row = np.concatenate((va1,va2,va3,va4,va5,vb1,vb2,vb3,vb4,vb5,vc1,vc2,vc3,vc4,vc5))\n",
    "            if nstations == 4:\n",
    "                vc_row = np.concatenate((va1,va2,va3,va4,va5,vb1,vb2,vb3,vb4,vb5,vc1,vc2,vc3,vc4,vc5,vd1,vd2,vd3,vd4,vd5))\n",
    "\n",
    "            vmt_full_set[i_day] = vc_row\n",
    "            #vmt_full_set = np.vstack((vmt_full_set, vc_row))\n",
    "            print(vmt_full_set.shape)\n",
    "        else:\n",
    "            is_first = 0\n",
    "            \n",
    "            if nstations == 1:\n",
    "                vmt_full_set[i_day] = np.concatenate((va1,va2,va3,va4,va5))\n",
    "            if nstations == 2:\n",
    "                vmt_full_set[i_day] = np.concatenate((va1,va2,va3,va4,va5,vb1,vb2,vb3,vb4,vb5))\n",
    "            if nstations == 3:\n",
    "                vmt_full_set[i_day] = np.concatenate((va1,va2,va3,va4,va5,vb1,vb2,vb3,vb4,vb5,vc1,vc2,vc3,vc4,vc5))\n",
    "            if nstations == 4:\n",
    "                vmt_full_set[i_day] = np.concatenate((va1,va2,va3,va4,va5,vb1,vb2,vb3,vb4,vb5,vc1,vc2,vc3,vc4,vc5,vd1,vd2,vd3,vd4,vd5))\n",
    "\n",
    "    return vmt_full_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temp to test fn06\n",
    "#Parameters governing data matrix\n",
    "stid_keys = dt_analysis_set_keys\n",
    "nstations = 3\n",
    "ipd = ipd02\n",
    "nhours = 24\n",
    "ndays = 365\n",
    "nyears_row = 3 \n",
    "nyears_col = 2\n",
    "n_metrics = 5\n",
    "\n",
    "#rtn_matrix = fn_make_matrix_pr06(stid_keys,nstations,ipd,nhours,ndays,nyears_row,nyears_col,n_metrics)\n",
    "#print(rtn_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = {\n",
    "#     'st_id': [72508014740,72508014740,72508014740, 72504094702,72504094702,72504094702,72510094746,72510094746,72510094746,72505004781,72505004781,72505004781],\n",
    "#     'target_station': [72508014740,72508014740,72508014740,72508014740,72508014740,72508014740,72508014740,72508014740,72508014740,72508014740,72508014740,72508014740],\n",
    "#     'fnames': ['72508014740_model_data_airtemp.csv', '72508014740_model_data_precip.csv', '72508014740_model_data_relhum.csv',\n",
    "#         '72504094702_model_data_airtemp.csv', '72504094702_model_data_precip.csv', '72504094702_model_data_relhum.csv',\n",
    "#         '72510094746_model_data_airtemp.csv', '72510094746_model_data_precip.csv', '72510094746_model_data_relhum.csv',\n",
    "#         '72505004781_model_data_airtemp.csv', '72505004781_model_data_precip.csv', '72505004781_model_data_relhum.csv'],\n",
    "#     'path': ['D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\'],\n",
    "#     'vect_close_bkt2': [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4]\n",
    "# }\n",
    "\n",
    "# dt_analysis_set_keys = pd.DataFrame(data)\n",
    "# dt_analysis_set_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 1.  Select a station using ID to get its related stations\n",
    "# Get file of target stations and their related stations\n",
    "dt_trg_set = ipd02 + \"Target_station_analysis_set.csv\"\n",
    "lst_target_stations = [\n",
    "  \"74486094789\",\n",
    "  \"72518014735\",\n",
    "  \"72508014740\",\n",
    "  \"72526014860\",\n",
    "  \"72406093721\",\n",
    "  \"72401013740\",\n",
    "  \"72219013874\",\n",
    "  \"72423093821\",\n",
    "  \"72428014821\",\n",
    "  \"72327013897\"\n",
    "]\n",
    "\n",
    "#Select the target station for which predictions will be made\n",
    "#72508014740 is JFK airport\n",
    "st_id = lst_target_stations[2]\n",
    "num_st_keep = 4   #num of stations to keep for analysis\n",
    "analysis_set = dt_trg_set\n",
    "dt_analysis_set = pd.read_csv(analysis_set)\n",
    "dt_analysis_set['path'] = ipd02\n",
    "dt01 = dt_analysis_set[dt_analysis_set['target_station']==int(st_id)]\n",
    "#uniq_stat = dt01['vect_close_bkt2'].unique()[:2]\n",
    "uniq_station = dt01['st_id'].unique()[:num_st_keep]\n",
    "dt02 = dt01[dt01['st_id'].isin(uniq_station)]\n",
    "\n",
    "selected_columns = ['st_id','target_station',\n",
    "                    'fnames','path','vect_close_bkt2']\n",
    "#The output below is fed to STEP 2 to run the analysis\n",
    "dt_analysis_set_keys = dt02[selected_columns].copy()\n",
    "\n",
    "dt_analysis_set_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 2.  Use elements created in STEP 1 to get a data matrix for analysis\n",
    "#All the available stations which have all three metrics needed for model\n",
    "#vc_stid = dt_analysis_set_keys[['st_id']].drop_duplicates()\n",
    "\n",
    "#Organize the data based on the nature of the data so they all conform to the same size matrices\n",
    "#The metrics included are\n",
    "#  AT - air temperature in Celcius\n",
    "#  RH - relative humidity in Percent\n",
    "#  PR - precipitation in MM\n",
    "\n",
    "#Put them all in appropriate matrices and create list of metrics\n",
    "#Data files are of two types\n",
    "#    -  24 hour * 365 days * n years  (AT, PR)\n",
    "#    -  365 days * n years (RH)\n",
    "\n",
    "#Parameters governing data matrix\n",
    "in_data = dt_analysis_set_keys\n",
    "in_nstations = 2 #nstations\n",
    "in_path = ipd02\n",
    "nhours = 24\n",
    "ndays = 365\n",
    "nyears_row = 2\n",
    "nyears_col = 2\n",
    "n_metrics = 5\n",
    "\n",
    "print(ipd02)\n",
    "rtn_matrix = fn_make_matrix_pr06(in_data,in_nstations,in_path,nhours,ndays,nyears_row,nyears_col,n_metrics)\n",
    "rtn_mtx_pqt = pd.DataFrame(rtn_matrix)\n",
    "rtn_mtx_pqt.to_parquet(f\"{ipd02}{st_id}_2st_2RYr_2CYr.parquet\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtn_matrix.shape\n",
    "rtn_mtx_pqt = pd.DataFrame(rtn_matrix)\n",
    "rtn_mtx_pqt.to_parquet(f\"{ipd02}{st_id}_1Yr.parquet\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_make_matrix(vc_measure,offset_measure,nhours,ndays,nyears_col,nyears_row):\n",
    "    #the offset from the start of the vector to align the data points\n",
    "    vector = vc_measure\n",
    "\n",
    "    nrows = nhours*ndays*nyears_row   #the number of values put in each row of matrix days*hours*years\n",
    "    ncols = nhours*ndays*nyears_col   #the number of values put in each row of matrix days*hours*years\n",
    "    \n",
    "    # Create an empty matrix to store the results\n",
    "    matrix = np.zeros((nrows, ncols))\n",
    "\n",
    "    # Fill the matrix\n",
    "    for i in range(nrows):\n",
    "        matrix[i] = vector[(offset_measure + i):(offset_measure + i + ncols)]\n",
    "     \n",
    "    return matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  (8597, 43800)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "category",
         "type": "unknown"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "5706d311-4ce0-4c74-81be-5c9044100fac",
       "rows": [
        [
         "0",
         "8239"
        ],
        [
         "1",
         "358"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 2
       }
      },
      "text/plain": [
       "0\n",
       "0    8239\n",
       "1     358\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform_matrix(df, bins, labels):\n",
    "    \"\"\"\n",
    "    Transforms the given matrix by binning the first column, dropping the first row and column,\n",
    "    and adding the binned column as the first column.\n",
    "\n",
    "    Parameters:\n",
    "        matrix (pd.DataFrame): The input matrix.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The transformed matrix.\n",
    "    \"\"\"\n",
    "    # Extract the first column (dependent variable)\n",
    "    firstcolumn = df.iloc[:, 0].astype('float32')\n",
    "\n",
    "    # Bin the first column\n",
    "    binned_column = pd.cut(firstcolumn, bins=bins, labels=labels)\n",
    "\n",
    "    # Drop the first first column\n",
    "    df = df.iloc[:, 1:]\n",
    "\n",
    "    # Add the binned column as the first column\n",
    "    df.insert(0, 0, binned_column.iloc[0:].values)\n",
    "\n",
    "    # Return the final matrix\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "# Example matrix (replace this with your actual matrix)\n",
    "jj   = rtn_matrix\n",
    "df = pd.DataFrame(jj).iloc[1:,] # Convert to DataFrame and drop the first row for binning\n",
    "# Drop rows where the first column has missing values\n",
    "# df = df.dropna(subset=[0])\n",
    "fc = df.iloc[:,0].astype('float32') # Ensure the first column is float for binning\n",
    "#DEL#bins = [-float('inf'), 10, 20, 25, float('inf')]\n",
    "\n",
    "bins = [-float('inf'), 1, float('inf')]\n",
    "# bins = [-float('inf'), 0] + list(pd.qcut(fc[fc > 0], q=4, retbins=True)[1][1:]) + [float('inf')]\n",
    "labels = [0, 1]\n",
    "# print(bins, labels)\n",
    "\n",
    "\n",
    "# bins = [-float('inf'), 0] + list(pd.qcut(fc[fc > 0], q=4, retbins=True)[1][1:]) + [float('inf')]\n",
    "#labels = [0, 1, 2, 3]\n",
    "# print(bins, labels)\n",
    "\n",
    "df_transformed = transform_matrix(df, bins, labels)\n",
    "df_transformed = df_transformed[df_transformed[0]>= 0]\n",
    "print(\"Dataset: \", df_transformed.shape)\n",
    "df_transformed[0].value_counts(dropna=False).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: (6019, 43800)\n",
      "0\n",
      "0    5730\n",
      "1     289\n",
      "Name: count, dtype: int64\n",
      "Validation Set: (859, 43800)\n",
      "0\n",
      "0    852\n",
      "1      7\n",
      "Name: count, dtype: int64\n",
      "Test Set: (1719, 43800)\n",
      "0\n",
      "0    1657\n",
      "1      62\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "def split_time_series_data(df, train_ratio=0.7, val_ratio=0.1, test_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Splits a time series dataset into training, validation, and test sets.\n",
    "    Works with pandas.DataFrame, polars.DataFrame, and polars.LazyFrame.\n",
    "    Uses lazy evaluation for polars LazyFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame | pl.DataFrame | pl.LazyFrame): The input dataset, \n",
    "            ordered in descending dates.\n",
    "        train_ratio (float): Proportion of data for training.\n",
    "        val_ratio (float): Proportion of data for validation.\n",
    "        test_ratio (float): Proportion of data for testing.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (train, validation, test) of the same type as `df`.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        total_ratio = train_ratio + val_ratio + test_ratio\n",
    "        if abs(total_ratio - 1.0) > 1e-9:\n",
    "            raise ValueError(f\"Ratios must sum to 1.0 (got {total_ratio:.4f})\")\n",
    "\n",
    "        # train_end = int(n * train_ratio)\n",
    "        # val_end = train_end + int(n * val_ratio)\n",
    "        \n",
    "        # --- Handle pandas ---\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            n = len(df)\n",
    "            if n == 0:\n",
    "                raise ValueError(\"Input DataFrame is empty\")\n",
    "\n",
    "            train_end = int(n * train_ratio)\n",
    "            val_end = train_end + int(n * val_ratio)       \n",
    "\n",
    "          \n",
    "            train_data = df.iloc[:train_end].sort_index(ascending=True)\n",
    "            val_data = df.iloc[train_end:val_end].sort_index(ascending=True)\n",
    "            test_data = df.iloc[val_end:].sort_index(ascending=True)\n",
    "\n",
    "\n",
    "            logging.info(\"Split pandas DataFrame into train=%d, val=%d, test=%d\",\n",
    "                        len(train_data), len(val_data), len(test_data))\n",
    "\n",
    "            return train_data, val_data, test_data\n",
    "\n",
    "        # --- Handle polars DataFrame ---\n",
    "        elif isinstance(df, pl.DataFrame):\n",
    "            n = df.height\n",
    "            if n == 0:\n",
    "                raise ValueError(\"Input Polars DataFrame is empty\")\n",
    "\n",
    "            train_end = int(n * train_ratio)\n",
    "            val_end = train_end + int(n * val_ratio)  \n",
    "            \n",
    "            train_data = df.slice(0, train_end)\n",
    "            val_data = df.slice(train_end, val_end - train_end)\n",
    "            test_data = df.slice(val_end, n - val_end)\n",
    "           \n",
    "\n",
    "            logging.info(\"Split polars DataFrame into train=%d, val=%d, test=%d\",\n",
    "                        train_data.height, val_data.height, test_data.height)\n",
    "\n",
    "            return train_data, val_data, test_data\n",
    "\n",
    "        # --- Handle polars LazyFrame ---\n",
    "        elif isinstance(df, pl.LazyFrame):\n",
    "            n = df.collect().height  # collect just to compute length\n",
    "            if n == 0:\n",
    "                raise ValueError(\"Input Polars LazyFrame is empty\")\n",
    "\n",
    "            train_end = int(n * train_ratio)\n",
    "            val_end = train_end + int(n * val_ratio)  \n",
    "\n",
    "            # slicing remains lazy\n",
    "            train_data = df.slice(0, train_end)      \n",
    "            val_data = df.slice(train_end, val_end - train_end)\n",
    "            test_data = df.slice(val_end, n - val_end)\n",
    "         \n",
    "\n",
    "            logging.info(\"Prepared lazy splits for polars LazyFrame (rows=%d)\", n)\n",
    "\n",
    "            return train_data, val_data, test_data\n",
    "\n",
    "        else:\n",
    "            raise TypeError(\"Input must be a pandas.DataFrame, polars.DataFrame, or polars.LazyFrame\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(\"Failed to split time series data: %s\", str(e))\n",
    "        raise\n",
    "\n",
    "# Example usage\n",
    "train_data, val_data, test_data = split_time_series_data(df_transformed)\n",
    "print(\"Training Set:\", train_data.shape)\n",
    "print(train_data[0].value_counts(dropna=False).sort_index())\n",
    "print(\"Validation Set:\", val_data.shape)\n",
    "print(val_data[0].value_counts(dropna=False).sort_index())\n",
    "print(\"Test Set:\", test_data.shape)\n",
    "print(test_data[0].value_counts(dropna=False).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets exported successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the station ID and output directory\n",
    "station_id = st_id\n",
    "#output_dir = \"D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\\"\n",
    "output_dir = ipd02\n",
    "\n",
    "\n",
    "# Export datasets to CSV\n",
    "train_data.to_parquet(f\"{output_dir}{station_id}_train.parquet\", index=False)\n",
    "val_data.to_parquet(f\"{output_dir}{station_id}_validation.parquet\", index=False)\n",
    "test_data.to_parquet(f\"{output_dir}{station_id}_test.parquet\", index=False)\n",
    "\n",
    "# train_data.to_csv(f\"{output_dir}{station_id}_train.csv\", index=False)\n",
    "# val_data.to_csv(f\"{output_dir}{station_id}_validation.csv\", index=False)\n",
    "# test_data.to_csv(f\"{output_dir}{station_id}_test.csv\", index=False)\n",
    "\n",
    "print(\"Datasets exported successfully.\")\n",
    "\n",
    "#python -m s4model --modelname 72508014740 --trainset ../data/weathermetrics/72508014740_train.csv --valset ../data/weathermetrics/72508014740_validation.csv --testset ../data/weathermetrics/72508014740_test.csv --tabulardata --dependent_variable 0 --epochs 30\n",
    "\n",
    "#python -m s4model --modelname 72508014740 --trainset \\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72508014740_train.parquet --valset \\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72508014740_validation.parquet --testset \\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72508014740_test.parquet --tabulardata --dependent_variable 0 --epochs 30\n",
    "\n",
    "# python -m charts --df ../results/72508014740_Test_results_20250424_104121PM.csv --actual 0 --predicted Predicted \n",
    "# python -m charts --df ../results/72508014740_Test_results_20250427_100132PM.csv --actual 0 --predicted Predicted \n",
    "# python -m charts --df ../results/72508014740_Test_results_20250428_091259PM.csv --actual 0 --predicted Predicted \n",
    "# python -m charts --df ../results/72508014740_Test_results_20250428_094033PM.csv --actual 0 --predicted Predicted \n",
    "# python -m charts --df ../results/72508014740_Test_results_20250429_091538PM.csv --actual 0 --predicted Predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_id\n",
    "ipd02\n",
    "python -m charts --df ../results/72508014740_Test_results_20250527_093455PM.csv --actual 0 --predicted Predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_first_non_nan(vector):\n",
    "#     return next((i for i, x in enumerate(vector) if not math.isnan(x)), None)\n",
    "\n",
    "# first_number_at1 = find_first_non_nan(vc_at1)\n",
    "# first_number_rh1 = find_first_non_nan(vc_rh1)\n",
    "# first_number_rh2 = find_first_non_nan(vc_rh2)\n",
    "# first_number_rh3 = find_first_non_nan(vc_rh3)\n",
    "# first_number_pr1 = find_first_non_nan(vc_pr1)\n",
    "\n",
    "# #Number of missing data points\n",
    "# print(f\"AT1 Number of missing cells: {np.count_nonzero(np.isnan(vc_at1))}\")\n",
    "# print(f\"AT1 Vector Length: {len(vc_at1)}\")\n",
    "\n",
    "# print(f\"RH1 Number of missing cells: {np.count_nonzero(np.isnan(vc_rh1))}\")\n",
    "# print(f\"RH1 Vector Length: {len(vc_rh1)}\")\n",
    "\n",
    "# print(f\"PR1 Number of missing cells: {np.count_nonzero(np.isnan(vc_pr1))}\")\n",
    "# print(f\"PR1 Vector Length: {len(vc_pr1)}\")\n",
    "\n",
    "# #Once a day metrics\n",
    "# v_data_first_once_day = max(\n",
    "#                    first_number_rh1,\n",
    "#                    first_number_rh2,\n",
    "#                    first_number_rh3)\n",
    "# print(v_data_first_once_day)\n",
    "\n",
    "# #24 times a day metrics\n",
    "# v_data_first_24h_day = max(first_number_at1,\n",
    "#                    first_number_pr1)\n",
    "# print(v_data_first_24h_day)\n",
    "\n",
    "\n",
    "#These vectors all start with a numeric value so missing values can be filled\n",
    "\n",
    "\n",
    "#print(first_number_at1)\n",
    "#print(first_number_rh1)\n",
    "#print(first_number_rh2)\n",
    "#print(first_number_rh3)\n",
    "#print(first_number_pr1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(jj)\n",
    "fc = df.iloc[:,0].astype('float32')\n",
    "print(fc.shape)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(fc, bins=50, color='blue', alpha=0.7, density=True)\n",
    "plt.title('Normalized Histogram of a')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom bins\n",
    "\n",
    "a = fc[~fc.isnull()]\n",
    "\n",
    "# Calculate min, max, and percentiles by 10 for 'a'\n",
    "min_val = a.min()\n",
    "max_val = a.max()\n",
    "percentiles = np.percentile(a, np.arange(0, 101, 10))\n",
    "\n",
    "print(f\"Min: {min_val}\")\n",
    "print(f\"Max: {max_val}\")\n",
    "print(f\"Percentiles: {percentiles}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s4_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
