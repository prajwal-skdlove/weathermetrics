{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_make_matrix_pr01(stid,ipd,nhours,ndays,nyears_row,nyears_col):\n",
    "\n",
    "    #Create a matrix for precipitation data that feeds\n",
    "    #The S4 model\n",
    "    # 1. Read in all data files\n",
    "    # 2. Find the common starting date that will be used to align the matrices\n",
    "    # 3. Keep only the values that fit the common starting date\n",
    "    # 4. Combine the data into one large matrix for the StationID \n",
    "\n",
    "    # 1. Read in data files and get start date v_date... for each\n",
    "    # Check that data exist for all metrics\n",
    "\n",
    "    #Air temperature (one variable recorded once an hour a day)\n",
    "    df_at = pd.read_csv(ipd + stid + \"_model_data_airtemp.csv\")\n",
    "    v_date_at = datetime.strptime(df_at.iloc[0,2], \"%Y-%m-%d\").date()\n",
    "    #Relative Hunidity (3 variables recorded once a day)\n",
    "    df_rh = pd.read_csv(ipd + stid + \"_model_data_relhum.csv\")\n",
    "    v_date_rh = datetime.strptime(df_rh.iloc[0,4], \"%Y-%m-%d\").date()\n",
    "    #Precipitation (one variable recorded once an hour a day)\n",
    "    df_pr = pd.read_csv(ipd + stid + \"_model_data_precip.csv\")\n",
    "    v_date_pr = datetime.strptime(df_pr.iloc[0,2], \"%Y-%m-%d\").date()\n",
    "\n",
    "\n",
    "    # 2. Find the offsets needed to all datasets start at same starting date\n",
    "    v_date_minimum = min(v_date_at,v_date_rh,v_date_pr)\n",
    "    v_date_minimum\n",
    "\n",
    "    #Identify offsets to use so all datadata vectors are appropriately aligned \n",
    "    offset_at = -(v_date_minimum - v_date_at).days\n",
    "    offset_rh = -(v_date_minimum - v_date_rh).days\n",
    "    offset_pr = -(v_date_minimum - v_date_pr).days\n",
    "\n",
    "    # 3. Get vectors of data for each measure\n",
    "    #These vectors all start on the same date\n",
    "    vc_at1 = df_at.iloc[offset_at:(len(df_at.iloc[:,1])-offset_at),1]\n",
    "    vc_rh1 = df_rh.iloc[offset_rh:(len(df_rh.iloc[:,1])-offset_rh),1]\n",
    "    vc_rh2 = df_rh.iloc[offset_rh:(len(df_rh.iloc[:,2])-offset_rh),2]\n",
    "    vc_rh3 = df_rh.iloc[offset_rh:(len(df_rh.iloc[:,3])-offset_rh),3]\n",
    "    vc_pr1 = df_pr.iloc[offset_pr:(len(df_pr.iloc[:,1])-offset_pr),1]\n",
    "\n",
    "\n",
    "    # 4. Interleave vectors so we get a complete vector of all metrics\n",
    "    #Start with the once a day vectors and loop over the number of days as defined by once a day vector\n",
    "    #These set how many years of data are needed : nyears_row + nyears_col\n",
    "    nyears_data_limit = nyears_row + nyears_col + 1\n",
    "    nrows = nhours*ndays*nyears_row\n",
    "    ncols = nhours*ndays*nyears_col \n",
    "    \n",
    "    # set the number of metrics created each day\n",
    "    # Metrics measured at hourly intervals are put into day metrics\n",
    "    # And ordered from most recent hour to most distant hour from left to right\n",
    "    # Day metrics are also ordered most recent day to most distant day left to right\n",
    "    ndaily_metrics = 2*nhours + 3*1\n",
    "\n",
    "    #Set the total number of days of data needed for the model\n",
    "    num_days = nyears_data_limit*ndays\n",
    "    #print(f\"Number of days: {num_days}\")\n",
    "\n",
    "    is_first = 1\n",
    "\n",
    "    #Construct the vector that holds all data values (valid and missing)\n",
    "    for i_day in range(num_days):\n",
    "        i_day_hour_end = (i_day + 1)*nhours - 1\n",
    "        i_day_hour_start = i_day_hour_end - (nhours -1)\n",
    "        va = vc_pr1[(i_day_hour_start + offset_pr):(i_day_hour_end + offset_pr)]\n",
    "        vb = vc_rh1[(i_day + offset_rh):(i_day + offset_rh)]\n",
    "        vc = vc_rh2[(i_day + offset_rh):(i_day + offset_rh)]\n",
    "        vd = vc_rh3[(i_day + offset_rh):(i_day + offset_rh)]\n",
    "        ve = vc_at1[(i_day_hour_start + offset_pr):(i_day_hour_end + offset_pr)]\n",
    "\n",
    "        #vmt_full set = np.concatenate((vmt_one_day,va,vb,vc,vd,ve)) \n",
    "        #print(i_day)\n",
    "        if is_first == 0:\n",
    "            vmt_full_set = np.concatenate((vmt_full_set,va,vb,vc,vd,ve))\n",
    "            #print(len(vmt_full_set))\n",
    "        else:\n",
    "            is_first = 0\n",
    "            vmt_full_set = np.concatenate((va,vb,vc,vd,ve))\n",
    "            #print(len(vmt_full_set))\n",
    "\n",
    "    #print(f\"Length of data vector: {len(vmt_full_set)}\")\n",
    "\n",
    "\n",
    "    #Organize the data vector into an array which will be returned to the function call\n",
    "    #Create an empty matrix in which data are organized\n",
    "    nrows = nyears_row*ndays\n",
    "    ncols = nyears_col*ndays*ndaily_metrics\n",
    "    nrow_days =  nyears_row*ndays\n",
    "    ncol_days =  nyears_col*ndays\n",
    "    data_limit = len(vmt_full_set)\n",
    "    #print(f\"Number of columns in matrix: {ncols}\")\n",
    "    #print(f\"Limit of data: {data_limit}\")\n",
    "\n",
    "    #Make template matrix to house data\n",
    "    template_matrix = np.zeros((nrows, ncols), dtype=np.float16)\n",
    "\n",
    "    # Fill the matrix\n",
    "    for i in range(nrow_days):\n",
    "        vc_start = i*(ndaily_metrics)\n",
    "        vc_end = ncols + vc_start\n",
    "        \n",
    "        if vc_end < data_limit:\n",
    "            template_matrix[i] = vmt_full_set[(vc_start):(vc_end)]\n",
    "\n",
    "    #   print(vc_start)\n",
    "    #   print(vc_end)\n",
    "    #   print(len(vmt_full_set[(vc_start):(vc_end)]))\n",
    "        \n",
    "    return template_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_make_matrix_pr02(stid,ipd,nhours,ndays,nyears_row,nyears_col):\n",
    "\n",
    "    #Create a matrix for precipitation data that feeds\n",
    "    #The S4 model\n",
    "    # 1. Read in all data files\n",
    "    # 2. Find the common starting date that will be used to align the matrices\n",
    "    # 3. Keep only the values that fit the common starting date\n",
    "    # 4. Combine the data into one large matrix for the StationID \n",
    "\n",
    "    # 1. Read in data files and get start date v_date... for each\n",
    "    # Check that data exist for all metrics\n",
    "\n",
    "    #Air temperature (one variable recorded once an hour a day)\n",
    "    df_at = pd.read_csv(ipd + stid + \"_model_data_airtemp.csv\")\n",
    "    v_date_at = datetime.strptime(df_at.iloc[0,2], \"%Y-%m-%d\").date()\n",
    "    #Relative Hunidity (3 variables recorded once a day)\n",
    "    df_rh = pd.read_csv(ipd + stid + \"_model_data_relhum.csv\")\n",
    "    v_date_rh = datetime.strptime(df_rh.iloc[0,4], \"%Y-%m-%d\").date()\n",
    "    #Precipitation (one variable recorded once an hour a day)\n",
    "    df_pr = pd.read_csv(ipd + stid + \"_model_data_precip.csv\")\n",
    "    v_date_pr = datetime.strptime(df_pr.iloc[0,2], \"%Y-%m-%d\").date()\n",
    "\n",
    "\n",
    "    # 2. Find the offsets needed to all datasets start at same starting date\n",
    "    v_date_minimum = min(v_date_at,v_date_rh,v_date_pr)\n",
    "    v_date_minimum\n",
    "\n",
    "    #Identify offsets to use so all datadata vectors are appropriately aligned \n",
    "    offset_at = -(v_date_minimum - v_date_at).days\n",
    "    offset_rh = -(v_date_minimum - v_date_rh).days\n",
    "    offset_pr = -(v_date_minimum - v_date_pr).days\n",
    "\n",
    "    # 3. Get vectors of data for each measure\n",
    "    #These vectors all start on the same date\n",
    "    vc_at1 = df_at.iloc[offset_at:(len(df_at.iloc[:,1])-offset_at),1]\n",
    "    vc_rh1 = df_rh.iloc[offset_rh:(len(df_rh.iloc[:,1])-offset_rh),1]\n",
    "    vc_rh2 = df_rh.iloc[offset_rh:(len(df_rh.iloc[:,2])-offset_rh),2]\n",
    "    vc_rh3 = df_rh.iloc[offset_rh:(len(df_rh.iloc[:,3])-offset_rh),3]\n",
    "    vc_pr1 = df_pr.iloc[offset_pr:(len(df_pr.iloc[:,1])-offset_pr),1]\n",
    "\n",
    "\n",
    "    # 4. Interleave vectors so we get a complete vector of all metrics\n",
    "    #Start with the once a day vectors and loop over the number of days as defined by once a day vector\n",
    "    #These set how many years of data are needed : nyears_row + nyears_col\n",
    "    nyears_data_limit = nyears_row + nyears_col + 1\n",
    "    nrows = nhours*ndays*nyears_row\n",
    "    ncols = nhours*ndays*nyears_col \n",
    "    \n",
    "    # set the number of metrics created each day\n",
    "    # Metrics measured at hourly intervals are put into day metrics\n",
    "    # And ordered from most recent hour to most distant hour from left to right\n",
    "    # Day metrics are also ordered most recent day to most distant day left to right\n",
    "    ndaily_metrics = 2*nhours + 3*1\n",
    "\n",
    "    #Set the total number of days of data needed for the model\n",
    "    num_days = nyears_data_limit*ndays\n",
    "    #print(f\"Number of days: {num_days}\")\n",
    "\n",
    "    is_first = 1\n",
    "\n",
    "    #Construct the vector that holds all data values (valid and missing)\n",
    "    for i_day in range(num_days):\n",
    "        i_day_hour_end = (i_day + 1)*nhours - 1\n",
    "        i_day_hour_start = i_day_hour_end - (nhours -1)\n",
    "        va = vc_pr1[(i_day_hour_start + offset_pr):(i_day_hour_end + offset_pr)]\n",
    "        vb = vc_rh1[(i_day + offset_rh):(i_day + offset_rh)]\n",
    "        vc = vc_rh2[(i_day + offset_rh):(i_day + offset_rh)]\n",
    "        vd = vc_rh3[(i_day + offset_rh):(i_day + offset_rh)]\n",
    "        ve = vc_at1[(i_day_hour_start + offset_pr):(i_day_hour_end + offset_pr)]\n",
    "\n",
    "        #vmt_full set = np.concatenate((vmt_one_day,va,vb,vc,vd,ve)) \n",
    "        #print(i_day)\n",
    "        if is_first == 0:\n",
    "            vmt_full_set = np.concatenate((vmt_full_set,va,vb,vc,vd,ve))\n",
    "            #print(len(vmt_full_set))\n",
    "        else:\n",
    "            is_first = 0\n",
    "            vmt_full_set = np.concatenate((va,vb,vc,vd,ve))\n",
    "            #print(len(vmt_full_set))\n",
    "\n",
    "    #print(f\"Length of data vector: {len(vmt_full_set)}\")\n",
    "\n",
    "\n",
    "    #Organize the data vector into an array which will be returned to the function call\n",
    "    #Create an empty matrix in which data are organized\n",
    "    nrows = nyears_row*ndays\n",
    "    ncols = nyears_col*ndays*ndaily_metrics\n",
    "    nrow_days =  nyears_row*ndays\n",
    "    ncol_days =  nyears_col*ndays\n",
    "    data_limit = len(vmt_full_set)\n",
    "    #print(f\"Number of columns in matrix: {ncols}\")\n",
    "    #print(f\"Limit of data: {data_limit}\")\n",
    "\n",
    "    #Make template matrix to house data\n",
    "    template_matrix = np.zeros((nrows, ncols), dtype=np.float16)\n",
    "\n",
    "    # Fill the matrix\n",
    "    for i in range(nrow_days):\n",
    "        vc_start = i*(ndaily_metrics)\n",
    "        vc_end = ncols + vc_start\n",
    "        \n",
    "        if vc_end < data_limit:\n",
    "            template_matrix[i] = vmt_full_set[(vc_start):(vc_end)]\n",
    "\n",
    "    #   print(vc_start)\n",
    "    #   print(vc_end)\n",
    "    #   print(len(vmt_full_set[(vc_start):(vc_end)]))\n",
    "        \n",
    "    return template_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "st_id",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "6b4306e4-ef85-46e8-908b-7c0eec847a0c",
       "rows": [
        [
         "0",
         "72502014734"
        ],
        [
         "3",
         "72503014732"
        ],
        [
         "6",
         "72505394728"
        ],
        [
         "9",
         "74486094789"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>st_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72502014734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72503014732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>72505394728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>74486094789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         st_id\n",
       "0  72502014734\n",
       "3  72503014732\n",
       "6  72505394728\n",
       "9  74486094789"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the data for a station and organize for precipitation model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "# Path to data files\n",
    "ipd = \"\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\2024\"\n",
    "st_id = \"72502014734\"\n",
    "v_dist_degrees = \"3\"\n",
    "num_st_keep = 4\n",
    "analysis_set = ipd + \"\\\\\" + st_id + \"_station_analysis_set_\" + v_dist_degrees + \".csv\"\n",
    "dt_analysis_set = pd.read_csv(analysis_set)\n",
    "dt_analysis_set = dt_analysis_set[dt_analysis_set['station_rank'] <= num_st_keep]\n",
    "\n",
    "selected_columns = ['st_id','target_station',\n",
    "                    'fnames','path','station_rank']\n",
    "dt_analysis_set_keys = dt_analysis_set[selected_columns].copy()\n",
    "\n",
    "#dt_analysis_set_keys\n",
    "\n",
    "#All the available stations which have all three metrics needed for model\n",
    "vc_stid = dt_analysis_set_keys[['st_id']].drop_duplicates()\n",
    "\n",
    "#Organize the data based on the nature of the data so they all conform to the same size matrices\n",
    "#The metrics included are\n",
    "#  AT - air temperature in Celcius\n",
    "#  RH - relative humidity in Percent\n",
    "#  PR - precipitation in MM\n",
    "\n",
    "#Put them all in appropriate matrices and create list of metrics\n",
    "#Data files are of two types\n",
    "#    -  24 hour * 365 days * n years  (AT, PR)\n",
    "#    -  365 days * n years (RH)\n",
    "\n",
    "#Parameters governing data matrix\n",
    "nhours = 24\n",
    "ndays = 365\n",
    "nyears_row = 6\n",
    "nyears_col = 3\n",
    "\n",
    "#Get the matrix \n",
    "def get_matrix(in_stid):\n",
    "    return fn_make_matrix_pr01(in_stid,ipd,nhours,ndays,nyears_row,nyears_col)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#lst_matrix = list(map(get_matrix,vc_stid.iloc[1,0]))\n",
    "\n",
    "#print(lst_matrix)\n",
    "#print(jj.shape)\n",
    "jj_st_id <- \"72502014734\"\n",
    "#jj <- fn_make_matrix_pr(\"72381523161\",ipd,nhours,ndays,nyears_row,nyears_col)\n",
    "jj <- fn_make_matrix_pr01(jj_st_id,ipd,nhours,ndays,nyears_row,nyears_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_make_matrix(vc_measure,offset_measure,nhours,ndays,nyears_col,nyears_row):\n",
    "    #the offset from the start of the vector to align the data points\n",
    "    vector = vc_measure\n",
    "\n",
    "    nrows = nhours*ndays*nyears_row   #the number of values put in each row of matrix days*hours*years\n",
    "    ncols = nhours*ndays*nyears_col   #the number of values put in each row of matrix days*hours*years\n",
    "    \n",
    "    # Create an empty matrix to store the results\n",
    "    matrix = np.zeros((nrows, ncols))\n",
    "\n",
    "    # Fill the matrix\n",
    "    for i in range(nrows):\n",
    "        matrix[i] = vector[(offset_measure + i):(offset_measure + i + ncols)]\n",
    "     \n",
    "    return matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_first_non_nan(vector):\n",
    "    return next((i for i, x in enumerate(vector) if not math.isnan(x)), None)\n",
    "\n",
    "first_number_at1 = find_first_non_nan(vc_at1)\n",
    "first_number_rh1 = find_first_non_nan(vc_rh1)\n",
    "first_number_rh2 = find_first_non_nan(vc_rh2)\n",
    "first_number_rh3 = find_first_non_nan(vc_rh3)\n",
    "first_number_pr1 = find_first_non_nan(vc_pr1)\n",
    "\n",
    "#Number of missing data points\n",
    "print(f\"AT1 Number of missing cells: {np.count_nonzero(np.isnan(vc_at1))}\")\n",
    "print(f\"AT1 Vector Length: {len(vc_at1)}\")\n",
    "\n",
    "print(f\"RH1 Number of missing cells: {np.count_nonzero(np.isnan(vc_rh1))}\")\n",
    "print(f\"RH1 Vector Length: {len(vc_rh1)}\")\n",
    "\n",
    "print(f\"PR1 Number of missing cells: {np.count_nonzero(np.isnan(vc_pr1))}\")\n",
    "print(f\"PR1 Vector Length: {len(vc_pr1)}\")\n",
    "\n",
    "#Once a day metrics\n",
    "v_data_first_once_day = max(\n",
    "                   first_number_rh1,\n",
    "                   first_number_rh2,\n",
    "                   first_number_rh3)\n",
    "print(v_data_first_once_day)\n",
    "\n",
    "#24 times a day metrics\n",
    "v_data_first_24h_day = max(first_number_at1,\n",
    "                   first_number_pr1)\n",
    "print(v_data_first_24h_day)\n",
    "\n",
    "\n",
    "#These vectors all start with a numeric value so missing values can be filled\n",
    "\n",
    "\n",
    "#print(first_number_at1)\n",
    "#print(first_number_rh1)\n",
    "#print(first_number_rh2)\n",
    "#print(first_number_rh3)\n",
    "#print(first_number_pr1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s4_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
