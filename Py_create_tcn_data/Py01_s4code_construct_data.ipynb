{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data for a station and organize for precipitation model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20250406\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def date_to_integer(date_string, date_format='%Y-%m-%d'):\n",
    "    \"\"\"\n",
    "    Converts a date string to an integer representation.\n",
    "\n",
    "    Args:\n",
    "        date_string (str): The date string to convert.\n",
    "        date_format (str, optional): The format of the date string. Defaults to '%Y-%m-%d'.\n",
    "\n",
    "    Returns:\n",
    "        int: The integer representation of the date.\n",
    "    \"\"\"\n",
    "    date_object = datetime.datetime.strptime(date_string, date_format).date()\n",
    "    return int(date_object.strftime('%Y%m%d'))\n",
    "\n",
    "# Example usage\n",
    "date_str = '2025-04-06'\n",
    "date_int = date_to_integer(date_str)\n",
    "print(date_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_make_matrix_pr01(stid,ipd,nhours,ndays,nyears_row,nyears_col):\n",
    "\n",
    "    #Create a matrix for precipitation data that feeds\n",
    "    #The S4 model\n",
    "    # 1. Read in all data files\n",
    "    # 2. Find the common starting date that will be used to align the matrices\n",
    "    # 3. Keep only the values that fit the common starting date\n",
    "    # 4. Combine the data into one large matrix for the StationID \n",
    "\n",
    "    # 1. Read in data files and get start date v_date... for each\n",
    "    # Check that data exist for all metrics\n",
    "\n",
    "    #Air temperature (one variable recorded once an hour a day)\n",
    "    df_at = pd.read_csv(ipd + stid + \"_model_data_airtemp.csv\")\n",
    "    v_date_at = datetime.strptime(df_at.iloc[0,2], \"%Y-%m-%d\").date()\n",
    "    #Relative Hunidity (3 variables recorded once a day)\n",
    "    df_rh = pd.read_csv(ipd + stid + \"_model_data_relhum.csv\")\n",
    "    v_date_rh = datetime.strptime(df_rh.iloc[0,4], \"%Y-%m-%d\").date()\n",
    "    #Precipitation (one variable recorded once an hour a day)\n",
    "    df_pr = pd.read_csv(ipd + stid + \"_model_data_precip.csv\")\n",
    "    v_date_pr = datetime.strptime(df_pr.iloc[0,2], \"%Y-%m-%d\").date()\n",
    "\n",
    "\n",
    "    # 2. Find the offsets needed to all datasets start at same starting date\n",
    "    v_date_minimum = min(v_date_at,v_date_rh,v_date_pr)\n",
    "    v_date_minimum\n",
    "\n",
    "    #Identify offsets to use so all datadata vectors are appropriately aligned \n",
    "    offset_at = -(v_date_minimum - v_date_at).days\n",
    "    offset_rh = -(v_date_minimum - v_date_rh).days\n",
    "    offset_pr = -(v_date_minimum - v_date_pr).days\n",
    "\n",
    "    # 3. Get vectors of data for each measure\n",
    "    #These vectors all start on the same date\n",
    "    vc_at1 = df_at.iloc[offset_at:(len(df_at.iloc[:,1])-offset_at),1]\n",
    "    vc_rh1 = df_rh.iloc[offset_rh:(len(df_rh.iloc[:,1])-offset_rh),1]\n",
    "    vc_rh2 = df_rh.iloc[offset_rh:(len(df_rh.iloc[:,2])-offset_rh),2]\n",
    "    vc_rh3 = df_rh.iloc[offset_rh:(len(df_rh.iloc[:,3])-offset_rh),3]\n",
    "    vc_pr1 = df_pr.iloc[offset_pr:(len(df_pr.iloc[:,1])-offset_pr),1]\n",
    "\n",
    "\n",
    "    # 4. Interleave vectors so we get a complete vector of all metrics\n",
    "    #Start with the once a day vectors and loop over the number of days as defined by once a day vector\n",
    "    #These set how many years of data are needed : nyears_row + nyears_col\n",
    "    nyears_data_limit = nyears_row + nyears_col + 1\n",
    "    nrows = nhours*ndays*nyears_row\n",
    "    ncols = nhours*ndays*nyears_col \n",
    "    \n",
    "    # set the number of metrics created each day\n",
    "    # Metrics measured at hourly intervals are put into day metrics\n",
    "    # And ordered from most recent hour to most distant hour from left to right\n",
    "    # Day metrics are also ordered most recent day to most distant day left to right\n",
    "    ndaily_metrics = 2*nhours + 3*1\n",
    "\n",
    "    #Set the total number of days of data needed for the model\n",
    "    num_days = nyears_data_limit*ndays\n",
    "    #print(f\"Number of days: {num_days}\")\n",
    "\n",
    "    is_first = 1\n",
    "\n",
    "    #Construct the vector that holds all data values (valid and missing)\n",
    "    for i_day in range(num_days):\n",
    "        i_day_hour_end = (i_day + 1)*nhours - 1\n",
    "        i_day_hour_start = i_day_hour_end - (nhours -1)\n",
    "        va = vc_pr1[(i_day_hour_start + offset_pr):(i_day_hour_end + offset_pr)]\n",
    "        vb = vc_rh1[(i_day + offset_rh):(i_day + offset_rh)]\n",
    "        vc = vc_rh2[(i_day + offset_rh):(i_day + offset_rh)]\n",
    "        vd = vc_rh3[(i_day + offset_rh):(i_day + offset_rh)]\n",
    "        ve = vc_at1[(i_day_hour_start + offset_pr):(i_day_hour_end + offset_pr)]\n",
    "\n",
    "        #vmt_full set = np.concatenate((vmt_one_day,va,vb,vc,vd,ve)) \n",
    "        #print(i_day)\n",
    "        if is_first == 0:\n",
    "            vmt_full_set = np.concatenate((vmt_full_set,va,vb,vc,vd,ve))\n",
    "            #print(len(vmt_full_set))\n",
    "        else:\n",
    "            is_first = 0\n",
    "            vmt_full_set = np.concatenate((va,vb,vc,vd,ve))\n",
    "            #print(len(vmt_full_set))\n",
    "\n",
    "    #print(f\"Length of data vector: {len(vmt_full_set)}\")\n",
    "\n",
    "\n",
    "    #Organize the data vector into an array which will be returned to the function call\n",
    "    #Create an empty matrix in which data are organized\n",
    "    nrows = nyears_row*ndays\n",
    "    ncols = nyears_col*ndays*ndaily_metrics\n",
    "    nrow_days =  nyears_row*ndays\n",
    "    ncol_days =  nyears_col*ndays\n",
    "    data_limit = len(vmt_full_set)\n",
    "    #print(f\"Number of columns in matrix: {ncols}\")\n",
    "    #print(f\"Limit of data: {data_limit}\")\n",
    "\n",
    "    #Make template matrix to house data\n",
    "    template_matrix = np.zeros((nrows, ncols), dtype=np.float16)\n",
    "\n",
    "    # Fill the matrix\n",
    "    for i in range(nrow_days):\n",
    "        vc_start = i*(ndaily_metrics)\n",
    "        vc_end = ncols + vc_start\n",
    "        \n",
    "        if vc_end < data_limit:\n",
    "            template_matrix[i] = vmt_full_set[(vc_start):(vc_end)]\n",
    "\n",
    "    #   print(vc_start)\n",
    "    #   print(vc_end)\n",
    "    #   print(len(vmt_full_set[(vc_start):(vc_end)]))\n",
    "        \n",
    "    return template_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_make_matrix_pr04(stid_keys,nstations,ipd,nhours,ndays,nyears_row,nyears_col):\n",
    "\n",
    "    #Create a matrix for precipitation data that feeds\n",
    "    #The S4 model\n",
    "    # 1. Read in all data files for multiple stations\n",
    "    # 2. Find the common starting date that will be used to align the matrices\n",
    "    # 3. Keep only the values that fit the common starting date\n",
    "    # 4. Combine the data into one large matrix for the StationID \n",
    "\n",
    "    # 1. Read in data files and get start date v_date... for each\n",
    "    # Check that data exist for all metrics\n",
    "\n",
    "    #Create two objects of length 12\n",
    "    #data_list which contains the metrics for stations 1-4, the first is the target\n",
    "    #data_start_date which contains the most recent date for each file\n",
    "    vc_first_rcd = [0,3,6,9]\n",
    "    num_stations = len(vc_first_rcd)\n",
    "    vc_st_id = stid_keys.iloc[:,0]\n",
    "    vc_path = stid_keys.iloc[:,3]\n",
    "    vc_fnames = stid_keys.iloc[:,2]\n",
    "\n",
    "    data_list = []\n",
    "    data_start_date = []\n",
    "    \n",
    "    for i in range(0, num_stations):\n",
    "            #Air temperature (one variable recorded once an hour a day)\n",
    "            infile = vc_path.iloc[vc_first_rcd[i]] + \"\\\\\" + vc_fnames.iloc[vc_first_rcd[i]]\n",
    "            df_at = pd.read_csv(infile)\n",
    "            data_list.append(df_at)\n",
    "            dt_first = datetime.strptime(df_at.iloc[0,2], \"%Y-%m-%d\").date()\n",
    "            data_start_date.append(dt_first)\n",
    "            print(dt_first)\n",
    "            \n",
    "            #Precipitation (one variable recorded once an hour a day)\n",
    "            infile = vc_path.iloc[vc_first_rcd[i]] + \"\\\\\" + vc_fnames.iloc[vc_first_rcd[i]+1] \n",
    "            df_pr = pd.read_csv(infile)\n",
    "            data_list.append(df_pr)\n",
    "            dt_first = datetime.strptime(df_pr.iloc[0,2], \"%Y-%m-%d\").date()\n",
    "            data_start_date.append(dt_first)\n",
    "            print(dt_first)\n",
    "            \n",
    "            #Relative Humidity (3 variables recorded once a day)\n",
    "            infile = vc_path.iloc[vc_first_rcd[i]] + \"\\\\\" + vc_fnames.iloc[vc_first_rcd[i]+2]\n",
    "            df_rh = pd.read_csv(infile)\n",
    "            data_list.append(df_rh)\n",
    "            dt_first = datetime.strptime(df_rh.iloc[0,4], \"%Y-%m-%d\").date()\n",
    "            data_start_date.append(dt_first)\n",
    "            print(dt_first)\n",
    "            \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhugh\\AppData\\Local\\Temp\\ipykernel_6924\\1852830526.py:29: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_at = pd.read_csv(infile)\n",
      "C:\\Users\\jhugh\\AppData\\Local\\Temp\\ipykernel_6924\\1852830526.py:38: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_pr = pd.read_csv(infile)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09\n",
      "(271752, 3)\n",
      "(6660, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhugh\\AppData\\Local\\Temp\\ipykernel_6924\\1852830526.py:29: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_at = pd.read_csv(infile)\n",
      "C:\\Users\\jhugh\\AppData\\Local\\Temp\\ipykernel_6924\\1852830526.py:38: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_pr = pd.read_csv(infile)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09\n",
      "(271752, 3)\n",
      "(6662, 5)\n",
      "2025-04-09\n",
      "(175272, 3)\n",
      "(4983, 5)\n",
      "2025-03-05\n",
      "(175272, 3)\n",
      "(4992, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jj = fn_make_matrix_pr04(dt_analysis_set_keys,nstations,ipd02,nhours,ndays,nyears_row,nyears_col)\n",
    "jj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_make_matrix_pr02(stid_keys,nstations,ipd,nhours,ndays,nyears_row,nyears_col):\n",
    "\n",
    "    #Create a matrix for precipitation data that feeds\n",
    "    #The S4 model\n",
    "    # 1. Read in all data files for multiple stations\n",
    "    # 2. Find the common starting date that will be used to align the matrices\n",
    "    # 3. Keep only the values that fit the common starting date\n",
    "    # 4. Combine the data into one large matrix for the StationID \n",
    "\n",
    "    # 1. Read in data files and get start date v_date... for each\n",
    "    # Check that data exist for all metrics\n",
    "\n",
    "    #Create two objects of length 12\n",
    "    #data_list which contains the metrics for stations 1-4, the first is the target\n",
    "    #data_start_date which contains the most recent date for each file\n",
    "    vc_first_rcd = [0,3,6,9]\n",
    "    num_stations = len(vc_first_rcd)\n",
    "    vc_st_id = stid_keys.iloc[:,0]\n",
    "    vc_path = stid_keys.iloc[:,3]\n",
    "    vc_fnames = stid_keys.iloc[:,2]\n",
    "\n",
    "    data_list = []\n",
    "    data_start_date = []\n",
    "\n",
    "    for i in range(0, num_stations):\n",
    "            #Air temperature (one variable recorded once an hour a day)\n",
    "            infile = vc_path[vc_first_rcd[i]] + \"\\\\\" + vc_fnames[vc_first_rcd[i]]\n",
    "            df_at = pd.read_csv(infile)\n",
    "            data_list.append(df_at)\n",
    "            dt_first = datetime.strptime(df_at.iloc[0,2], \"%Y-%m-%d\").date()\n",
    "            data_start_date.append(dt_first)\n",
    "\n",
    "            #Precipitation (one variable recorded once an hour a day)\n",
    "            infile = vc_path[vc_first_rcd[i]] + \"\\\\\" + vc_fnames[vc_first_rcd[i]+1] \n",
    "            df_pr = pd.read_csv(infile)\n",
    "            data_list.append(df_pr)\n",
    "            dt_first = datetime.strptime(df_pr.iloc[0,2], \"%Y-%m-%d\").date()\n",
    "            data_start_date.append(dt_first)\n",
    "            \n",
    "            #Relative Humidity (3 variables recorded once a day)\n",
    "            infile = vc_path[vc_first_rcd[i]] + \"\\\\\" + vc_fnames[vc_first_rcd[i]+2]\n",
    "            df_rh = pd.read_csv(infile)\n",
    "            data_list.append(df_rh)\n",
    "            dt_first = datetime.strptime(df_rh.iloc[0,4], \"%Y-%m-%d\").date()\n",
    "            data_start_date.append(dt_first)\n",
    "\n",
    "\n",
    "    # 2. Find the offsets needed to all datasets start at same starting date\n",
    "    v_date_minimum = min(data_start_date)\n",
    "\n",
    "    #Identify offsets to use so all datadata vectors are appropriately aligned \n",
    "    upper_range = len(data_start_date)\n",
    "\n",
    "    date_offsets_list = []\n",
    "    for i in range(0, upper_range):\n",
    "        date_offset = data_start_date[i] - v_date_minimum\n",
    "        #Temporarily set to 0, revise later\n",
    "        date_offset = 0\n",
    "        date_offsets_list.append(date_offset)\n",
    "\n",
    "    # 3. Get vectors of data for each measure\n",
    "    #These vectors all start on the same date\n",
    "\n",
    "    data_vectors_list = []\n",
    "    for i in vc_first_rcd:\n",
    "        tdt = data_list[i]\n",
    "        tof = date_offsets_list[i]\n",
    "        vc_at1 = tdt.iloc[date_offsets_list[i]:(len(tdt.iloc[:,1])-tof),1]\n",
    "        data_vectors_list.append(vc_at1)\n",
    "            \n",
    "        tdt = data_list[i+1]\n",
    "        tof = date_offsets_list[i+1]\n",
    "        vc_pr1 = tdt.iloc[date_offsets_list[i+1]:(len(tdt.iloc[:,1])-tof),1]\n",
    "        data_vectors_list.append(vc_pr1)\n",
    "            \n",
    "        tdt = data_list[i+2]\n",
    "        tof = date_offsets_list[i+2]\n",
    "        vc_rh1 = tdt.iloc[date_offsets_list[i+2]:(len(tdt.iloc[:,1])-tof),1]\n",
    "        data_vectors_list.append(vc_rh1)\n",
    "\n",
    "        tdt = data_list[i+2]\n",
    "        tof = date_offsets_list[i+2]\n",
    "        vc_rh2 = tdt.iloc[date_offsets_list[i+2]:(len(tdt.iloc[:,1])-tof),2]\n",
    "        data_vectors_list.append(vc_rh2)\n",
    "\n",
    "        tdt = data_list[i+2]\n",
    "        tof = date_offsets_list[i+2]\n",
    "        vc_rh3 = tdt.iloc[date_offsets_list[i+2]:(len(tdt.iloc[:,1])-tof),3]\n",
    "        data_vectors_list.append(vc_rh3)\n",
    "\n",
    "    \n",
    "\n",
    "    # 4. Interleave vectors so we get a complete vector of all metrics\n",
    "    #Start with the once a day vectors and loop over the number of days as defined by once a day vector\n",
    "    #These set how many years of data are needed : nyears_row + nyears_col\n",
    "    nyears_data_limit = nyears_row + nyears_col + 1\n",
    "    nrows = nhours*ndays*nyears_row\n",
    "    ncols = nhours*ndays*nyears_col \n",
    "        \n",
    "    # set the number of metrics created each day\n",
    "    # Metrics measured at hourly intervals are put into day metrics\n",
    "    # And ordered from most recent hour to most distant hour from left to right\n",
    "    # Day metrics are also ordered most recent day to most distant day left to right\n",
    "    ndaily_metrics = 2*nhours + 3*1\n",
    "\n",
    "    #Set the total number of days of data needed for the model\n",
    "    num_days = nyears_data_limit*ndays\n",
    "    #print(f\"Number of days: {num_days}\")\n",
    "\n",
    "    is_first = 1\n",
    "\n",
    "    #Construct the vector that holds all data values (valid and missing)\n",
    "    #Assumes that all vectors in data_vectors_list match on date start\n",
    "    #data are organized by day, target station, other stations\n",
    "    for i_day in range(num_days):\n",
    "        i_day_hour_end = (i_day + 1)*nhours - 1\n",
    "        i_day_hour_start = i_day_hour_end - (nhours -1)\n",
    "            \n",
    "        va2 = data_vectors_list[0][(i_day_hour_start):(i_day_hour_end)]  #AT\n",
    "        va1 = data_vectors_list[1][(i_day_hour_start):(i_day_hour_end)]  #PR\n",
    "        va3 = data_vectors_list[2][i_day:i_day]                          #RH1\n",
    "        va4 = data_vectors_list[3][i_day:i_day]                          #RH2\n",
    "        va5 = data_vectors_list[4][i_day:i_day]                          #RH3\n",
    "\n",
    "        vb2 = data_vectors_list[5][(i_day_hour_start):(i_day_hour_end)]  #AT\n",
    "        vb1 = data_vectors_list[6][(i_day_hour_start):(i_day_hour_end)]  #PR\n",
    "        vb3 = data_vectors_list[7][i_day:i_day]                          #RH1\n",
    "        vb4 = data_vectors_list[8][i_day:i_day]                          #RH2\n",
    "        vb5 = data_vectors_list[9][i_day:i_day]                          #RH3\n",
    "\n",
    "        vc2 = data_vectors_list[10][(i_day_hour_start):(i_day_hour_end)]  #AT\n",
    "        vc1 = data_vectors_list[11][(i_day_hour_start):(i_day_hour_end)]  #PR\n",
    "        vc3 = data_vectors_list[12][i_day:i_day]                          #RH1\n",
    "        vc4 = data_vectors_list[13][i_day:i_day]                          #RH2\n",
    "        vc5 = data_vectors_list[14][i_day:i_day]                          #RH3\n",
    "\n",
    "        vd2 = data_vectors_list[15][(i_day_hour_start):(i_day_hour_end)]  #AT\n",
    "        vd1 = data_vectors_list[16][(i_day_hour_start):(i_day_hour_end)]  #PR\n",
    "        vd3 = data_vectors_list[17][i_day:i_day]                          #RH1\n",
    "        vd4 = data_vectors_list[18][i_day:i_day]                          #RH2\n",
    "        vd5 = data_vectors_list[19][i_day:i_day]                          #RH3\n",
    "            \n",
    "\n",
    "        #vmt_full set = np.concatenate((vmt_one_day,va,vb,vc,vd,ve)) \n",
    "        #print(i_day)\n",
    "        if is_first == 0:\n",
    "            vmt_full_set = np.concatenate((vmt_full_set,va1,va2,va3,va4,va5,vb1,vb2,vb3,vb4,vb5,vc1,vc2,vc3,vc4,vc5,vd1,vd2,vd3,vd4,vd5))\n",
    "            #print(len(vmt_full_set))\n",
    "        else:\n",
    "            is_first = 0\n",
    "            vmt_full_set = np.concatenate((va1,va2,va3,va4,va5,vb1,vb2,vb3,vb4,vb5,vc1,vc2,vc3,vc4,vc5,vd1,vd2,vd3,vd4,vd5))\n",
    "            #print(len(vmt_full_set))\n",
    "\n",
    "    print(f\"Length of data vector: {len(vmt_full_set)}\")\n",
    "\n",
    "\n",
    "    #Organize the data vector into an array which will be returned to the function call\n",
    "    #Create an empty matrix in which data are organized\n",
    "    nstations = 4\n",
    "    nrows = nyears_row*ndays\n",
    "    ncols = nstations*nyears_col*ndays*ndaily_metrics\n",
    "    nrow_days =  nyears_row*ndays\n",
    "    ncol_days =  nyears_col*ndays\n",
    "    data_limit = len(vmt_full_set)\n",
    "    #print(f\"Number of columns in matrix: {ncols}\")\n",
    "    #print(f\"Limit of data: {data_limit}\")\n",
    "\n",
    "    #Make template matrix to house data\n",
    "    template_matrix = np.zeros((nrows, ncols), dtype=np.float16)\n",
    "\n",
    "    # Fill the matrix\n",
    "    for i in range(nrow_days):\n",
    "        vc_start = i*(ndaily_metrics)\n",
    "        vc_end = ncols + vc_start\n",
    "            \n",
    "        if vc_end < data_limit:\n",
    "            template_matrix[i] = vmt_full_set[(vc_start):(vc_end)]\n",
    "\n",
    "        #   print(vc_start)\n",
    "        #   print(vc_end)\n",
    "        #   print(len(vmt_full_set[(vc_start):(vc_end)]))\n",
    "            \n",
    "    return template_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file of target stations and their related stations\n",
    "ipd01 = \"\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\2024\\\\\"\n",
    "ipd02 = \"\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\\"\n",
    "dt_trg_set = \"\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\Target_station_analysis_set.csv\"\n",
    "lst_target_stations = [\n",
    "  \"74486094789\",\n",
    "  \"72518014735\",\n",
    "  \"72508014740\",\n",
    "  \"72526014860\",\n",
    "  \"72406093721\",\n",
    "  \"72401013740\",\n",
    "  \"72219013874\",\n",
    "  \"72423093821\",\n",
    "  \"72428014821\",\n",
    "  \"72327013897\"\n",
    "]\n",
    "\n",
    "st_id = lst_target_stations[2]\n",
    "num_st_keep = 4   #num of stations to keep for analysis\n",
    "analysis_set = dt_trg_set\n",
    "dt_analysis_set = pd.read_csv(analysis_set)\n",
    "dt01 = dt_analysis_set[dt_analysis_set['target_station']==int(st_id)]\n",
    "#uniq_stat = dt01['vect_close_bkt2'].unique()[:2]\n",
    "uniq_station = dt01['st_id'].unique()[:num_st_keep]\n",
    "dt02 = dt01[dt01['st_id'].isin(uniq_station)]\n",
    "\n",
    "selected_columns = ['st_id','target_station',\n",
    "                    'fnames','path','vect_close_bkt2']\n",
    "dt_analysis_set_keys = dt02[selected_columns].copy()\n",
    "#dt_analysis_set_keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#All the available stations which have all three metrics needed for model\n",
    "#vc_stid = dt_analysis_set_keys[['st_id']].drop_duplicates()\n",
    "\n",
    "#Organize the data based on the nature of the data so they all conform to the same size matrices\n",
    "#The metrics included are\n",
    "#  AT - air temperature in Celcius\n",
    "#  RH - relative humidity in Percent\n",
    "#  PR - precipitation in MM\n",
    "\n",
    "#Put them all in appropriate matrices and create list of metrics\n",
    "#Data files are of two types\n",
    "#    -  24 hour * 365 days * n years  (AT, PR)\n",
    "#    -  365 days * n years (RH)\n",
    "\n",
    "#Parameters governing data matrix\n",
    "nhours = 24\n",
    "ndays = 365\n",
    "nyears_row = 4\n",
    "nyears_col = 2\n",
    "nstations = num_st_keep\n",
    "\n",
    "rtn_matrix = fn_make_matrix_pr04(dt_analysis_set_keys,nstations,ipd02,nhours,ndays,nyears_row,nyears_col)\n",
    "#rtn_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_make_matrix(vc_measure,offset_measure,nhours,ndays,nyears_col,nyears_row):\n",
    "    #the offset from the start of the vector to align the data points\n",
    "    vector = vc_measure\n",
    "\n",
    "    nrows = nhours*ndays*nyears_row   #the number of values put in each row of matrix days*hours*years\n",
    "    ncols = nhours*ndays*nyears_col   #the number of values put in each row of matrix days*hours*years\n",
    "    \n",
    "    # Create an empty matrix to store the results\n",
    "    matrix = np.zeros((nrows, ncols))\n",
    "\n",
    "    # Fill the matrix\n",
    "    for i in range(nrows):\n",
    "        matrix[i] = vector[(offset_measure + i):(offset_measure + i + ncols)]\n",
    "     \n",
    "    return matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_first_non_nan(vector):\n",
    "    return next((i for i, x in enumerate(vector) if not math.isnan(x)), None)\n",
    "\n",
    "first_number_at1 = find_first_non_nan(vc_at1)\n",
    "first_number_rh1 = find_first_non_nan(vc_rh1)\n",
    "first_number_rh2 = find_first_non_nan(vc_rh2)\n",
    "first_number_rh3 = find_first_non_nan(vc_rh3)\n",
    "first_number_pr1 = find_first_non_nan(vc_pr1)\n",
    "\n",
    "#Number of missing data points\n",
    "print(f\"AT1 Number of missing cells: {np.count_nonzero(np.isnan(vc_at1))}\")\n",
    "print(f\"AT1 Vector Length: {len(vc_at1)}\")\n",
    "\n",
    "print(f\"RH1 Number of missing cells: {np.count_nonzero(np.isnan(vc_rh1))}\")\n",
    "print(f\"RH1 Vector Length: {len(vc_rh1)}\")\n",
    "\n",
    "print(f\"PR1 Number of missing cells: {np.count_nonzero(np.isnan(vc_pr1))}\")\n",
    "print(f\"PR1 Vector Length: {len(vc_pr1)}\")\n",
    "\n",
    "#Once a day metrics\n",
    "v_data_first_once_day = max(\n",
    "                   first_number_rh1,\n",
    "                   first_number_rh2,\n",
    "                   first_number_rh3)\n",
    "print(v_data_first_once_day)\n",
    "\n",
    "#24 times a day metrics\n",
    "v_data_first_24h_day = max(first_number_at1,\n",
    "                   first_number_pr1)\n",
    "print(v_data_first_24h_day)\n",
    "\n",
    "\n",
    "#These vectors all start with a numeric value so missing values can be filled\n",
    "\n",
    "\n",
    "#print(first_number_at1)\n",
    "#print(first_number_rh1)\n",
    "#print(first_number_rh2)\n",
    "#print(first_number_rh3)\n",
    "#print(first_number_pr1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s4_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
