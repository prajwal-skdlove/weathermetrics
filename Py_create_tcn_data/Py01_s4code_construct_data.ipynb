{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data for a station and organize for precipitation model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def date_to_integer(date_string, date_format='%Y-%m-%d'):\n",
    "    \"\"\"\n",
    "    Converts a date string to an integer representation.\n",
    "\n",
    "    Args:\n",
    "        date_string (str): The date string to convert.\n",
    "        date_format (str, optional): The format of the date string. Defaults to '%Y-%m-%d'.\n",
    "\n",
    "    Returns:\n",
    "        int: The integer representation of the date.\n",
    "    \"\"\"\n",
    "    date_object = datetime.datetime.strptime(date_string, date_format).date()\n",
    "    return int(date_object.strftime('%Y%m%d'))\n",
    "\n",
    "# Example usage\n",
    "# date_str = '2025-04-06'\n",
    "# date_int = date_to_integer(date_str)\n",
    "# print(date_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_make_matrix_pr01(stid,ipd,nhours,ndays,nyears_row,nyears_col):\n",
    "\n",
    "    #Create a matrix for precipitation data that feeds\n",
    "    #The S4 model\n",
    "    # 1. Read in all data files\n",
    "    # 2. Find the common starting date that will be used to align the matrices\n",
    "    # 3. Keep only the values that fit the common starting date\n",
    "    # 4. Combine the data into one large matrix for the StationID \n",
    "\n",
    "    # 1. Read in data files and get start date v_date... for each\n",
    "    # Check that data exist for all metrics\n",
    "\n",
    "    #Air temperature (one variable recorded once an hour a day)\n",
    "    df_at = pd.read_csv(ipd + stid + \"_model_data_airtemp.csv\")\n",
    "    v_date_at = datetime.strptime(df_at.iloc[0,2], \"%Y-%m-%d\").date()\n",
    "    #Relative Hunidity (3 variables recorded once a day)\n",
    "    df_rh = pd.read_csv(ipd + stid + \"_model_data_relhum.csv\")\n",
    "    v_date_rh = datetime.strptime(df_rh.iloc[0,4], \"%Y-%m-%d\").date()\n",
    "    #Precipitation (one variable recorded once an hour a day)\n",
    "    df_pr = pd.read_csv(ipd + stid + \"_model_data_precip.csv\")\n",
    "    v_date_pr = datetime.strptime(df_pr.iloc[0,2], \"%Y-%m-%d\").date()\n",
    "\n",
    "\n",
    "    # 2. Find the offsets needed to all datasets start at same starting date\n",
    "    v_date_minimum = min(v_date_at,v_date_rh,v_date_pr)\n",
    "    v_date_minimum\n",
    "\n",
    "    #Identify offsets to use so all datadata vectors are appropriately aligned \n",
    "    offset_at = -(v_date_minimum - v_date_at).days\n",
    "    offset_rh = -(v_date_minimum - v_date_rh).days\n",
    "    offset_pr = -(v_date_minimum - v_date_pr).days\n",
    "\n",
    "    # 3. Get vectors of data for each measure\n",
    "    #These vectors all start on the same date\n",
    "    vc_at1 = df_at.iloc[offset_at:(len(df_at.iloc[:,1])-offset_at),1]\n",
    "    vc_rh1 = df_rh.iloc[offset_rh:(len(df_rh.iloc[:,1])-offset_rh),1]\n",
    "    vc_rh2 = df_rh.iloc[offset_rh:(len(df_rh.iloc[:,2])-offset_rh),2]\n",
    "    vc_rh3 = df_rh.iloc[offset_rh:(len(df_rh.iloc[:,3])-offset_rh),3]\n",
    "    vc_pr1 = df_pr.iloc[offset_pr:(len(df_pr.iloc[:,1])-offset_pr),1]\n",
    "\n",
    "\n",
    "    # 4. Interleave vectors so we get a complete vector of all metrics\n",
    "    #Start with the once a day vectors and loop over the number of days as defined by once a day vector\n",
    "    #These set how many years of data are needed : nyears_row + nyears_col\n",
    "    nyears_data_limit = nyears_row + nyears_col + 1\n",
    "    nrows = nhours*ndays*nyears_row\n",
    "    ncols = nhours*ndays*nyears_col \n",
    "    \n",
    "    # set the number of metrics created each day\n",
    "    # Metrics measured at hourly intervals are put into day metrics\n",
    "    # And ordered from most recent hour to most distant hour from left to right\n",
    "    # Day metrics are also ordered most recent day to most distant day left to right\n",
    "    ndaily_metrics = 2*nhours + 3*1\n",
    "\n",
    "    #Set the total number of days of data needed for the model\n",
    "    num_days = nyears_data_limit*ndays\n",
    "    #print(f\"Number of days: {num_days}\")\n",
    "\n",
    "    is_first = 1\n",
    "\n",
    "    #Construct the vector that holds all data values (valid and missing)\n",
    "    for i_day in range(num_days):\n",
    "        i_day_hour_end = (i_day + 1)*nhours - 1\n",
    "        i_day_hour_start = i_day_hour_end - (nhours -1)\n",
    "        va = vc_pr1[(i_day_hour_start + offset_pr):(i_day_hour_end + offset_pr)]\n",
    "        vb = vc_rh1[(i_day + offset_rh):(i_day + offset_rh)]\n",
    "        vc = vc_rh2[(i_day + offset_rh):(i_day + offset_rh)]\n",
    "        vd = vc_rh3[(i_day + offset_rh):(i_day + offset_rh)]\n",
    "        ve = vc_at1[(i_day_hour_start + offset_pr):(i_day_hour_end + offset_pr)]\n",
    "\n",
    "        #vmt_full set = np.concatenate((vmt_one_day,va,vb,vc,vd,ve)) \n",
    "        #print(i_day)\n",
    "        if is_first == 0:\n",
    "            vmt_full_set = np.concatenate((vmt_full_set,va,vb,vc,vd,ve))\n",
    "            #print(len(vmt_full_set))\n",
    "        else:\n",
    "            is_first = 0\n",
    "            vmt_full_set = np.concatenate((va,vb,vc,vd,ve))\n",
    "            #print(len(vmt_full_set))\n",
    "\n",
    "    #print(f\"Length of data vector: {len(vmt_full_set)}\")\n",
    "\n",
    "\n",
    "    #Organize the data vector into an array which will be returned to the function call\n",
    "    #Create an empty matrix in which data are organized\n",
    "    nrows = nyears_row*ndays\n",
    "    ncols = nyears_col*ndays*ndaily_metrics\n",
    "    nrow_days =  nyears_row*ndays\n",
    "    ncol_days =  nyears_col*ndays\n",
    "    data_limit = len(vmt_full_set)\n",
    "    #print(f\"Number of columns in matrix: {ncols}\")\n",
    "    #print(f\"Limit of data: {data_limit}\")\n",
    "\n",
    "    #Make template matrix to house data\n",
    "    template_matrix = np.zeros((nrows, ncols), dtype=np.float16)\n",
    "\n",
    "    # Fill the matrix\n",
    "    for i in range(nrow_days):\n",
    "        vc_start = i*(ndaily_metrics)\n",
    "        vc_end = ncols + vc_start\n",
    "        \n",
    "        if vc_end < data_limit:\n",
    "            template_matrix[i] = vmt_full_set[(vc_start):(vc_end)]\n",
    "\n",
    "    #   print(vc_start)\n",
    "    #   print(vc_end)\n",
    "    #   print(len(vmt_full_set[(vc_start):(vc_end)]))\n",
    "        \n",
    "    return template_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_make_matrix_pr04(stid_keys,nstations,ipd,nhours,ndays,nyears_row,nyears_col):\n",
    "\n",
    "    #Create a matrix for precipitation data that feeds\n",
    "    #The S4 model\n",
    "    # 1. Read in all data files for multiple stations\n",
    "    # 2. Find the common starting date that will be used to align the matrices\n",
    "    # 3. Keep only the values that fit the common starting date\n",
    "    # 4. Combine the data into one large matrix for the StationID \n",
    "\n",
    "    # 1. Read in data files and get start date v_date... for each\n",
    "    # Check that data exist for all metrics\n",
    "\n",
    "    #Create two objects of length 12\n",
    "    #data_list which contains the metrics for stations 1-4, the first is the target\n",
    "    #data_start_date which contains the most recent date for each file\n",
    "    vc_first_rcd = [0,3,6,9]\n",
    "    num_stations = len(vc_first_rcd)\n",
    "    vc_st_id = stid_keys.iloc[:,0]\n",
    "    vc_path = stid_keys.iloc[:,3]\n",
    "    vc_fnames = stid_keys.iloc[:,2]\n",
    "\n",
    "    data_list = []\n",
    "    data_start_date = []\n",
    "    \n",
    "    for i in range(0, num_stations):\n",
    "            #Air temperature (one variable recorded once an hour a day)\n",
    "            infile = vc_path.iloc[vc_first_rcd[i]] + \"\\\\\" + vc_fnames.iloc[vc_first_rcd[i]]\n",
    "            df_at = pd.read_csv(infile)\n",
    "            data_list.append(df_at)\n",
    "            dt_first = datetime.strptime(df_at.iloc[0,2], \"%Y-%m-%d\").date()\n",
    "            data_start_date.append(dt_first)\n",
    "            #print(dt_first)\n",
    "            \n",
    "            #Precipitation (one variable recorded once an hour a day)\n",
    "            infile = vc_path.iloc[vc_first_rcd[i]] + \"\\\\\" + vc_fnames.iloc[vc_first_rcd[i]+1] \n",
    "            df_pr = pd.read_csv(infile)\n",
    "            data_list.append(df_pr)\n",
    "            dt_first = datetime.strptime(df_pr.iloc[0,2], \"%Y-%m-%d\").date()\n",
    "            data_start_date.append(dt_first)\n",
    "            #print(dt_first)\n",
    "            \n",
    "            #Relative Humidity (3 variables recorded once a day)\n",
    "            infile = vc_path.iloc[vc_first_rcd[i]] + \"\\\\\" + vc_fnames.iloc[vc_first_rcd[i]+2]\n",
    "            df_rh = pd.read_csv(infile)\n",
    "            data_list.append(df_rh)\n",
    "            dt_first = datetime.strptime(df_rh.iloc[0,4], \"%Y-%m-%d\").date()\n",
    "            data_start_date.append(dt_first)\n",
    "            #print(dt_first)\n",
    " \n",
    "     # 2. Find the offsets needed to all datasets start at same starting date\n",
    "    v_date_minimum = min(data_start_date)\n",
    "    print(v_date_minimum)\n",
    "\n",
    "    #Identify offsets to use so all datadata vectors are appropriately aligned \n",
    "    upper_range = len(data_start_date)\n",
    "    \n",
    "    date_offsets_list = []\n",
    "    for i in range(0, upper_range):\n",
    "        date_offset = data_start_date[i] - v_date_minimum\n",
    "        #Temporarily set to 0, revise later\n",
    "        #date_offset = 0\n",
    "        date_offsets_list.append(date_offset)\n",
    "    \n",
    "    # 3. Get vectors of data for each measure\n",
    "    #These vectors all start on the same date\n",
    "\n",
    "    data_vectors_list = []\n",
    "    for i in vc_first_rcd:\n",
    "        tdt = data_list[i]\n",
    "        tof = date_offsets_list[i]\n",
    "        \n",
    "        i_start = tof.days\n",
    "        i_end = len(tdt.iloc[:,1]) - i_start\n",
    "        vc_at1 = tdt.iloc[i_start:i_end,1]\n",
    "        data_vectors_list.append(vc_at1)\n",
    "\n",
    "        tdt = data_list[i+1]\n",
    "        tof = date_offsets_list[i+1]\n",
    "        i_start = tof.days\n",
    "        i_end = len(tdt.iloc[:,1]) - i_start\n",
    "        vc_pr1 = tdt.iloc[i_start:i_end,1]\n",
    "        data_vectors_list.append(vc_pr1)\n",
    "            \n",
    "        tdt = data_list[i+2]\n",
    "        tof = date_offsets_list[i+2]\n",
    "        i_start = tof.days\n",
    "        i_end = len(tdt.iloc[:,1]) - i_start        \n",
    "        vc_rh1 = tdt.iloc[i_start:i_end,1]\n",
    "        data_vectors_list.append(vc_rh1)\n",
    "\n",
    "        tdt = data_list[i+2]\n",
    "        tof = date_offsets_list[i+2]\n",
    "        i_start = tof.days\n",
    "        i_end = len(tdt.iloc[:,1]) - i_start        \n",
    "        vc_rh2 = tdt.iloc[i_start:i_end,2]\n",
    "        data_vectors_list.append(vc_rh2)\n",
    "\n",
    "        tdt = data_list[i+2]\n",
    "        tof = date_offsets_list[i+2]\n",
    "        i_start = tof.days\n",
    "        i_end = len(tdt.iloc[:,1]) - i_start        \n",
    "        vc_rh3 = tdt.iloc[i_start:i_end,3]\n",
    "        data_vectors_list.append(vc_rh3)\n",
    "    \n",
    "    # 4. Interleave vectors so we get a complete vector of all metrics\n",
    "    #Start with the once a day vectors and loop over the number of days as defined by once a day vector\n",
    "    #These set how many years of data are needed : nyears_row + nyears_col\n",
    "    nyears_data_limit = nyears_row + nyears_col + 1\n",
    "    nrows = nhours*ndays*nyears_row\n",
    "    ncols = nhours*ndays*nyears_col \n",
    "        \n",
    "    # set the number of metrics created each day\n",
    "    # Metrics measured at hourly intervals are put into day metrics\n",
    "    # And ordered from most recent hour to most distant hour from left to right\n",
    "    # Day metrics are also ordered most recent day to most distant day left to right\n",
    "    ndaily_metrics = 2*nhours + 3*1\n",
    "\n",
    "    #Set the total number of days of data needed for the model\n",
    "    num_days = nyears_data_limit*ndays\n",
    "    #print(f\"Number of days: {num_days}\")\n",
    "\n",
    "    is_first = 1\n",
    "\n",
    "    #Construct the vector that holds all data values (valid and missing)\n",
    "    #Assumes that all vectors in data_vectors_list match on date start\n",
    "    #data are organized by day, target station, other stations\n",
    "    for i_day in range(num_days):\n",
    "        i_day_hour_end = (i_day + 1)*nhours - 1\n",
    "        i_day_hour_start = i_day_hour_end - (nhours -1)\n",
    "        \n",
    "        #Target Station\n",
    "        va2 = data_vectors_list[0][(i_day_hour_start):(i_day_hour_end)]  #AT\n",
    "        va1 = data_vectors_list[1][(i_day_hour_start):(i_day_hour_end)]  #PR\n",
    "        va3 = data_vectors_list[2][i_day:i_day]                          #RH1\n",
    "        va4 = data_vectors_list[3][i_day:i_day]                          #RH2\n",
    "        va5 = data_vectors_list[4][i_day:i_day]                          #RH3\n",
    "        #Station 1\n",
    "        vb2 = data_vectors_list[5][(i_day_hour_start):(i_day_hour_end)]  #AT\n",
    "        vb1 = data_vectors_list[6][(i_day_hour_start):(i_day_hour_end)]  #PR\n",
    "        vb3 = data_vectors_list[7][i_day:i_day]                          #RH1\n",
    "        vb4 = data_vectors_list[8][i_day:i_day]                          #RH2\n",
    "        vb5 = data_vectors_list[9][i_day:i_day]                          #RH3\n",
    "        #Station 2\n",
    "        vc2 = data_vectors_list[10][(i_day_hour_start):(i_day_hour_end)]  #AT\n",
    "        vc1 = data_vectors_list[11][(i_day_hour_start):(i_day_hour_end)]  #PR\n",
    "        vc3 = data_vectors_list[12][i_day:i_day]                          #RH1\n",
    "        vc4 = data_vectors_list[13][i_day:i_day]                          #RH2\n",
    "        vc5 = data_vectors_list[14][i_day:i_day]                          #RH3\n",
    "        #Station 3\n",
    "        vd2 = data_vectors_list[15][(i_day_hour_start):(i_day_hour_end)]  #AT\n",
    "        vd1 = data_vectors_list[16][(i_day_hour_start):(i_day_hour_end)]  #PR\n",
    "        vd3 = data_vectors_list[17][i_day:i_day]                          #RH1\n",
    "        vd4 = data_vectors_list[18][i_day:i_day]                          #RH2\n",
    "        vd5 = data_vectors_list[19][i_day:i_day]                          #RH3\n",
    "            \n",
    "\n",
    "        #vmt_full set = np.concatenate((vmt_one_day,va,vb,vc,vd,ve)) \n",
    "        #print(i_day)\n",
    "        if is_first == 0:\n",
    "            vmt_full_set = np.concatenate((vmt_full_set,va1,va2,va3,va4,va5,vb1,vb2,vb3,vb4,vb5,vc1,vc2,vc3,vc4,vc5,vd1,vd2,vd3,vd4,vd5))\n",
    "            #print(len(vmt_full_set))\n",
    "        else:\n",
    "            is_first = 0\n",
    "            vmt_full_set = np.concatenate((va1,va2,va3,va4,va5,vb1,vb2,vb3,vb4,vb5,vc1,vc2,vc3,vc4,vc5,vd1,vd2,vd3,vd4,vd5))\n",
    "            #print(len(vmt_full_set))\n",
    "\n",
    "    print(f\"Length of data vector: {len(vmt_full_set)}\")\n",
    "\n",
    "\n",
    "    #Organize the data vector into an array which will be returned to the function call\n",
    "    #Create an empty matrix in which data are organized\n",
    "    #nstations = 4\n",
    "    nrows = nyears_row*ndays\n",
    "    ncols = nstations*nyears_col*ndays*ndaily_metrics\n",
    "    nrow_days =  nyears_row*ndays\n",
    "    ncol_days =  nyears_col*ndays\n",
    "    data_limit = len(vmt_full_set)\n",
    "    #print(f\"Number of columns in matrix: {ncols}\")\n",
    "    #print(f\"Limit of data: {data_limit}\")\n",
    "\n",
    "    #Make template matrix to house data\n",
    "    template_matrix = np.zeros((nrows, ncols), dtype=np.float16)\n",
    "\n",
    "    # Fill the matrix\n",
    "    for i in range(nrow_days):\n",
    "        vc_start = i*(ndaily_metrics)\n",
    "        vc_end = ncols + vc_start\n",
    "            \n",
    "        if vc_end < data_limit:\n",
    "            template_matrix[i] = vmt_full_set[(vc_start):(vc_end)]\n",
    "\n",
    "            \n",
    "    return template_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = {\n",
    "#     'st_id': [72508014740,72508014740,72508014740, 72504094702,72504094702,72504094702,72510094746,72510094746,72510094746,72505004781,72505004781,72505004781],\n",
    "#     'target_station': [72508014740,72508014740,72508014740,72508014740,72508014740,72508014740,72508014740,72508014740,72508014740,72508014740,72508014740,72508014740],\n",
    "#     'fnames': ['72508014740_model_data_airtemp.csv', '72508014740_model_data_precip.csv', '72508014740_model_data_relhum.csv',\n",
    "#         '72504094702_model_data_airtemp.csv', '72504094702_model_data_precip.csv', '72504094702_model_data_relhum.csv',\n",
    "#         '72510094746_model_data_airtemp.csv', '72510094746_model_data_precip.csv', '72510094746_model_data_relhum.csv',\n",
    "#         '72505004781_model_data_airtemp.csv', '72505004781_model_data_precip.csv', '72505004781_model_data_relhum.csv'],\n",
    "#     'path': ['D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\'],\n",
    "#     'vect_close_bkt2': [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4]\n",
    "# }\n",
    "\n",
    "# dt_analysis_set_keys = pd.DataFrame(data)\n",
    "# dt_analysis_set_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jj = fn_make_matrix_pr04(dt_analysis_set_keys,4,ipd02,nhours,ndays,nyears_row,nyears_col)\n",
    "# jj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt_analysis_set_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_make_matrix_pr02(stid_keys,nstations,ipd,nhours,ndays,nyears_row,nyears_col):\n",
    "\n",
    "    #Create a matrix for precipitation data that feeds\n",
    "    #The S4 model\n",
    "    # 1. Read in all data files for multiple stations\n",
    "    # 2. Find the common starting date that will be used to align the matrices\n",
    "    # 3. Keep only the values that fit the common starting date\n",
    "    # 4. Combine the data into one large matrix for the StationID \n",
    "\n",
    "    # 1. Read in data files and get start date v_date... for each\n",
    "    # Check that data exist for all metrics\n",
    "\n",
    "    #Create two objects of length 12\n",
    "    #data_list which contains the metrics for stations 1-4, the first is the target\n",
    "    #data_start_date which contains the most recent date for each file\n",
    "    vc_first_rcd = [0,3,6,9]\n",
    "    num_stations = len(vc_first_rcd)\n",
    "    vc_st_id = stid_keys.iloc[:,0]\n",
    "    vc_path = stid_keys.iloc[:,3]\n",
    "    vc_fnames = stid_keys.iloc[:,2]\n",
    "\n",
    "    data_list = []\n",
    "    data_start_date = []\n",
    "\n",
    "    for i in range(0, num_stations):\n",
    "            #Air temperature (one variable recorded once an hour a day)\n",
    "            infile = vc_path[vc_first_rcd[i]] + \"\\\\\" + vc_fnames[vc_first_rcd[i]]\n",
    "            df_at = pd.read_csv(infile)\n",
    "            data_list.append(df_at)\n",
    "            dt_first = datetime.strptime(df_at.iloc[0,2], \"%Y-%m-%d\").date()\n",
    "            data_start_date.append(dt_first)\n",
    "\n",
    "            #Precipitation (one variable recorded once an hour a day)\n",
    "            infile = vc_path[vc_first_rcd[i]] + \"\\\\\" + vc_fnames[vc_first_rcd[i]+1] \n",
    "            df_pr = pd.read_csv(infile)\n",
    "            data_list.append(df_pr)\n",
    "            dt_first = datetime.strptime(df_pr.iloc[0,2], \"%Y-%m-%d\").date()\n",
    "            data_start_date.append(dt_first)\n",
    "            \n",
    "            #Relative Humidity (3 variables recorded once a day)\n",
    "            infile = vc_path[vc_first_rcd[i]] + \"\\\\\" + vc_fnames[vc_first_rcd[i]+2]\n",
    "            df_rh = pd.read_csv(infile)\n",
    "            data_list.append(df_rh)\n",
    "            dt_first = datetime.strptime(df_rh.iloc[0,4], \"%Y-%m-%d\").date()\n",
    "            data_start_date.append(dt_first)\n",
    "\n",
    "\n",
    "    # 2. Find the offsets needed to all datasets start at same starting date\n",
    "    v_date_minimum = min(data_start_date)\n",
    "\n",
    "    #Identify offsets to use so all datadata vectors are appropriately aligned \n",
    "    upper_range = len(data_start_date)\n",
    "\n",
    "    date_offsets_list = []\n",
    "    for i in range(0, upper_range):\n",
    "        date_offset = data_start_date[i] - v_date_minimum\n",
    "        #Temporarily set to 0, revise later\n",
    "        date_offset = 0\n",
    "        date_offsets_list.append(date_offset)\n",
    "\n",
    "    # 3. Get vectors of data for each measure\n",
    "    #These vectors all start on the same date\n",
    "\n",
    "    data_vectors_list = []\n",
    "    for i in vc_first_rcd:\n",
    "        tdt = data_list[i]\n",
    "        tof = date_offsets_list[i]\n",
    "        vc_at1 = tdt.iloc[date_offsets_list[i]:(len(tdt.iloc[:,1])-tof),1]\n",
    "        data_vectors_list.append(vc_at1)\n",
    "            \n",
    "        tdt = data_list[i+1]\n",
    "        tof = date_offsets_list[i+1]\n",
    "        vc_pr1 = tdt.iloc[date_offsets_list[i+1]:(len(tdt.iloc[:,1])-tof),1]\n",
    "        data_vectors_list.append(vc_pr1)\n",
    "            \n",
    "        tdt = data_list[i+2]\n",
    "        tof = date_offsets_list[i+2]\n",
    "        vc_rh1 = tdt.iloc[date_offsets_list[i+2]:(len(tdt.iloc[:,1])-tof),1]\n",
    "        data_vectors_list.append(vc_rh1)\n",
    "\n",
    "        tdt = data_list[i+2]\n",
    "        tof = date_offsets_list[i+2]\n",
    "        vc_rh2 = tdt.iloc[date_offsets_list[i+2]:(len(tdt.iloc[:,1])-tof),2]\n",
    "        data_vectors_list.append(vc_rh2)\n",
    "\n",
    "        tdt = data_list[i+2]\n",
    "        tof = date_offsets_list[i+2]\n",
    "        vc_rh3 = tdt.iloc[date_offsets_list[i+2]:(len(tdt.iloc[:,1])-tof),3]\n",
    "        data_vectors_list.append(vc_rh3)\n",
    "\n",
    "    \n",
    "\n",
    "    # 4. Interleave vectors so we get a complete vector of all metrics\n",
    "    #Start with the once a day vectors and loop over the number of days as defined by once a day vector\n",
    "    #These set how many years of data are needed : nyears_row + nyears_col\n",
    "    nyears_data_limit = nyears_row + nyears_col + 1\n",
    "    nrows = nhours*ndays*nyears_row\n",
    "    ncols = nhours*ndays*nyears_col \n",
    "        \n",
    "    # set the number of metrics created each day\n",
    "    # Metrics measured at hourly intervals are put into day metrics\n",
    "    # And ordered from most recent hour to most distant hour from left to right\n",
    "    # Day metrics are also ordered most recent day to most distant day left to right\n",
    "    ndaily_metrics = 2*nhours + 3*1\n",
    "\n",
    "    #Set the total number of days of data needed for the model\n",
    "    num_days = nyears_data_limit*ndays\n",
    "    #print(f\"Number of days: {num_days}\")\n",
    "\n",
    "    is_first = 1\n",
    "\n",
    "    #Construct the vector that holds all data values (valid and missing)\n",
    "    #Assumes that all vectors in data_vectors_list match on date start\n",
    "    #data are organized by day, target station, other stations\n",
    "    for i_day in range(num_days):\n",
    "        i_day_hour_end = (i_day + 1)*nhours - 1\n",
    "        i_day_hour_start = i_day_hour_end - (nhours -1)\n",
    "            \n",
    "        va2 = data_vectors_list[0][(i_day_hour_start):(i_day_hour_end)]  #AT\n",
    "        va1 = data_vectors_list[1][(i_day_hour_start):(i_day_hour_end)]  #PR\n",
    "        va3 = data_vectors_list[2][i_day:i_day]                          #RH1\n",
    "        va4 = data_vectors_list[3][i_day:i_day]                          #RH2\n",
    "        va5 = data_vectors_list[4][i_day:i_day]                          #RH3\n",
    "\n",
    "        vb2 = data_vectors_list[5][(i_day_hour_start):(i_day_hour_end)]  #AT\n",
    "        vb1 = data_vectors_list[6][(i_day_hour_start):(i_day_hour_end)]  #PR\n",
    "        vb3 = data_vectors_list[7][i_day:i_day]                          #RH1\n",
    "        vb4 = data_vectors_list[8][i_day:i_day]                          #RH2\n",
    "        vb5 = data_vectors_list[9][i_day:i_day]                          #RH3\n",
    "\n",
    "        vc2 = data_vectors_list[10][(i_day_hour_start):(i_day_hour_end)]  #AT\n",
    "        vc1 = data_vectors_list[11][(i_day_hour_start):(i_day_hour_end)]  #PR\n",
    "        vc3 = data_vectors_list[12][i_day:i_day]                          #RH1\n",
    "        vc4 = data_vectors_list[13][i_day:i_day]                          #RH2\n",
    "        vc5 = data_vectors_list[14][i_day:i_day]                          #RH3\n",
    "\n",
    "        vd2 = data_vectors_list[15][(i_day_hour_start):(i_day_hour_end)]  #AT\n",
    "        vd1 = data_vectors_list[16][(i_day_hour_start):(i_day_hour_end)]  #PR\n",
    "        vd3 = data_vectors_list[17][i_day:i_day]                          #RH1\n",
    "        vd4 = data_vectors_list[18][i_day:i_day]                          #RH2\n",
    "        vd5 = data_vectors_list[19][i_day:i_day]                          #RH3\n",
    "            \n",
    "\n",
    "        #vmt_full set = np.concatenate((vmt_one_day,va,vb,vc,vd,ve)) \n",
    "        #print(i_day)\n",
    "        if is_first == 0:\n",
    "            vmt_full_set = np.concatenate((vmt_full_set,va1,va2,va3,va4,va5,vb1,vb2,vb3,vb4,vb5,vc1,vc2,vc3,vc4,vc5,vd1,vd2,vd3,vd4,vd5))\n",
    "            #print(len(vmt_full_set))\n",
    "        else:\n",
    "            is_first = 0\n",
    "            vmt_full_set = np.concatenate((va1,va2,va3,va4,va5,vb1,vb2,vb3,vb4,vb5,vc1,vc2,vc3,vc4,vc5,vd1,vd2,vd3,vd4,vd5))\n",
    "            #print(len(vmt_full_set))\n",
    "\n",
    "    print(f\"Length of data vector: {len(vmt_full_set)}\")\n",
    "\n",
    "\n",
    "    #Organize the data vector into an array which will be returned to the function call\n",
    "    #Create an empty matrix in which data are organized\n",
    "    nstations = 4\n",
    "    nrows = nyears_row*ndays\n",
    "    ncols = nstations*nyears_col*ndays*ndaily_metrics\n",
    "    nrow_days =  nyears_row*ndays\n",
    "    ncol_days =  nyears_col*ndays\n",
    "    data_limit = len(vmt_full_set)\n",
    "    #print(f\"Number of columns in matrix: {ncols}\")\n",
    "    #print(f\"Limit of data: {data_limit}\")\n",
    "\n",
    "    #Make template matrix to house data\n",
    "    template_matrix = np.zeros((nrows, ncols), dtype=np.float16)\n",
    "\n",
    "    # Fill the matrix\n",
    "    for i in range(nrow_days):\n",
    "        vc_start = i*(ndaily_metrics)\n",
    "        vc_end = ncols + vc_start\n",
    "            \n",
    "        if vc_end < data_limit:\n",
    "            template_matrix[i] = vmt_full_set[(vc_start):(vc_end)]\n",
    "\n",
    "        #   print(vc_start)\n",
    "        #   print(vc_end)\n",
    "        #   print(len(vmt_full_set[(vc_start):(vc_end)]))\n",
    "            \n",
    "    return template_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "st_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "target_station",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "fnames",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "vect_close_bkt2",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "fad3f276-2953-412f-bd6c-aa303eb5c43b",
       "rows": [
        [
         "120",
         "72508014740",
         "72508014740",
         "72508014740_model_data_airtemp.csv",
         "D:\\CodeLibrary\\Python\\weathermetrics\\data\\weathermetrics\\",
         "1"
        ],
        [
         "121",
         "72508014740",
         "72508014740",
         "72508014740_model_data_precip.csv",
         "D:\\CodeLibrary\\Python\\weathermetrics\\data\\weathermetrics\\",
         "1"
        ],
        [
         "122",
         "72508014740",
         "72508014740",
         "72508014740_model_data_relhum.csv",
         "D:\\CodeLibrary\\Python\\weathermetrics\\data\\weathermetrics\\",
         "1"
        ],
        [
         "123",
         "72504094702",
         "72508014740",
         "72504094702_model_data_airtemp.csv",
         "D:\\CodeLibrary\\Python\\weathermetrics\\data\\weathermetrics\\",
         "3"
        ],
        [
         "124",
         "72504094702",
         "72508014740",
         "72504094702_model_data_precip.csv",
         "D:\\CodeLibrary\\Python\\weathermetrics\\data\\weathermetrics\\",
         "3"
        ],
        [
         "125",
         "72504094702",
         "72508014740",
         "72504094702_model_data_relhum.csv",
         "D:\\CodeLibrary\\Python\\weathermetrics\\data\\weathermetrics\\",
         "3"
        ],
        [
         "126",
         "72510094746",
         "72508014740",
         "72510094746_model_data_airtemp.csv",
         "D:\\CodeLibrary\\Python\\weathermetrics\\data\\weathermetrics\\",
         "3"
        ],
        [
         "127",
         "72510094746",
         "72508014740",
         "72510094746_model_data_precip.csv",
         "D:\\CodeLibrary\\Python\\weathermetrics\\data\\weathermetrics\\",
         "3"
        ],
        [
         "128",
         "72510094746",
         "72508014740",
         "72510094746_model_data_relhum.csv",
         "D:\\CodeLibrary\\Python\\weathermetrics\\data\\weathermetrics\\",
         "3"
        ],
        [
         "129",
         "72505004781",
         "72508014740",
         "72505004781_model_data_airtemp.csv",
         "D:\\CodeLibrary\\Python\\weathermetrics\\data\\weathermetrics\\",
         "4"
        ],
        [
         "130",
         "72505004781",
         "72508014740",
         "72505004781_model_data_precip.csv",
         "D:\\CodeLibrary\\Python\\weathermetrics\\data\\weathermetrics\\",
         "4"
        ],
        [
         "131",
         "72505004781",
         "72508014740",
         "72505004781_model_data_relhum.csv",
         "D:\\CodeLibrary\\Python\\weathermetrics\\data\\weathermetrics\\",
         "4"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 12
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>st_id</th>\n",
       "      <th>target_station</th>\n",
       "      <th>fnames</th>\n",
       "      <th>path</th>\n",
       "      <th>vect_close_bkt2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>72508014740</td>\n",
       "      <td>72508014740</td>\n",
       "      <td>72508014740_model_data_airtemp.csv</td>\n",
       "      <td>D:\\CodeLibrary\\Python\\weathermetrics\\data\\weat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>72508014740</td>\n",
       "      <td>72508014740</td>\n",
       "      <td>72508014740_model_data_precip.csv</td>\n",
       "      <td>D:\\CodeLibrary\\Python\\weathermetrics\\data\\weat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>72508014740</td>\n",
       "      <td>72508014740</td>\n",
       "      <td>72508014740_model_data_relhum.csv</td>\n",
       "      <td>D:\\CodeLibrary\\Python\\weathermetrics\\data\\weat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>72504094702</td>\n",
       "      <td>72508014740</td>\n",
       "      <td>72504094702_model_data_airtemp.csv</td>\n",
       "      <td>D:\\CodeLibrary\\Python\\weathermetrics\\data\\weat...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>72504094702</td>\n",
       "      <td>72508014740</td>\n",
       "      <td>72504094702_model_data_precip.csv</td>\n",
       "      <td>D:\\CodeLibrary\\Python\\weathermetrics\\data\\weat...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>72504094702</td>\n",
       "      <td>72508014740</td>\n",
       "      <td>72504094702_model_data_relhum.csv</td>\n",
       "      <td>D:\\CodeLibrary\\Python\\weathermetrics\\data\\weat...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>72510094746</td>\n",
       "      <td>72508014740</td>\n",
       "      <td>72510094746_model_data_airtemp.csv</td>\n",
       "      <td>D:\\CodeLibrary\\Python\\weathermetrics\\data\\weat...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>72510094746</td>\n",
       "      <td>72508014740</td>\n",
       "      <td>72510094746_model_data_precip.csv</td>\n",
       "      <td>D:\\CodeLibrary\\Python\\weathermetrics\\data\\weat...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>72510094746</td>\n",
       "      <td>72508014740</td>\n",
       "      <td>72510094746_model_data_relhum.csv</td>\n",
       "      <td>D:\\CodeLibrary\\Python\\weathermetrics\\data\\weat...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>72505004781</td>\n",
       "      <td>72508014740</td>\n",
       "      <td>72505004781_model_data_airtemp.csv</td>\n",
       "      <td>D:\\CodeLibrary\\Python\\weathermetrics\\data\\weat...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>72505004781</td>\n",
       "      <td>72508014740</td>\n",
       "      <td>72505004781_model_data_precip.csv</td>\n",
       "      <td>D:\\CodeLibrary\\Python\\weathermetrics\\data\\weat...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>72505004781</td>\n",
       "      <td>72508014740</td>\n",
       "      <td>72505004781_model_data_relhum.csv</td>\n",
       "      <td>D:\\CodeLibrary\\Python\\weathermetrics\\data\\weat...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           st_id  target_station                              fnames  \\\n",
       "120  72508014740     72508014740  72508014740_model_data_airtemp.csv   \n",
       "121  72508014740     72508014740   72508014740_model_data_precip.csv   \n",
       "122  72508014740     72508014740   72508014740_model_data_relhum.csv   \n",
       "123  72504094702     72508014740  72504094702_model_data_airtemp.csv   \n",
       "124  72504094702     72508014740   72504094702_model_data_precip.csv   \n",
       "125  72504094702     72508014740   72504094702_model_data_relhum.csv   \n",
       "126  72510094746     72508014740  72510094746_model_data_airtemp.csv   \n",
       "127  72510094746     72508014740   72510094746_model_data_precip.csv   \n",
       "128  72510094746     72508014740   72510094746_model_data_relhum.csv   \n",
       "129  72505004781     72508014740  72505004781_model_data_airtemp.csv   \n",
       "130  72505004781     72508014740   72505004781_model_data_precip.csv   \n",
       "131  72505004781     72508014740   72505004781_model_data_relhum.csv   \n",
       "\n",
       "                                                  path  vect_close_bkt2  \n",
       "120  D:\\CodeLibrary\\Python\\weathermetrics\\data\\weat...                1  \n",
       "121  D:\\CodeLibrary\\Python\\weathermetrics\\data\\weat...                1  \n",
       "122  D:\\CodeLibrary\\Python\\weathermetrics\\data\\weat...                1  \n",
       "123  D:\\CodeLibrary\\Python\\weathermetrics\\data\\weat...                3  \n",
       "124  D:\\CodeLibrary\\Python\\weathermetrics\\data\\weat...                3  \n",
       "125  D:\\CodeLibrary\\Python\\weathermetrics\\data\\weat...                3  \n",
       "126  D:\\CodeLibrary\\Python\\weathermetrics\\data\\weat...                3  \n",
       "127  D:\\CodeLibrary\\Python\\weathermetrics\\data\\weat...                3  \n",
       "128  D:\\CodeLibrary\\Python\\weathermetrics\\data\\weat...                3  \n",
       "129  D:\\CodeLibrary\\Python\\weathermetrics\\data\\weat...                4  \n",
       "130  D:\\CodeLibrary\\Python\\weathermetrics\\data\\weat...                4  \n",
       "131  D:\\CodeLibrary\\Python\\weathermetrics\\data\\weat...                4  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#STEP 1.  Select a station using ID to get its related stations\n",
    "# Get file of target stations and their related stations\n",
    "ipd01 = \"\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\2024\\\\\"\n",
    "ipd02 = \"D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\\"\n",
    "dt_trg_set = \"D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\Target_station_analysis_set.csv\"\n",
    "lst_target_stations = [\n",
    "  \"74486094789\",\n",
    "  \"72518014735\",\n",
    "  \"72508014740\",\n",
    "  \"72526014860\",\n",
    "  \"72406093721\",\n",
    "  \"72401013740\",\n",
    "  \"72219013874\",\n",
    "  \"72423093821\",\n",
    "  \"72428014821\",\n",
    "  \"72327013897\"\n",
    "]\n",
    "\n",
    "#Select the target station for which predictions will be made\n",
    "#72508014740 is JFK airport\n",
    "st_id = lst_target_stations[2]\n",
    "num_st_keep = 4   #num of stations to keep for analysis\n",
    "analysis_set = dt_trg_set\n",
    "dt_analysis_set = pd.read_csv(analysis_set)\n",
    "dt_analysis_set['path'] = ipd02\n",
    "dt01 = dt_analysis_set[dt_analysis_set['target_station']==int(st_id)]\n",
    "#uniq_stat = dt01['vect_close_bkt2'].unique()[:2]\n",
    "uniq_station = dt01['st_id'].unique()[:num_st_keep]\n",
    "dt02 = dt01[dt01['st_id'].isin(uniq_station)]\n",
    "\n",
    "selected_columns = ['st_id','target_station',\n",
    "                    'fnames','path','vect_close_bkt2']\n",
    "#The output below is fed to STEP 2 to run the analysis\n",
    "dt_analysis_set_keys = dt02[selected_columns].copy()\n",
    "\n",
    "dt_analysis_set_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\CodeLibrary\\Python\\weathermetrics\\data\\weathermetrics\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prajw\\AppData\\Local\\Temp\\ipykernel_19060\\294075795.py:28: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_at = pd.read_csv(infile)\n",
      "C:\\Users\\prajw\\AppData\\Local\\Temp\\ipykernel_19060\\294075795.py:36: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_pr = pd.read_csv(infile)\n",
      "C:\\Users\\prajw\\AppData\\Local\\Temp\\ipykernel_19060\\294075795.py:28: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_at = pd.read_csv(infile)\n",
      "C:\\Users\\prajw\\AppData\\Local\\Temp\\ipykernel_19060\\294075795.py:36: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_pr = pd.read_csv(infile)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-05\n",
      "Length of data vector: 940240\n",
      "(3650, 223380)\n"
     ]
    }
   ],
   "source": [
    "#STEP 2.  Use elements created in STEP 1 to get a data matrix for analysis\n",
    "#All the available stations which have all three metrics needed for model\n",
    "#vc_stid = dt_analysis_set_keys[['st_id']].drop_duplicates()\n",
    "\n",
    "#Organize the data based on the nature of the data so they all conform to the same size matrices\n",
    "#The metrics included are\n",
    "#  AT - air temperature in Celcius\n",
    "#  RH - relative humidity in Percent\n",
    "#  PR - precipitation in MM\n",
    "\n",
    "#Put them all in appropriate matrices and create list of metrics\n",
    "#Data files are of two types\n",
    "#    -  24 hour * 365 days * n years  (AT, PR)\n",
    "#    -  365 days * n years (RH)\n",
    "\n",
    "#Parameters governing data matrix\n",
    "nhours = 24\n",
    "ndays = 365\n",
    "nyears_row = 10\n",
    "nyears_col = 3\n",
    "nstations = num_st_keep\n",
    "\n",
    "print(ipd02)\n",
    "rtn_matrix = fn_make_matrix_pr04(dt_analysis_set_keys,nstations,ipd02,nhours,ndays,nyears_row,nyears_col)\n",
    "print(rtn_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_make_matrix(vc_measure,offset_measure,nhours,ndays,nyears_col,nyears_row):\n",
    "    #the offset from the start of the vector to align the data points\n",
    "    vector = vc_measure\n",
    "\n",
    "    nrows = nhours*ndays*nyears_row   #the number of values put in each row of matrix days*hours*years\n",
    "    ncols = nhours*ndays*nyears_col   #the number of values put in each row of matrix days*hours*years\n",
    "    \n",
    "    # Create an empty matrix to store the results\n",
    "    matrix = np.zeros((nrows, ncols))\n",
    "\n",
    "    # Fill the matrix\n",
    "    for i in range(nrows):\n",
    "        matrix[i] = vector[(offset_measure + i):(offset_measure + i + ncols)]\n",
    "     \n",
    "    return matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  (3616, 223380)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0\n",
       "0    2522\n",
       "1    1094\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform_matrix(df, bins, labels):\n",
    "    \"\"\"\n",
    "    Transforms the given matrix by binning the first column, dropping the first row and column,\n",
    "    and adding the binned column as the first column.\n",
    "\n",
    "    Parameters:\n",
    "        matrix (pd.DataFrame): The input matrix.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The transformed matrix.\n",
    "    \"\"\"\n",
    "    # Extract the first column (dependent variable)\n",
    "    firstcolumn = df.iloc[:, 0].astype('float32')\n",
    "\n",
    "    # Bin the first column\n",
    "    binned_column = pd.cut(firstcolumn, bins=bins, labels=labels)\n",
    "\n",
    "    # Drop the first first column\n",
    "    df = df.iloc[:, 1:]\n",
    "\n",
    "    # Add the binned column as the first column\n",
    "    df.insert(0, 0, binned_column.iloc[0:].values)\n",
    "\n",
    "    # Return the final matrix\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "# Example matrix (replace this with your actual matrix)\n",
    "jj   = rtn_matrix\n",
    "df = pd.DataFrame(jj).iloc[1:,] # Convert to DataFrame and drop the first row for binning\n",
    "# Drop rows where the first column has missing values\n",
    "# df = df.dropna(subset=[0])\n",
    "fc = df.iloc[:,0].astype('float32') # Ensure the first column is float for binning\n",
    "bins = [-float('inf'), 10, float('inf')]\n",
    "# bins = [-float('inf'), 0] + list(pd.qcut(fc[fc > 0], q=4, retbins=True)[1][1:]) + [float('inf')]\n",
    "labels = [0, 1]\n",
    "# print(bins, labels)\n",
    "\n",
    "df_transformed = transform_matrix(df, bins, labels)\n",
    "df_transformed = df_transformed[df_transformed[0]>= 0]\n",
    "print(\"Dataset: \", df_transformed.shape)\n",
    "df_transformed[0].value_counts(dropna=False).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: (2532, 223380)\n",
      "0\n",
      "0    1810\n",
      "1     722\n",
      "Name: count, dtype: int64\n",
      "Validation Set: (361, 223380)\n",
      "0\n",
      "0    258\n",
      "1    103\n",
      "Name: count, dtype: int64\n",
      "Test Set: (723, 223380)\n",
      "0\n",
      "0    454\n",
      "1    269\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def split_time_series_data(df, train_ratio=0.7, val_ratio=0.1, test_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Splits a time series dataset into training, validation, and test sets.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input time series dataset, ordered in descending dates.\n",
    "        train_ratio (float): The proportion of data to use for training.\n",
    "        val_ratio (float): The proportion of data to use for validation.\n",
    "        test_ratio (float): The proportion of data to use for testing.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing three DataFrames (train, validation, test).\n",
    "    \"\"\"\n",
    "    assert train_ratio + val_ratio + test_ratio == 1.0, \"Ratios must sum to 1.0\"\n",
    "\n",
    "    # Calculate split indices\n",
    "    n = len(df)\n",
    "    test_end = int(n * test_ratio)    \n",
    "    val_end = test_end + int(n * val_ratio)\n",
    "    \n",
    "\n",
    "    # Split the data\n",
    "    test_data = df.iloc[:test_end].sort_index(ascending=False)\n",
    "    val_data = df.iloc[test_end:val_end].sort_index(ascending=False)\n",
    "    train_data = df.iloc[val_end:].sort_index(ascending=False)\n",
    "\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "# Example usage\n",
    "train_data, val_data, test_data = split_time_series_data(df_transformed)\n",
    "print(\"Training Set:\", train_data.shape)\n",
    "print(train_data[0].value_counts(dropna=False).sort_index())\n",
    "print(\"Validation Set:\", val_data.shape)\n",
    "print(val_data[0].value_counts(dropna=False).sort_index())\n",
    "print(\"Test Set:\", test_data.shape)\n",
    "print(test_data[0].value_counts(dropna=False).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets exported successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the station ID and output directory\n",
    "station_id = \"72508014740\"\n",
    "output_dir = \"D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\\"\n",
    "\n",
    "\n",
    "\n",
    "# Export datasets to CSV\n",
    "train_data.to_parquet(f\"{output_dir}{station_id}_train.parquet\", index=False)\n",
    "val_data.to_parquet(f\"{output_dir}{station_id}_validation.parquet\", index=False)\n",
    "test_data.to_parquet(f\"{output_dir}{station_id}_test.parquet\", index=False)\n",
    "\n",
    "# train_data.to_csv(f\"{output_dir}{station_id}_train.csv\", index=False)\n",
    "# val_data.to_csv(f\"{output_dir}{station_id}_validation.csv\", index=False)\n",
    "# test_data.to_csv(f\"{output_dir}{station_id}_test.csv\", index=False)\n",
    "\n",
    "print(\"Datasets exported successfully.\")\n",
    "\n",
    "#python -m s4model --modelname 72508014740 --trainset ../data/weathermetrics/72508014740_train.csv --valset ../data/weathermetrics/72508014740_validation.csv --testset ../data/weathermetrics/72508014740_test.csv --tabulardata --dependent_variable 0 --epochs 30\n",
    "\n",
    "#python -m s4model --modelname 72508014740 --trainset ../data/weathermetrics/72508014740_train.parquet --valset ../data/weathermetrics/72508014740_validation.parquet --testset ../data/weathermetrics/72508014740_test.parquet --tabulardata --dependent_variable 0 --epochs 30\n",
    "\n",
    "# python -m charts --df ../results/72508014740_Test_results_20250424_104121PM.csv --actual 0 --predicted Predicted \n",
    "# python -m charts --df ../results/72508014740_Test_results_20250427_100132PM.csv --actual 0 --predicted Predicted \n",
    "# python -m charts --df ../results/72508014740_Test_results_20250428_091259PM.csv --actual 0 --predicted Predicted \n",
    "# python -m charts --df ../results/72508014740_Test_results_20250428_094033PM.csv --actual 0 --predicted Predicted \n",
    "# python -m charts --df ../results/72508014740_Test_results_20250429_091538PM.csv --actual 0 --predicted Predicted\n",
    "# python -m charts --df ../results/72508014740_Test_results_20250504_085953PM.csv --actual 0 --predicted Predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_first_non_nan(vector):\n",
    "#     return next((i for i, x in enumerate(vector) if not math.isnan(x)), None)\n",
    "\n",
    "# first_number_at1 = find_first_non_nan(vc_at1)\n",
    "# first_number_rh1 = find_first_non_nan(vc_rh1)\n",
    "# first_number_rh2 = find_first_non_nan(vc_rh2)\n",
    "# first_number_rh3 = find_first_non_nan(vc_rh3)\n",
    "# first_number_pr1 = find_first_non_nan(vc_pr1)\n",
    "\n",
    "# #Number of missing data points\n",
    "# print(f\"AT1 Number of missing cells: {np.count_nonzero(np.isnan(vc_at1))}\")\n",
    "# print(f\"AT1 Vector Length: {len(vc_at1)}\")\n",
    "\n",
    "# print(f\"RH1 Number of missing cells: {np.count_nonzero(np.isnan(vc_rh1))}\")\n",
    "# print(f\"RH1 Vector Length: {len(vc_rh1)}\")\n",
    "\n",
    "# print(f\"PR1 Number of missing cells: {np.count_nonzero(np.isnan(vc_pr1))}\")\n",
    "# print(f\"PR1 Vector Length: {len(vc_pr1)}\")\n",
    "\n",
    "# #Once a day metrics\n",
    "# v_data_first_once_day = max(\n",
    "#                    first_number_rh1,\n",
    "#                    first_number_rh2,\n",
    "#                    first_number_rh3)\n",
    "# print(v_data_first_once_day)\n",
    "\n",
    "# #24 times a day metrics\n",
    "# v_data_first_24h_day = max(first_number_at1,\n",
    "#                    first_number_pr1)\n",
    "# print(v_data_first_24h_day)\n",
    "\n",
    "\n",
    "#These vectors all start with a numeric value so missing values can be filled\n",
    "\n",
    "\n",
    "#print(first_number_at1)\n",
    "#print(first_number_rh1)\n",
    "#print(first_number_rh2)\n",
    "#print(first_number_rh3)\n",
    "#print(first_number_pr1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4380,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(jj)\n",
    "fc = df.iloc[:,0].astype('float32')\n",
    "print(fc.shape)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(fc, bins=50, color='blue', alpha=0.7, density=True)\n",
    "plt.title('Normalized Histogram of a')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: -15.8515625\n",
      "Max: 35.59375\n",
      "Percentiles: [-15.8515625    0.           0.           0.           0.\n",
      "   0.30004883   5.          10.6015625   16.703125    22.203125\n",
      "  35.59375   ]\n"
     ]
    }
   ],
   "source": [
    "# Custom bins\n",
    "\n",
    "a = fc[~fc.isnull()]\n",
    "\n",
    "# Calculate min, max, and percentiles by 10 for 'a'\n",
    "min_val = a.min()\n",
    "max_val = a.max()\n",
    "percentiles = np.percentile(a, np.arange(0, 101, 10))\n",
    "\n",
    "print(f\"Min: {min_val}\")\n",
    "print(f\"Max: {max_val}\")\n",
    "print(f\"Percentiles: {percentiles}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
