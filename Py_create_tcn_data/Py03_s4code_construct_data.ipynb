{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IV Load the data for a station and organize for precipitation model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths\n",
    "#Need to set up based on who is running\n",
    "usr = \"JH\"\n",
    "if usr == \"PK\":\n",
    "    ipd01 = \"\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\2024\\\\\"\n",
    "    ipd02 = \"D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\\"\n",
    "    \n",
    "if usr == \"JH\":\n",
    "    ipd01 = \"\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\2024\\\\\"\n",
    "    ipd02 = \"\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\\"\n",
    "    ipd03 = \"\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\Py_S4\\\\Py_S4_v02_JHH\\\\NCEI_data\\\\\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_select_data_vectors_pr07(stid_keys,nstations,nhours,nmonths_row,nmonths_col):\n",
    "    #Gte the matrix of data to input into marix\n",
    "    #The S4 model\n",
    "    # 1. Read in combined data files for multiple stations\n",
    "    # 2. Find the common dates that will be used to align the matrices\n",
    "    #    Keep only the values that fit the common starting date \n",
    "\n",
    "    # 1. Read in data files and get start date v_date... for each\n",
    "    # Check that data exist for all metrics\n",
    "\n",
    "    df_stid = stid_keys\n",
    "    df_stid2 = df_stid.iloc[:, [0, 3, 4]]  #df_stid[df_stid['target_station'] == stid_target]\n",
    "    df_stid2 = df_stid2.drop_duplicates() # unique()\n",
    "\n",
    "    n_rows_stid = min(len(df_stid2),nstations)\n",
    "    \n",
    "    #Matrix in which measures are stored\n",
    "    n_vmt_rows = nhours*30*max(nmonths_row,nmonths_col)\n",
    "    #vmt_full_set = np.empty(n_vmt_rows,np.float16)\n",
    "    #is_first = 1\n",
    "    \n",
    "    data_list = []\n",
    "    date_max_list =[]\n",
    "\n",
    "    #1. Read in data files\n",
    "    for i in range(0, n_rows_stid):\n",
    "        infile = ipd03 + str(df_stid2.iloc[i,0]) + \"_model_data_combined.csv\"\n",
    "        print(infile)\n",
    "        df_metrics = pd.read_csv(infile)\n",
    "        df_metrics['str_hr'] = df_metrics['nHOUR'].apply(lambda x: f'{x:02d}')\n",
    "        df_metrics['date_hr'] = pd.to_datetime(df_metrics['Date_YYYYMMDD'] + ' ' + df_metrics['str_hr'])\n",
    "        df_metrics['month_number'] = df_metrics['date_hr'].dt.month\n",
    "        df_metrics['week_number'] = df_metrics['date_hr'].dt.isocalendar().week\n",
    "        df_metrics['stid'] = str(df_stid2.iloc[i,0])\n",
    "        data_list.append(df_metrics)\n",
    "\n",
    "    #2. Align dates across data files\n",
    "    for i in range(0, n_rows_stid):\n",
    "        df = data_list[i]\n",
    "        max_date = max(df['date_hr'])     \n",
    "        date_max_list.append(max_date)\n",
    "    \n",
    "    date_filter = min(date_max_list)\n",
    "\n",
    "    for i in range(0, n_rows_stid):\n",
    "        df = data_list[i]\n",
    "        #place holder for filtering to a specific end date\n",
    "        #date_filter = pd.to_datetime(\"2023-8-11\" + ' ' + \"17\")\n",
    "        filtered_df = df[df['date_hr'] < date_filter]\n",
    "        filtered_df = filtered_df.iloc[(len(filtered_df) - n_vmt_rows):len(filtered_df)]\n",
    "        \n",
    "        idx_col = filtered_df.columns\n",
    "        idx_col = str(df_stid2.iloc[i,0]) + '_' + idx_col\n",
    "        filtered_df.columns = idx_col\n",
    "\n",
    "        print(filtered_df.shape)\n",
    "        \n",
    "        data_list[i] = filtered_df\n",
    "        \n",
    "    return data_list[0:n_rows_stid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_create_data_matrix_pr07(lst_data,tgt_metric,nstations,nmetrics,nmonths_col):\n",
    "    #Create a matrix for precipitation data that feeds\n",
    "    #The S4 model\n",
    "    # 1. Get data list and select target metric\n",
    "    #Two choices now\n",
    "    #    - prec\n",
    "    #    - temp\n",
    "    # 2.  Create output and move data into approprate cells\n",
    "\n",
    "    # 1. get data list details and reduce to metrics\n",
    "    data_vectors_list = []\n",
    "    dt_rows = len(lst_data[0])\n",
    "    df00 = []\n",
    "\n",
    "    dt_rows_minus_1 = dt_rows - 1\n",
    "    \n",
    "    if tgt_metric == \"prec\": tgt_cols = [8,2,3,4,5,6,9,10]\n",
    "    if tgt_metric == \"temp\": tgt_cols = [8,3,2,4,5,6,9,10]\n",
    "    \n",
    "    #Reset the indices as all records are ordered by date and hour\n",
    "    for i in range(nstations):\n",
    "        df00.append(lst_data[i].iloc[:,tgt_cols])\n",
    "    \n",
    "    df01 = [df.reset_index(drop=True) for df in df00]\n",
    "    rs = pd.concat(df01, axis=1)\n",
    "    print(rs.shape)\n",
    "    #print(rs.iloc[1:10,:])\n",
    "    #Order from most recent to most distant in time\n",
    "    rs = rs.iloc[::-1]\n",
    "    #print(rs.iloc[1:10,:])\n",
    "    \n",
    "    #2. Create output matrix and move data into appropratie cells\n",
    "    \n",
    "    nmetrics_mon_week = nmetrics + 2\n",
    "    mt_cols = 1 + nstations*(nmetrics_mon_week)*nmonths_col*30\n",
    "    mt_rows = len(rs) #- nmonths_col*30*24 - 1\n",
    "    \n",
    "    mt_full_set = np.empty((mt_rows,mt_cols ),np.float16)\n",
    "    \n",
    "    #rng = mt_rows\n",
    "    #Set up first column set of data\n",
    "    #tgt_cols = [1,2,3,4,5,7,8,9,10,11,13,14,15,16,17,19,20,21,22,23]\n",
    "    tgt_cols = [1,2,3,4,5,6,7, 9,10,11,12,13,14,15,17,18,19,20,21,22,23,25,26,27,28,29,30,31]\n",
    "    tgt_cols_num = len(tgt_cols)\n",
    "    for i in range(0,mt_rows):\n",
    "        mt_full_set[i,range(0,tgt_cols_num)] = rs.iloc[i,tgt_cols]\n",
    "    \n",
    "\n",
    "    #Add additional column sets\n",
    "    nc = nstations*nmetrics_mon_week\n",
    "    lpi = mt_rows - nmonths_col*30\n",
    "    #limit of rows in data set so all marix rows have data\n",
    "    lpj = nmonths_col*30 - nc  #number of colum sets in columns\n",
    "    \n",
    "    for j in range(0,lpj):\n",
    "        for i in range(0,lpi):\n",
    "            row_get = i + j + 1\n",
    "            col_get_start = 0\n",
    "            col_get_end = nc\n",
    "\n",
    "            row_put = i\n",
    "            col_put_start = nc*(j+1)\n",
    "            col_put_end = col_put_start + nc\n",
    "\n",
    "            mt_full_set[row_put,col_put_start:col_put_end] = mt_full_set[row_get,col_get_start:col_get_end]\n",
    "    \n",
    "    return mt_full_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_create_tgt_matrix_pr07(mtx_data,tgt_metric,tgt_mod):\n",
    "    \n",
    "    mtx01 = mtx_data\n",
    "    \n",
    "    nrows = mtx01.shape[0]\n",
    "    ncols = mtx01.shape[1]\n",
    "    \n",
    "    #1. Modify tgt variable\n",
    "    # Shift the features down 1 row so they always occur before the tgt\n",
    "    vc_tgt = mtx01[0:(nrows-1),0].copy()\n",
    "    mt_feat = mtx01[1:(nrows),0:(ncols)].copy()\n",
    "    mt_feat[:,0]=vc_tgt\n",
    "    \n",
    "    #Modify the target variable depending on the metric (precipitation or temperature)\n",
    "    #Modify tgt depending on tgt_mod, options could be running totals, moving averages, \n",
    "        \n",
    "    #2. Check that all columns and rows have some type of data\n",
    "    \n",
    "    \n",
    "        \n",
    "    #return the matrix for analysis\n",
    "        \n",
    "    return mt_feat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = {\n",
    "#     'st_id': [72508014740,72508014740,72508014740, 72504094702,72504094702,72504094702,72510094746,72510094746,72510094746,72505004781,72505004781,72505004781],\n",
    "#     'target_station': [72508014740,72508014740,72508014740,72508014740,72508014740,72508014740,72508014740,72508014740,72508014740,72508014740,72508014740,72508014740],\n",
    "#     'fnames': ['72508014740_model_data_airtemp.csv', '72508014740_model_data_precip.csv', '72508014740_model_data_relhum.csv',\n",
    "#         '72504094702_model_data_airtemp.csv', '72504094702_model_data_precip.csv', '72504094702_model_data_relhum.csv',\n",
    "#         '72510094746_model_data_airtemp.csv', '72510094746_model_data_precip.csv', '72510094746_model_data_relhum.csv',\n",
    "#         '72505004781_model_data_airtemp.csv', '72505004781_model_data_precip.csv', '72505004781_model_data_relhum.csv'],\n",
    "#     'path': ['D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\'],\n",
    "#     'vect_close_bkt2': [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4]\n",
    "# }\n",
    "\n",
    "# dt_analysis_set_keys = pd.DataFrame(data)\n",
    "# dt_analysis_set_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "st_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "target_station",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "fnames",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "vect_close_bkt2",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "d4e62e76-2caf-4b83-b676-beec51d697d4",
       "rows": [
        [
         "207",
         "72406093721",
         "72406093721",
         "72406093721_model_data_airtemp.csv",
         "\\\\CXA01\\Users\\jhugh\\Documents\\HT\\NCEI_data\\metrics_csv\\",
         "1"
        ],
        [
         "208",
         "72406093721",
         "72406093721",
         "72406093721_model_data_precip.csv",
         "\\\\CXA01\\Users\\jhugh\\Documents\\HT\\NCEI_data\\metrics_csv\\",
         "1"
        ],
        [
         "209",
         "72406093721",
         "72406093721",
         "72406093721_model_data_relhum.csv",
         "\\\\CXA01\\Users\\jhugh\\Documents\\HT\\NCEI_data\\metrics_csv\\",
         "1"
        ],
        [
         "210",
         "72405013743",
         "72406093721",
         "72405013743_model_data_airtemp.csv",
         "\\\\CXA01\\Users\\jhugh\\Documents\\HT\\NCEI_data\\metrics_csv\\",
         "2"
        ],
        [
         "211",
         "72405013743",
         "72406093721",
         "72405013743_model_data_precip.csv",
         "\\\\CXA01\\Users\\jhugh\\Documents\\HT\\NCEI_data\\metrics_csv\\",
         "2"
        ],
        [
         "212",
         "72405013743",
         "72406093721",
         "72405013743_model_data_relhum.csv",
         "\\\\CXA01\\Users\\jhugh\\Documents\\HT\\NCEI_data\\metrics_csv\\",
         "2"
        ],
        [
         "213",
         "74594493784",
         "72406093721",
         "74594493784_model_data_airtemp.csv",
         "\\\\CXA01\\Users\\jhugh\\Documents\\HT\\NCEI_data\\metrics_csv\\",
         "2"
        ],
        [
         "214",
         "74594493784",
         "72406093721",
         "74594493784_model_data_precip.csv",
         "\\\\CXA01\\Users\\jhugh\\Documents\\HT\\NCEI_data\\metrics_csv\\",
         "2"
        ],
        [
         "215",
         "74594493784",
         "72406093721",
         "74594493784_model_data_relhum.csv",
         "\\\\CXA01\\Users\\jhugh\\Documents\\HT\\NCEI_data\\metrics_csv\\",
         "2"
        ],
        [
         "216",
         "72403093738",
         "72406093721",
         "72403093738_model_data_airtemp.csv",
         "\\\\CXA01\\Users\\jhugh\\Documents\\HT\\NCEI_data\\metrics_csv\\",
         "3"
        ],
        [
         "217",
         "72403093738",
         "72406093721",
         "72403093738_model_data_precip.csv",
         "\\\\CXA01\\Users\\jhugh\\Documents\\HT\\NCEI_data\\metrics_csv\\",
         "3"
        ],
        [
         "218",
         "72403093738",
         "72406093721",
         "72403093738_model_data_relhum.csv",
         "\\\\CXA01\\Users\\jhugh\\Documents\\HT\\NCEI_data\\metrics_csv\\",
         "3"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 12
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>st_id</th>\n",
       "      <th>target_station</th>\n",
       "      <th>fnames</th>\n",
       "      <th>path</th>\n",
       "      <th>vect_close_bkt2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>72406093721</td>\n",
       "      <td>72406093721</td>\n",
       "      <td>72406093721_model_data_airtemp.csv</td>\n",
       "      <td>\\\\CXA01\\Users\\jhugh\\Documents\\HT\\NCEI_data\\met...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>72406093721</td>\n",
       "      <td>72406093721</td>\n",
       "      <td>72406093721_model_data_precip.csv</td>\n",
       "      <td>\\\\CXA01\\Users\\jhugh\\Documents\\HT\\NCEI_data\\met...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>72406093721</td>\n",
       "      <td>72406093721</td>\n",
       "      <td>72406093721_model_data_relhum.csv</td>\n",
       "      <td>\\\\CXA01\\Users\\jhugh\\Documents\\HT\\NCEI_data\\met...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>72405013743</td>\n",
       "      <td>72406093721</td>\n",
       "      <td>72405013743_model_data_airtemp.csv</td>\n",
       "      <td>\\\\CXA01\\Users\\jhugh\\Documents\\HT\\NCEI_data\\met...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>72405013743</td>\n",
       "      <td>72406093721</td>\n",
       "      <td>72405013743_model_data_precip.csv</td>\n",
       "      <td>\\\\CXA01\\Users\\jhugh\\Documents\\HT\\NCEI_data\\met...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>72405013743</td>\n",
       "      <td>72406093721</td>\n",
       "      <td>72405013743_model_data_relhum.csv</td>\n",
       "      <td>\\\\CXA01\\Users\\jhugh\\Documents\\HT\\NCEI_data\\met...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>74594493784</td>\n",
       "      <td>72406093721</td>\n",
       "      <td>74594493784_model_data_airtemp.csv</td>\n",
       "      <td>\\\\CXA01\\Users\\jhugh\\Documents\\HT\\NCEI_data\\met...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>74594493784</td>\n",
       "      <td>72406093721</td>\n",
       "      <td>74594493784_model_data_precip.csv</td>\n",
       "      <td>\\\\CXA01\\Users\\jhugh\\Documents\\HT\\NCEI_data\\met...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>74594493784</td>\n",
       "      <td>72406093721</td>\n",
       "      <td>74594493784_model_data_relhum.csv</td>\n",
       "      <td>\\\\CXA01\\Users\\jhugh\\Documents\\HT\\NCEI_data\\met...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>72403093738</td>\n",
       "      <td>72406093721</td>\n",
       "      <td>72403093738_model_data_airtemp.csv</td>\n",
       "      <td>\\\\CXA01\\Users\\jhugh\\Documents\\HT\\NCEI_data\\met...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>72403093738</td>\n",
       "      <td>72406093721</td>\n",
       "      <td>72403093738_model_data_precip.csv</td>\n",
       "      <td>\\\\CXA01\\Users\\jhugh\\Documents\\HT\\NCEI_data\\met...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>72403093738</td>\n",
       "      <td>72406093721</td>\n",
       "      <td>72403093738_model_data_relhum.csv</td>\n",
       "      <td>\\\\CXA01\\Users\\jhugh\\Documents\\HT\\NCEI_data\\met...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           st_id  target_station                              fnames  \\\n",
       "207  72406093721     72406093721  72406093721_model_data_airtemp.csv   \n",
       "208  72406093721     72406093721   72406093721_model_data_precip.csv   \n",
       "209  72406093721     72406093721   72406093721_model_data_relhum.csv   \n",
       "210  72405013743     72406093721  72405013743_model_data_airtemp.csv   \n",
       "211  72405013743     72406093721   72405013743_model_data_precip.csv   \n",
       "212  72405013743     72406093721   72405013743_model_data_relhum.csv   \n",
       "213  74594493784     72406093721  74594493784_model_data_airtemp.csv   \n",
       "214  74594493784     72406093721   74594493784_model_data_precip.csv   \n",
       "215  74594493784     72406093721   74594493784_model_data_relhum.csv   \n",
       "216  72403093738     72406093721  72403093738_model_data_airtemp.csv   \n",
       "217  72403093738     72406093721   72403093738_model_data_precip.csv   \n",
       "218  72403093738     72406093721   72403093738_model_data_relhum.csv   \n",
       "\n",
       "                                                  path  vect_close_bkt2  \n",
       "207  \\\\CXA01\\Users\\jhugh\\Documents\\HT\\NCEI_data\\met...                1  \n",
       "208  \\\\CXA01\\Users\\jhugh\\Documents\\HT\\NCEI_data\\met...                1  \n",
       "209  \\\\CXA01\\Users\\jhugh\\Documents\\HT\\NCEI_data\\met...                1  \n",
       "210  \\\\CXA01\\Users\\jhugh\\Documents\\HT\\NCEI_data\\met...                2  \n",
       "211  \\\\CXA01\\Users\\jhugh\\Documents\\HT\\NCEI_data\\met...                2  \n",
       "212  \\\\CXA01\\Users\\jhugh\\Documents\\HT\\NCEI_data\\met...                2  \n",
       "213  \\\\CXA01\\Users\\jhugh\\Documents\\HT\\NCEI_data\\met...                2  \n",
       "214  \\\\CXA01\\Users\\jhugh\\Documents\\HT\\NCEI_data\\met...                2  \n",
       "215  \\\\CXA01\\Users\\jhugh\\Documents\\HT\\NCEI_data\\met...                2  \n",
       "216  \\\\CXA01\\Users\\jhugh\\Documents\\HT\\NCEI_data\\met...                3  \n",
       "217  \\\\CXA01\\Users\\jhugh\\Documents\\HT\\NCEI_data\\met...                3  \n",
       "218  \\\\CXA01\\Users\\jhugh\\Documents\\HT\\NCEI_data\\met...                3  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#STEP 1.  Select a station using ID to get its related stations\n",
    "# Get file of target stations and their related stations\n",
    "dt_trg_set = ipd02 + \"Target_station_analysis_set.csv\"\n",
    "lst_target_stations = [\n",
    "  \"74486094789\",\n",
    "  \"72518014735\",\n",
    "  \"72508014740\",\n",
    "  \"72526014860\",\n",
    "  \"72406093721\",\n",
    "  \"72401013740\",\n",
    "  \"72219013874\",\n",
    "  \"72423093821\",\n",
    "  \"72428014821\",\n",
    "  \"72327013897\"\n",
    "]\n",
    "\n",
    "#Select the target station for which predictions will be made\n",
    "#72508014740 is JFK airport\n",
    "st_id = lst_target_stations[4]\n",
    "num_st_keep = 4   #num of stations to keep for analysis\n",
    "analysis_set = dt_trg_set\n",
    "dt_analysis_set = pd.read_csv(analysis_set)\n",
    "dt_analysis_set['path'] = ipd02\n",
    "dt01 = dt_analysis_set[dt_analysis_set['target_station']==int(st_id)]\n",
    "#uniq_stat = dt01['vect_close_bkt2'].unique()[:2]\n",
    "uniq_station = dt01['st_id'].unique()[:num_st_keep]\n",
    "dt02 = dt01[dt01['st_id'].isin(uniq_station)]\n",
    "\n",
    "selected_columns = ['st_id','target_station',\n",
    "                    'fnames','path','vect_close_bkt2']\n",
    "#The output below is fed to STEP 2 to run the analysis\n",
    "dt_analysis_set_keys = dt02[selected_columns].copy()\n",
    "\n",
    "dt_analysis_set_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\\CXA01\\Users\\jhugh\\Documents\\Py_S4\\Py_S4_v02_JHH\\NCEI_data\\\n",
      "\\\\CXA01\\Users\\jhugh\\Documents\\HT\\NCEI_data\\metrics_csv\\72406093721_4stations_144RowMn_30ColMn.parquet\n"
     ]
    }
   ],
   "source": [
    "#STEP 2.  Use elements created in STEP 1 to get a data matrix for analysis\n",
    "#All the available stations which have all three metrics needed for model\n",
    "#vc_stid = dt_analysis_set_keys[['st_id']].drop_duplicates()\n",
    "\n",
    "#Organize the data based on the nature of the data so they all conform to the same size matrices\n",
    "#The metrics included are\n",
    "#  AT - air temperature in Celcius\n",
    "#  RH - relative humidity in Percent\n",
    "#  PR - precipitation in MM\n",
    "\n",
    "#Put them all in appropriate matrices and create list of metrics\n",
    "#Data files are of two types\n",
    "#    -  24 hour * 365 days * n years  (AT, PR)\n",
    "#    -  365 days * n years (RH)\n",
    "\n",
    "#Parameters governing data matrix\n",
    "stid_target = \"72406093721\"\n",
    "stid_keys = dt_analysis_set_keys\n",
    "nstations = 4\n",
    "ipd = ipd02\n",
    "nhours = 24\n",
    "ndays = 365\n",
    "nmonths_row = 144\n",
    "nmonths_col = 30\n",
    "nmetrics = 5\n",
    "tgt_metric =  \"temp\" #\"prec\"  \"temp\"\n",
    "tgt_mod = \"No Mod\"\n",
    "\n",
    "#1. Get the relevant data in a list\n",
    "#lst_data = fn_select_data_vectors_pr07(stid_keys,nstations,nhours,nmonths_row,nmonths_col)\n",
    "\n",
    "#2. Construct a matrix from most recent to most distant from the list data\n",
    "#mtx_data = fn_create_data_matrix_pr07(lst_data,tgt_metric,nstations,nmetrics,nmonths_col)\n",
    "\n",
    "#3.  Create a target variable and select feature variables\n",
    "#s4_data = fn_create_tgt_matrix_pr07(mtx_data,tgt_metric,tgt_mod)\n",
    "\n",
    "\n",
    "print(ipd03)\n",
    "rtn_mtx_pqt = pd.DataFrame(s4_data)\n",
    "fn = f\"{ipd03}{st_id}_{nstations}stations_{nmonths_row}RowMn_{nmonths_col}ColMn.parquet\"\n",
    "print(fn)\n",
    "rtn_mtx_pqt.to_parquet(fn, index=False)\n",
    "\n",
    "# fn = \"\\\\CXA01\\Users\\jhugh\\Documents\\Py_S4\\Py_S4_v02_JHH\\NCEI_data\\72406093721_4stations_144RowMn_30ColMn.parquet\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  (82052, 18001)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "category",
         "type": "unknown"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "e592b9d1-66ab-40c1-8278-9b5454d66f62",
       "rows": [
        [
         "0",
         "7726"
        ],
        [
         "1",
         "74326"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 2
       }
      },
      "text/plain": [
       "0\n",
       "0     7726\n",
       "1    74326\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform_matrix(df, bins, labels):\n",
    "    \"\"\"\n",
    "    Transforms the given matrix by binning the first column, dropping the first row and column,\n",
    "    and adding the binned column as the first column.\n",
    "\n",
    "    Parameters:\n",
    "        matrix (pd.DataFrame): The input matrix.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The transformed matrix.\n",
    "    \"\"\"\n",
    "    # Extract the first column (dependent variable)\n",
    "    firstcolumn = df.iloc[:, 0].astype('float32')\n",
    "\n",
    "    # Bin the first column\n",
    "    binned_column = pd.cut(firstcolumn, bins=bins, labels=labels)\n",
    "\n",
    "    # Drop the first first column\n",
    "    df = df.iloc[:, 1:]\n",
    "\n",
    "    # Add the binned column as the first column\n",
    "    df.insert(0, 0, binned_column.iloc[0:].values)\n",
    "\n",
    "    # Return the final matrix\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "# Example matrix (replace this with your actual matrix)\n",
    "jj   = s4_data\n",
    "df = pd.DataFrame(jj).iloc[1:,] # Convert to DataFrame and drop the first row for binning\n",
    "# Drop rows where the first column has missing values\n",
    "# df = df.dropna(subset=[0])\n",
    "fc = df.iloc[:,0].astype('float32') # Ensure the first column is float for binning\n",
    "#DEL#bins = [-float('inf'), 10, 20, 25, float('inf')]\n",
    "\n",
    "bins = [-float('inf'), 1, float('inf')]\n",
    "# bins = [-float('inf'), 0] + list(pd.qcut(fc[fc > 0], q=4, retbins=True)[1][1:]) + [float('inf')]\n",
    "labels = [0, 1]\n",
    "# print(bins, labels)\n",
    "\n",
    "\n",
    "# bins = [-float('inf'), 0] + list(pd.qcut(fc[fc > 0], q=4, retbins=True)[1][1:]) + [float('inf')]\n",
    "#labels = [0, 1, 2, 3]\n",
    "# print(bins, labels)\n",
    "\n",
    "df_transformed = transform_matrix(df, bins, labels)\n",
    "df_transformed = df_transformed[df_transformed[0]>= 0]\n",
    "print(\"Dataset: \", df_transformed.shape)\n",
    "df_transformed[0].value_counts(dropna=False).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: (57437, 18001)\n",
      "0\n",
      "0     6338\n",
      "1    51099\n",
      "Name: count, dtype: int64\n",
      "Validation Set: (8205, 18001)\n",
      "0\n",
      "0     511\n",
      "1    7694\n",
      "Name: count, dtype: int64\n",
      "Test Set: (16410, 18001)\n",
      "0\n",
      "0      877\n",
      "1    15533\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def split_time_series_data(df, train_ratio=0.7, val_ratio=0.1, test_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Splits a time series dataset into training, validation, and test sets.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input time series dataset, ordered in descending dates.\n",
    "        train_ratio (float): The proportion of data to use for training.\n",
    "        val_ratio (float): The proportion of data to use for validation.\n",
    "        test_ratio (float): The proportion of data to use for testing.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing three DataFrames (train, validation, test).\n",
    "    \"\"\"\n",
    "    assert train_ratio + val_ratio + test_ratio == 1.0, \"Ratios must sum to 1.0\"\n",
    "\n",
    "    # Calculate split indices\n",
    "    n = len(df)\n",
    "    test_end = int(n * test_ratio)    \n",
    "    val_end = test_end + int(n * val_ratio)\n",
    "    \n",
    "\n",
    "    # Split the data\n",
    "    test_data = df.iloc[:test_end].sort_index(ascending=False)\n",
    "    val_data = df.iloc[test_end:val_end].sort_index(ascending=False)\n",
    "    train_data = df.iloc[val_end:].sort_index(ascending=False)\n",
    "\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "# Example usage\n",
    "train_data, val_data, test_data = split_time_series_data(df_transformed)\n",
    "print(\"Training Set:\", train_data.shape)\n",
    "print(train_data[0].value_counts(dropna=False).sort_index())\n",
    "print(\"Validation Set:\", val_data.shape)\n",
    "print(val_data[0].value_counts(dropna=False).sort_index())\n",
    "print(\"Test Set:\", test_data.shape)\n",
    "print(test_data[0].value_counts(dropna=False).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipd02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets exported successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the station ID and output directory\n",
    "station_id = st_id\n",
    "output_dir = ipd02\n",
    "\n",
    "\n",
    "# Export datasets to CSV\n",
    "train_data.to_parquet(f\"{output_dir}{station_id}_train.parquet\", index=False)\n",
    "val_data.to_parquet(f\"{output_dir}{station_id}_validation.parquet\", index=False)\n",
    "test_data.to_parquet(f\"{output_dir}{station_id}_test.parquet\", index=False)\n",
    "\n",
    "# train_data.to_csv(f\"{output_dir}{station_id}_train.csv\", index=False)\n",
    "# val_data.to_csv(f\"{output_dir}{station_id}_validation.csv\", index=False)\n",
    "# test_data.to_csv(f\"{output_dir}{station_id}_test.csv\", index=False)\n",
    "\n",
    "print(\"Datasets exported successfully.\")\n",
    "\n",
    "\n",
    "#python -m s4model --modelname 72508014740 --trainset ../data/weathermetrics/72508014740_train.csv --valset ../data/weathermetrics/72508014740_validation.csv --testset ../data/weathermetrics/72508014740_test.csv --tabulardata --dependent_variable 0 --epochs 30\n",
    "\n",
    "#python -m s4model --modelname 72508014740 --trainset \\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72508014740_train.parquet --valset \\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72508014740_validation.parquet --testset \\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72508014740_test.parquet --tabulardata --dependent_variable 0 --epochs 30\n",
    "\n",
    "# python -m charts --df ../results/72508014740_Test_results_20250424_104121PM.csv --actual 0 --predicted Predicted \n",
    "# python -m charts --df ../results/72508014740_Test_results_20250427_100132PM.csv --actual 0 --predicted Predicted \n",
    "# python -m charts --df ../results/72508014740_Test_results_20250428_091259PM.csv --actual 0 --predicted Predicted \n",
    "# python -m charts --df ../results/72508014740_Test_results_20250428_094033PM.csv --actual 0 --predicted Predicted \n",
    "# python -m charts --df ../results/72508014740_Test_results_20250429_091538PM.csv --actual 0 --predicted Predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1761420546.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[14], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    python -m charts --modelname 72406093721 --trainset \\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72406093721_train.parquet --valset \\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72406093721_validation.parquet --testset \\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72406093721_test.parquet --tabulardata --dependent_variable 0 --epochs 30\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "st_id\n",
    "ipd02\n",
    "\n",
    "python -m charts --modelname 72406093721 --trainset \\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72406093721_train.parquet --valset \\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72406093721_validation.parquet --testset \\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72406093721_test.parquet --tabulardata --dependent_variable 0 --epochs 30\n",
    "\n",
    "\n",
    "#python -m charts --df ../results/72508014740_Test_results_20250527_093455PM.csv --actual 0 --predicted Predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_first_non_nan(vector):\n",
    "#     return next((i for i, x in enumerate(vector) if not math.isnan(x)), None)\n",
    "\n",
    "# first_number_at1 = find_first_non_nan(vc_at1)\n",
    "# first_number_rh1 = find_first_non_nan(vc_rh1)\n",
    "# first_number_rh2 = find_first_non_nan(vc_rh2)\n",
    "# first_number_rh3 = find_first_non_nan(vc_rh3)\n",
    "# first_number_pr1 = find_first_non_nan(vc_pr1)\n",
    "\n",
    "# #Number of missing data points\n",
    "# print(f\"AT1 Number of missing cells: {np.count_nonzero(np.isnan(vc_at1))}\")\n",
    "# print(f\"AT1 Vector Length: {len(vc_at1)}\")\n",
    "\n",
    "# print(f\"RH1 Number of missing cells: {np.count_nonzero(np.isnan(vc_rh1))}\")\n",
    "# print(f\"RH1 Vector Length: {len(vc_rh1)}\")\n",
    "\n",
    "# print(f\"PR1 Number of missing cells: {np.count_nonzero(np.isnan(vc_pr1))}\")\n",
    "# print(f\"PR1 Vector Length: {len(vc_pr1)}\")\n",
    "\n",
    "# #Once a day metrics\n",
    "# v_data_first_once_day = max(\n",
    "#                    first_number_rh1,\n",
    "#                    first_number_rh2,\n",
    "#                    first_number_rh3)\n",
    "# print(v_data_first_once_day)\n",
    "\n",
    "# #24 times a day metrics\n",
    "# v_data_first_24h_day = max(first_number_at1,\n",
    "#                    first_number_pr1)\n",
    "# print(v_data_first_24h_day)\n",
    "\n",
    "\n",
    "#These vectors all start with a numeric value so missing values can be filled\n",
    "\n",
    "\n",
    "#print(first_number_at1)\n",
    "#print(first_number_rh1)\n",
    "#print(first_number_rh2)\n",
    "#print(first_number_rh3)\n",
    "#print(first_number_pr1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(jj)\n",
    "fc = df.iloc[:,0].astype('float32')\n",
    "print(fc.shape)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(fc, bins=50, color='blue', alpha=0.7, density=True)\n",
    "plt.title('Normalized Histogram of a')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom bins\n",
    "\n",
    "a = fc[~fc.isnull()]\n",
    "\n",
    "# Calculate min, max, and percentiles by 10 for 'a'\n",
    "min_val = a.min()\n",
    "max_val = a.max()\n",
    "percentiles = np.percentile(a, np.arange(0, 101, 10))\n",
    "\n",
    "print(f\"Min: {min_val}\")\n",
    "print(f\"Max: {max_val}\")\n",
    "print(f\"Percentiles: {percentiles}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s4_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
