{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['72502014734_model_data_airtemp.csv', '72502014734_model_data_precip.csv', '72502014734_model_data_relhum.csv', '72504094702_model_data_airtemp.csv', '72504094702_model_data_precip.csv', '72504094702_model_data_relhum.csv', '72505004781_model_data_airtemp.csv', '72505004781_model_data_precip.csv', '72505004781_model_data_relhum.csv', '72508014740_model_data_airtemp.csv', '72508014740_model_data_precip.csv', '72508014740_model_data_relhum.csv', '72508014740_test.parquet', '72508014740_train.parquet', '72508014740_validation.parquet', '72510094746_model_data_airtemp.csv', '72510094746_model_data_precip.csv', '72510094746_model_data_relhum.csv', '74486094789_airtemp.csv', '74486094789_model_data_airtemp.csv', '74486094789_model_data_precip.csv', '74486094789_model_data_relhum.csv', '74486094789_precip.csv', '74486094789_relhum.csv', '74736093042_model_data_airtemp.csv', '74736093042_model_data_precip.csv', '74736093042_model_data_relhum.csv', '74739013961_model_data_airtemp.csv', '74739013961_model_data_precip.csv', '74739013961_model_data_relhum.csv', '74746003904_model_data_airtemp.csv', '74746003904_model_data_precip.csv', '74746003904_model_data_relhum.csv', 'Target_station_analysis_set.csv']\n",
      "           stid  has_all3\n",
      "0   72502014734         1\n",
      "3   72504094702         1\n",
      "6   72505004781         1\n",
      "9   72508014740         1\n",
      "12  72508014740         0\n",
      "15  72510094746         1\n",
      "18  74486094789         1\n",
      "24  74736093042         1\n",
      "27  74739013961         1\n",
      "30  74746003904         1\n",
      "33       Target         0\n"
     ]
    }
   ],
   "source": [
    "#Load the data for a station and organize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime\n",
    "import os\n",
    "# Path to data files\n",
    "ipd = \"D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\\"\n",
    "\n",
    "#Organize the data based on the nature of the data so they all conform to the same size matrices\n",
    "#The metrics included are\n",
    "#  AT - air temperature in Celcius\n",
    "#  RH - relative humidity in Percent\n",
    "#  PR - precipitation in MM\n",
    "\n",
    "#Identify which stations are in folder and have all three metrics\n",
    "#List only files\n",
    "files = [entry.name for entry in os.scandir(ipd) if entry.is_file()]\n",
    "\n",
    "print(files)\n",
    "\n",
    "def assign_flags(name, pattern):\n",
    "    return int(pattern in name.lower())\n",
    "\n",
    "# Example usage:\n",
    "# Unique_stations\n",
    "df_files = pd.DataFrame(files,columns=['File_name'])\n",
    "# Extract the station ID from the file names\n",
    "df_files['stid'] = df_files['File_name'].apply(lambda x: x.split('_')[0])\n",
    "df_files['has_airtem'] = df_files['File_name'].apply(assign_flags, pattern='airtemp')\n",
    "df_files['has_precip'] = df_files['File_name'].apply(assign_flags, pattern='precip')\n",
    "df_files['has_relhum'] = df_files['File_name'].apply(assign_flags, pattern='relhum')\n",
    "df_files['has_all3'] = df_files[['has_airtem', 'has_precip', 'has_relhum']].sum(axis=1)\n",
    "\n",
    "#All the available stations which have all three metrics needed for model\n",
    "vc_stid = df_files[['stid','has_all3']].drop_duplicates()\n",
    "#vc_stid <- vc_stid[1]\n",
    "\n",
    "#print(df_files)\n",
    "print(vc_stid)\n",
    "\n",
    "def keep_max_by_keys(df, keys):\n",
    "    \"\"\"\n",
    "    Given a DataFrame and a list of keys, keep the maximum value for all other columns grouped by the keys.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "    keys (list): The list of column names to group by.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with the maximum values for each group.\n",
    "    \"\"\"\n",
    "    return df.groupby(keys).max().reset_index()\n",
    "\n",
    "# keep_max_by_keys(df_files, ['stid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_make_matrix_pr(stid,ipd,nhours,ndays,nyears_row,nyears_col):\n",
    "\n",
    "    #Create a matrix for precipitation data that feeds\n",
    "    #The S4 model\n",
    "    # 1. Read in all data files\n",
    "    # 2. Find the common starting date that will be used to align the matrices\n",
    "    # 3. Keep only the values that fit the common starting date\n",
    "    # 4. Combine the data into one large matrix for the StationID \n",
    "\n",
    "    # 1. Read in data files and get start date v_date... for each\n",
    "    # Check that data exist for all metrics\n",
    "\n",
    "    #Air temperature (one variable recorded once an hour a day)\n",
    "    df_at = pd.read_csv(ipd + stid + \"_airtemp.csv\")\n",
    "    v_date_at = datetime.strptime(df_at.iloc[0,2], \"%Y-%m-%d\").date()\n",
    "    #Relative Hunidity (3 variables recorded once a day)\n",
    "    df_rh = pd.read_csv(ipd + stid + \"_relhum.csv\")\n",
    "    v_date_rh = datetime.strptime(df_rh.iloc[0,4], \"%Y-%m-%d\").date()\n",
    "    #Precipitation (one variable recorded once an hour a day)\n",
    "    df_pr = pd.read_csv(ipd + stid + \"_precip.csv\")\n",
    "    v_date_pr = datetime.strptime(df_pr.iloc[0,2], \"%Y-%m-%d\").date()\n",
    "\n",
    "\n",
    "    # 2. Find the offsets needed to all datasets start at same starting date\n",
    "    v_date_minimum = min(v_date_at,v_date_rh,v_date_pr)\n",
    "    v_date_minimum\n",
    "\n",
    "    #Identify offsets to use so all datadata vectors are appropriately aligned \n",
    "    offset_at = -(v_date_minimum - v_date_at).days\n",
    "    offset_rh = -(v_date_minimum - v_date_rh).days\n",
    "    offset_pr = -(v_date_minimum - v_date_pr).days\n",
    "\n",
    "    # 3. Get vectors of data for each measure\n",
    "    #These vectors all start on the same date\n",
    "    vc_at1 = df_at.iloc[offset_at:(len(df_at.iloc[:,1])-offset_at),1]\n",
    "    vc_rh1 = df_rh.iloc[offset_rh:(len(df_rh.iloc[:,1])-offset_rh),1]\n",
    "    vc_rh2 = df_rh.iloc[offset_rh:(len(df_rh.iloc[:,2])-offset_rh),2]\n",
    "    vc_rh3 = df_rh.iloc[offset_rh:(len(df_rh.iloc[:,3])-offset_rh),3]\n",
    "    vc_pr1 = df_pr.iloc[offset_pr:(len(df_pr.iloc[:,1])-offset_pr),1]\n",
    "\n",
    "\n",
    "    # 4. Interleave vectors so we get a complete vector of all metrics\n",
    "    #Start with the once a day vectors and loop over the number of days as defined by once a day vector\n",
    "    # These set how many years of data are needed : nyears_row + nyears_col\n",
    "    nyears_data_limit = nyears_row + nyears_col + 1\n",
    "    nrows = nhours*ndays*nyears_row\n",
    "    ncols = nhours*ndays*nyears_col \n",
    "    \n",
    "    # set the number of metrics created each day\n",
    "    # Metrics measured at hourly intervals are put into day metrics\n",
    "    # And ordered from most recent hour to most distant hour from left to right\n",
    "    # Day metrics are also ordered most recent day to most distant day left to right\n",
    "    ndaily_metrics = 2*nhours + 3*1\n",
    "\n",
    "    #Set the total number of days of data needed for the model\n",
    "    num_days = nyears_data_limit*ndays\n",
    "    #print(f\"Number of days: {num_days}\")\n",
    "\n",
    "    is_first = 1\n",
    "\n",
    "    #Construct the vector that holds all data values (valid and missing)\n",
    "    for i_day in range(num_days):\n",
    "        i_day_hour_end = (i_day + 1)*nhours - 1\n",
    "        i_day_hour_start = i_day_hour_end - (nhours -1)\n",
    "        va = vc_pr1[(i_day_hour_start + offset_pr):(i_day_hour_end + offset_pr)]\n",
    "        vb = vc_rh1[(i_day + offset_rh):(i_day + offset_rh)]\n",
    "        vc = vc_rh2[(i_day + offset_rh):(i_day + offset_rh)]\n",
    "        vd = vc_rh3[(i_day + offset_rh):(i_day + offset_rh)]\n",
    "        ve = vc_at1[(i_day_hour_start + offset_pr):(i_day_hour_end + offset_pr)]\n",
    "\n",
    "        #vmt_full set = np.concatenate((vmt_one_day,va,vb,vc,vd,ve)) \n",
    "        #print(i_day)\n",
    "        if is_first == 0:\n",
    "            vmt_full_set = np.concatenate((vmt_full_set,va,vb,vc,vd,ve))\n",
    "            #print(len(vmt_full_set))\n",
    "        else:\n",
    "            is_first = 0\n",
    "            vmt_full_set = np.concatenate((va,vb,vc,vd,ve))\n",
    "            #print(len(vmt_full_set))\n",
    "\n",
    "    #print(f\"Length of data vector: {len(vmt_full_set)}\")\n",
    "\n",
    "\n",
    "    #Organize the data vector into an array which will be returned to the function call\n",
    "    #Create an empty matrix in which data are organized\n",
    "    nrows = nyears_row*ndays\n",
    "    ncols = nyears_col*ndays*ndaily_metrics\n",
    "    nrow_days =  nyears_row*ndays\n",
    "    ncol_days =  nyears_col*ndays\n",
    "    data_limit = len(vmt_full_set)\n",
    "    #print(f\"Number of columns in matrix: {ncols}\")\n",
    "    #print(f\"Limit of data: {data_limit}\")\n",
    "\n",
    "    #Make template matrix to house data\n",
    "    template_matrix = np.zeros((nrows, ncols), dtype=np.float16)\n",
    "\n",
    "    # Fill the matrix\n",
    "    for i in range(nrow_days):\n",
    "        vc_start = i*(ndaily_metrics)\n",
    "        vc_end = ncols + vc_start\n",
    "        \n",
    "        if vc_end < data_limit:\n",
    "            template_matrix[i] = vmt_full_set[(vc_start):(vc_end)]\n",
    "\n",
    "    #   print(vc_start)\n",
    "    #   print(vc_end)\n",
    "    #   print(len(vmt_full_set[(vc_start):(vc_end)]))\n",
    "        \n",
    "    return template_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prajw\\AppData\\Local\\Temp\\ipykernel_10080\\2135009217.py:14: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_at = pd.read_csv(ipd + stid + \"_airtemp.csv\")\n",
      "C:\\Users\\prajw\\AppData\\Local\\Temp\\ipykernel_10080\\2135009217.py:20: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_pr = pd.read_csv(ipd + stid + \"_precip.csv\")\n"
     ]
    }
   ],
   "source": [
    "#Put them all in appropriate matrices and create list of metrics\n",
    "#Data files are of two types\n",
    "#    -  24 hour * 365 days * n years  (AT, PR)\n",
    "#    -  365 days * n years (RH)\n",
    "\n",
    "#Parameters governing data matrix\n",
    "nhours = 24\n",
    "ndays = 365\n",
    "nyears_row = 10\n",
    "nyears_col = 3\n",
    "\n",
    "def get_matrix(in_stid):\n",
    "    return fn_make_matrix_pr(in_stid,ipd,nhours,ndays,nyears_row,nyears_col)\n",
    "\n",
    "#lst_matrix = list(map(get_matrix,vc_stid.iloc[1,0]))\n",
    "\n",
    "#print(lst_matrix)\n",
    "#print(jj.shape)\n",
    "\n",
    "jj = fn_make_matrix_pr(\"74486094789\",ipd,nhours,ndays,nyears_row,nyears_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_matrix(df, bins, labels):\n",
    "    \"\"\"\n",
    "    Transforms the given matrix by binning the first column, dropping the first row and column,\n",
    "    and adding the binned column as the first column.\n",
    "\n",
    "    Parameters:\n",
    "        matrix (pd.DataFrame): The input matrix.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The transformed matrix.\n",
    "    \"\"\"\n",
    "    # Extract the first column (dependent variable)\n",
    "    firstcolumn = df.iloc[:, 0].astype('float32')\n",
    "\n",
    "    # Bin the first column\n",
    "    binned_column = pd.cut(firstcolumn, bins=bins, labels=labels)\n",
    "\n",
    "    # Drop the first first column\n",
    "    df = df.iloc[:, 1:]\n",
    "\n",
    "    # Add the binned column as the first column\n",
    "    df.insert(0, 0, binned_column.iloc[0:].values)\n",
    "\n",
    "    # Return the final matrix\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "# Example matrix (replace this with your actual matrix)\n",
    "df = pd.DataFrame(jj).iloc[1:,] # Convert to DataFrame and drop the first row for binning\n",
    "fc = df.iloc[:,0].astype('float32') # Ensure the first column is float for binning\n",
    "# bins = [-float('inf'), 0] + list(pd.qcut(fc[fc > 0], q=2, retbins=True)[1][1:]) + [float('inf')]\n",
    "bins = [-float('inf'), 10, float('inf')]\n",
    "labels = [0, 1]\n",
    "# print(bins, labels)\n",
    "\n",
    "df_transformed = transform_matrix(df, bins, labels)\n",
    "df_transformed = df_transformed[df_transformed[0]>= 0]\n",
    "# df_transformed[0].value_counts(dropna=False).sort_index()\n",
    "# df_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "0    2524\n",
       "1    1090\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transformed[0].value_counts(dropna=False).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: (2531, 55845)\n",
      "Validation Set: (361, 55845)\n",
      "Test Set: (722, 55845)\n"
     ]
    }
   ],
   "source": [
    "def split_time_series_data(df, train_ratio=0.7, val_ratio=0.1, test_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Splits a time series dataset into training, validation, and test sets.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input time series dataset, ordered in descending dates.\n",
    "        train_ratio (float): The proportion of data to use for training.\n",
    "        val_ratio (float): The proportion of data to use for validation.\n",
    "        test_ratio (float): The proportion of data to use for testing.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing three DataFrames (train, validation, test).\n",
    "    \"\"\"\n",
    "    assert train_ratio + val_ratio + test_ratio == 1.0, \"Ratios must sum to 1.0\"\n",
    "\n",
    "    # Calculate split indices\n",
    "    n = len(df)\n",
    "    test_end = int(n * test_ratio)    \n",
    "    val_end = test_end + int(n * val_ratio)\n",
    "    \n",
    "\n",
    "    # Split the data\n",
    "    test_data = df.iloc[:test_end].sort_index(ascending=False)\n",
    "    val_data = df.iloc[test_end:val_end].sort_index(ascending=False)\n",
    "    train_data = df.iloc[val_end:].sort_index(ascending=False)\n",
    "\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "# Example usage\n",
    "train_data, val_data, test_data = split_time_series_data(df_transformed)\n",
    "print(\"Training Set:\", train_data.shape)\n",
    "print(\"Validation Set:\", val_data.shape)\n",
    "print(\"Test Set:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the station ID and output directory\n",
    "station_id = \"74486094789\"\n",
    "output_dir = \"D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\\"\n",
    "\n",
    "# Export datasets to CSV\n",
    "train_data.to_parquet(f\"{output_dir}{station_id}_train.parquet\", index=False)\n",
    "val_data.to_parquet(f\"{output_dir}{station_id}_validation.parquet\", index=False)\n",
    "test_data.to_parquet(f\"{output_dir}{station_id}_test.parquet\", index=False)\n",
    "\n",
    "# Export datasets to CSV\n",
    "# train_data.to_csv(f\"{output_dir}{station_id}_train.csv\", index=False)\n",
    "# val_data.to_csv(f\"{output_dir}{station_id}_validation.csv\", index=False)\n",
    "# test_data.to_csv(f\"{output_dir}{station_id}_test.csv\", index=False)\n",
    "\n",
    "print(\"Datasets exported successfully.\")\n",
    "\n",
    "#python -m s4model --modelname 74486094789 --trainset ../data/weathermetrics/74486094789_train.csv --valset ../data/weathermetrics/74486094789_validation.csv --testset ../data/weathermetrics/74486094789_test.csv --tabulardata --dependent_variable 0 --epochs 30\n",
    "#python -m s4model --modelname 74486094789 --trainset ../data/weathermetrics/74486094789_train.parquet --valset ../data/weathermetrics/74486094789_validation.parquet --testset ../data/weathermetrics/74486094789_test.parquet --tabulardata --dependent_variable 0 --epochs 30\n",
    "\n",
    "# python -m charts --df ../results/74486094789_Test_results_20250408_093735PM.csv --actual 0 --predicted Predicted \n",
    "# python -m charts --df ../results/74486094789_Test_results_20250414_103853PM.csv --actual 0 --predicted Predicted \n",
    "# python -m charts --df ../results/74486094789_Test_results_20250415_082627PM.csv --actual 0 --predicted Predicted \n",
    "# python -m charts --df ../results/74486094789_Test_results_20250415_083725PM.csv --actual 0 --predicted Predicted \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0         1         2         3         4         5         6      \\\n",
      "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "2  1.500000  0.700195  1.299805  1.799805  1.000000  0.300049  0.000000   \n",
      "3  1.000000  0.500000  0.300049  0.000000  0.000000  0.000000  0.000000   \n",
      "4  0.000000  0.000000  0.000000  3.300781  3.300781  4.449219  5.000000   \n",
      "5  3.050781  3.900391  5.000000  3.300781  3.300781  2.199219  1.150391   \n",
      "\n",
      "      7         8         9      ...     55835      55836  55837     55838  \\\n",
      "1  0.000000  0.000000  2.000000  ...  16.09375  15.000000   15.0  16.09375   \n",
      "2  0.000000  0.000000  0.300049  ...   0.00000   0.000000    0.0   0.00000   \n",
      "3  0.000000  8.898438  9.398438  ...   0.50000   0.799805    0.0   0.00000   \n",
      "4  5.000000  5.300781  5.000000  ...   0.00000   0.000000    0.0   0.00000   \n",
      "5  0.600098 -1.099609 -3.300781  ...   0.00000   0.000000    0.0   0.00000   \n",
      "\n",
      "   55839     55840  55841      55842      55843      55844  \n",
      "1    0.0  0.000000    0.0   0.000000   0.000000   0.000000  \n",
      "2    0.0  0.000000    0.0   0.000000   0.000000   0.000000  \n",
      "3    1.5  0.300049    0.5   0.000000   0.000000   0.300049  \n",
      "4    0.0  0.000000    0.0   0.000000   0.000000   0.000000  \n",
      "5    0.0  0.000000    0.0  22.203125  22.203125  22.203125  \n",
      "\n",
      "[5 rows x 55845 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(jj)\n",
    "df =df.iloc[1:,:] # Remove first row of zeros which is a template row\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2190,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(jj)\n",
    "fc = df.iloc[:,0].astype('float32')\n",
    "print(fc.shape)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(fc, bins=50, color='blue', alpha=0.7, density=True)\n",
    "plt.title('Normalized Histogram of a')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "00. No Rain          1048\n",
      "03. Heavy Rain        777\n",
      "02. Moderate Rain     195\n",
      "01. Light Rain        139\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Custom bins\n",
    "a = fc  # Convert to float32 to avoid issues with float16 precision\n",
    "# bins = [-float('inf'), 0, 2, 5, float('inf')]\n",
    "# labels = ['Low', 'Medium', 'High', 'Very High']\n",
    "# a_categorized = pd.cut(a, bins=bins, labels=labels)\n",
    "\n",
    "# Quantile-based bins\n",
    "# Ensure unique bin edges by adding a small epsilon to duplicate edges\n",
    "# unique_a = a + np.random.uniform(0, 1e-10, size=len(a))\n",
    "# a_categorized = pd.qcut(unique_a, q=5, labels=[0, 1, 2, 3, 4])\n",
    "\n",
    "# print(a_categorized.value_counts())\n",
    "\n",
    "# quantile_cuts = pd.qcut(unique_a, q=5, retbins=True, precision= 1)[1]\n",
    "# print(\"Quantile Cuts:\", quantile_cuts)\n",
    "\n",
    "\n",
    "# Domain-specific thresholds\n",
    "bins = [-float('inf'), 0, 2.5, 7.6, float('inf')]\n",
    "labels = ['00. No Rain', '01. Light Rain', '02. Moderate Rain', '03. Heavy Rain']\n",
    "\n",
    "a_categorized = pd.cut(a, bins=bins, labels=labels, right=True)\n",
    "print(a_categorized.value_counts())\n",
    "\n",
    "combined = pd.DataFrame({'Value': a, 'Category': a_categorized})\n",
    "\n",
    "# print(combined.head())\n",
    "# Convert to string for better readability in the output\n",
    "combined['Category'] = combined['Category'].astype(str)\n",
    "combined['Value'] = combined['Value'].astype(str)\n",
    "\n",
    "b = pd.DataFrame(combined.groupby(['Category', 'Value']).size()).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5]\n",
      "Bin Boundaries: [-inf, 0, 6.69921875, 13.296875, 21.09375, 52.1875, inf]\n",
      "1\n",
      "0    27012\n",
      "1     7403\n",
      "3     7139\n",
      "2     6774\n",
      "4     6726\n",
      "5        0\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define bins for less than or equal to zero as \"No Rain\" and quantile-based bins for the rest\n",
    "bins = [-float('inf'), 0] + list(pd.qcut(a[a > 0], q=4, retbins=True)[1][1:]) + [float('inf')]\n",
    "labels = ['00. No Rain', '01. Light Rain', '02. Moderate Rain', '03. Heavy Rain', '04. Very Heavey Rain', '05. Really Heavy Rain']\n",
    "labels = [0, 1, 2, 3, 4, 5]\n",
    "print(labels)\n",
    "print(\"Bin Boundaries:\", bins)\n",
    "\n",
    "# Apply the new binning\n",
    "a_categorized = pd.cut(a, bins=bins, labels=labels, right=True)\n",
    "print(a_categorized.value_counts())\n",
    "\n",
    "# Combine the categorized data with the original values\n",
    "combined = pd.DataFrame({'Value': a, 'Category': a_categorized})\n",
    "\n",
    "# Convert to string for better readability in the output\n",
    "combined['Category'] = combined['Category'].astype(str)\n",
    "combined['Value'] = combined['Value'].astype(str)\n",
    "\n",
    "b = pd.DataFrame(combined.groupby(['Category', 'Value']).size()).sort_index()\n",
    "\n",
    "# Print the boundaries for the bins\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value\n",
       "-0.25            7\n",
       "-0.30004883     38\n",
       "-0.41992188      1\n",
       "-0.5498047      10\n",
       "-0.60009766    178\n",
       "              ... \n",
       "9.796875         3\n",
       "9.8359375        1\n",
       "9.875            1\n",
       "9.8984375        3\n",
       "nan            791\n",
       "Name: count, Length: 662, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined['Value'].value_counts(dropna= False).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import polars as pl\n",
    "\n",
    "def parse_value_and_calculate_datetime(value, reference_date):\n",
    "    \"\"\"\n",
    "    Parses the value and calculates the target variable, number of days, hours, \n",
    "    and the datetime difference based on the reference date.\n",
    "\n",
    "    Parameters:\n",
    "    value (str): The input value in the format 'ATD000044H23'.\n",
    "    reference_date (datetime.date): The reference date.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the target variable (str), number of days (int), \n",
    "           number of hours (int), and the calculated datetime (datetime.datetime).\n",
    "    \"\"\"\n",
    "    # Extract the target variable (first two characters)\n",
    "    target_variable = value[:2]\n",
    "    \n",
    "    # Extract the number of days (characters after 'D' and before 'H')\n",
    "    days_str = value.split('D')[1].split('H')[0]\n",
    "    num_days = int(days_str) - 1  # Subtract 1 to match the logic in the original code\n",
    "    \n",
    "    # Extract the number of hours (characters after 'H')\n",
    "    hours_str = value.split('H')[1]\n",
    "    num_hours = 24 - int(hours_str)\n",
    "    \n",
    "    # Calculate the datetime difference\n",
    "    delta = timedelta(days=num_days, hours=num_hours)\n",
    "    calculated_datetime = datetime.combine(reference_date, datetime.min.time()) - delta\n",
    "    \n",
    "    return target_variable, num_days, num_hours, calculated_datetime\n",
    "\n",
    "# def get_data_and_reference_date(location, stationid, filename):\n",
    "#     # Determine file extension and read the file accordingly\n",
    "#     file_path = location + stationid + filename\n",
    "#     if filename.endswith('.csv'):\n",
    "#         df = pl.read_csv(file_path)\n",
    "#     elif filename.endswith('.parquet'):\n",
    "#         df = pl.read_parquet(file_path)\n",
    "#     else:\n",
    "#         raise ValueError(\"Unsupported file format. Only .csv and .parquet are supported.\")\n",
    "    \n",
    "#     # Extract the reference date from the first row\n",
    "#     reference_date = datetime.strptime(df['ref_date'][0], \"%Y-%m-%d\").date()\n",
    "    \n",
    "#     # Use parse_value_and_calculate to calculate the date column\n",
    "#     df = df.with_columns(\n",
    "#         pl.col('var_name').apply(lambda x: parse_value_and_calculate_datetime(x, reference_date)[3]).alias('date')\n",
    "#     )\n",
    "    \n",
    "#     # Drop the original variable name and reference date columns if needed\n",
    "#     df = df.drop(['var_name', 'ref_date'])\n",
    "    \n",
    "#     # Set the date column as index and sort by date\n",
    "#     df = df.sort('date').set_sorted('date')\n",
    "    \n",
    "#     # Forward fill to make the dataset hourly if it is not\n",
    "#     df = df.groupby_dynamic('date', every='1h').agg(pl.all().forward_fill())\n",
    "    \n",
    "#     return df, reference_date\n",
    "\n",
    "# parse_value_and_calculate_datetime(\"ATD000044H23\", ref_date_at)\n",
    "\n",
    "\n",
    "def get_data_and_reference_date(location, stationid, filename):\n",
    "    file_path = f\"{location}{stationid}{filename}\"\n",
    "    \n",
    "    # Get reference date using lazy evaluation\n",
    "    ref_date_query = (\n",
    "        pl.scan_csv(file_path) if filename.endswith('.csv') \n",
    "        else pl.scan_parquet(file_path)\n",
    "    ).select(pl.col(\"ref_date\").first())\n",
    "    \n",
    "    reference_date = datetime.strptime(\n",
    "        ref_date_query.collect().item(),\n",
    "        \"%Y-%m-%d\"\n",
    "    ).date()\n",
    "\n",
    "    # Main processing pipeline (corrected map_elements and group_by_dynamic)\n",
    "    lf = (\n",
    "        pl.scan_csv(file_path) if filename.endswith('.csv') \n",
    "        else pl.scan_parquet(file_path)\n",
    "    ).with_columns(\n",
    "        pl.col(\"var_name\").map_elements(\n",
    "            lambda x: parse_value_and_calculate_datetime(x, reference_date)[3],\n",
    "            return_dtype=pl.Datetime\n",
    "        ).alias(\"date\")\n",
    "    ).drop([\"var_name\", \"ref_date\"]).sort(\"date\").set_sorted(\"date\")\n",
    "\n",
    "    # Corrected group_by_dynamic spelling and aggregation\n",
    "    resampled = lf.group_by_dynamic(\n",
    "        index_column=\"date\",\n",
    "        every=\"1h\",\n",
    "        closed=\"left\",\n",
    "        include_boundaries=False\n",
    "    ).agg(\n",
    "        pl.all().fill_null(strategy=\"forward\")\n",
    "    )\n",
    "\n",
    "    return resampled.collect(engine='streaming'), reference_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Air Temperature Reference Date: 2025-02-13\n",
      "Relative Humidity Reference Date: 2025-02-13\n",
      "Precipitation Reference Date: 2025-02-13\n",
      "Minimum Reference Date: 2025-02-13\n"
     ]
    }
   ],
   "source": [
    "stid = '74486094789'\n",
    "# Air temperature\n",
    "df_at, ref_date_at = get_data_and_reference_date(ipd, stid, \"_airtemp.csv\")\n",
    "# Relative Humidity\n",
    "df_rh, ref_date_rh = get_data_and_reference_date(ipd, stid, \"_relhum.csv\")\n",
    "# Precipitation\n",
    "df_pr, ref_date_pr = get_data_and_reference_date(ipd, stid, \"_precip.csv\")\n",
    "\n",
    "print(\"Air Temperature Reference Date:\", ref_date_at)\n",
    "print(\"Relative Humidity Reference Date:\", ref_date_rh)\n",
    "print(\"Precipitation Reference Date:\", ref_date_pr)\n",
    "\n",
    "# 2. Find the offsets needed to all datasets start at same starting date\n",
    "ref_date_minimum = min(ref_date_at,ref_date_rh, ref_date_pr)\n",
    "ref_date_minimum\n",
    "print(\"Minimum Reference Date:\", ref_date_minimum)\n",
    "\n",
    "min_date = min(df_rh.index.min(), df_at.index.min(), df_pr.index.min())\n",
    "max_date = max(df_rh.index.max(), df_at.index.max(), df_pr.index.max())\n",
    "print(\"Minimum Date in Data:\", min_date)\n",
    "print(\"Maximum Date in Data:\", max_date)\n",
    "\n",
    "combined_df = pd.DataFrame(index=pd.date_range(\n",
    "    start=min_date,\n",
    "    end=max_date,\n",
    "    freq='H')).join([df_at, df_rh, df_pr], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (271_752, 4)\n",
      "┌─────────────────────┬───────────────────────┬─────────────────────┬─────────────────────┐\n",
      "│ date                ┆ air_temp_degrees_cels ┆ date_right          ┆ precip_depth_sum_mm │\n",
      "│ ---                 ┆ ---                   ┆ ---                 ┆ ---                 │\n",
      "│ datetime[μs]        ┆ list[f64]             ┆ datetime[μs]        ┆ list[f64]           │\n",
      "╞═════════════════════╪═══════════════════════╪═════════════════════╪═════════════════════╡\n",
      "│ 1994-01-01 00:00:00 ┆ [-0.6]                ┆ 1994-01-01 00:00:00 ┆ [0.0]               │\n",
      "│ 1994-01-01 01:00:00 ┆ [-1.1]                ┆ 1994-01-01 01:00:00 ┆ [0.0]               │\n",
      "│ 1994-01-01 02:00:00 ┆ [-1.1]                ┆ 1994-01-01 02:00:00 ┆ [0.0]               │\n",
      "│ 1994-01-01 03:00:00 ┆ [-1.7]                ┆ 1994-01-01 03:00:00 ┆ [0.0]               │\n",
      "│ 1994-01-01 04:00:00 ┆ [-1.7]                ┆ 1994-01-01 04:00:00 ┆ [0.0]               │\n",
      "│ …                   ┆ …                     ┆ …                   ┆ …                   │\n",
      "│ 2024-12-31 19:00:00 ┆ [11.7]                ┆ 2024-12-31 19:00:00 ┆ [null]              │\n",
      "│ 2024-12-31 20:00:00 ┆ [10.0]                ┆ 2024-12-31 20:00:00 ┆ [null]              │\n",
      "│ 2024-12-31 21:00:00 ┆ [9.7]                 ┆ 2024-12-31 21:00:00 ┆ [null]              │\n",
      "│ 2024-12-31 22:00:00 ┆ [8.9]                 ┆ 2024-12-31 22:00:00 ┆ [null]              │\n",
      "│ 2024-12-31 23:00:00 ┆ [9.4]                 ┆ 2024-12-31 23:00:00 ┆ [null]              │\n",
      "└─────────────────────┴───────────────────────┴─────────────────────┴─────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "def combine_datasets_on_datetime(datasets, datetime_key=\"date\"):\n",
    "    \"\"\"\n",
    "    Combines multiple datasets on a datetime key, keeping all available data.\n",
    "\n",
    "    Parameters:\n",
    "    datasets (list of pl.LazyFrame): List of LazyFrames to combine.\n",
    "    datetime_key (str): The key to join on, typically a datetime column.\n",
    "\n",
    "    Returns:\n",
    "    pl.DataFrame: Combined DataFrame with all available data.\n",
    "    \"\"\"\n",
    "    # Start with the first dataset\n",
    "    combined = datasets[0]\n",
    "\n",
    "    # Iteratively join all datasets on the datetime key\n",
    "    for dataset in datasets[1:]:\n",
    "        combined = combined.join(dataset, on=datetime_key, how=\"full\")\n",
    "\n",
    "    # Collect the result into a DataFrame\n",
    "    return combined.collect()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming df_at, df_pr, and df_rh are LazyFrames\n",
    "datasets = [df_at.lazy(), df_pr.lazy() #, df_rh.lazy()\n",
    "            ]\n",
    "combined_df = combine_datasets_on_datetime(datasets)\n",
    "\n",
    "# Print the result\n",
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prajw\\AppData\\Local\\Temp\\ipykernel_22332\\1067836617.py:1: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  combined_df = pd.DataFrame(index=pd.date_range(\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     air_temp_degrees_cels  Max_RH_pct  Rel_Hum_pct  \\\n",
      "2024-10-01 00:00:00                  17.50        97.0         84.0   \n",
      "2024-10-01 01:00:00                  17.20        97.0         84.0   \n",
      "2024-10-01 02:00:00                  16.10        97.0         84.0   \n",
      "2024-10-01 03:00:00                  16.40        97.0         84.0   \n",
      "2024-10-01 04:00:00                  16.10        93.0         59.0   \n",
      "2024-10-01 05:00:00                  16.10         NaN          NaN   \n",
      "2024-10-01 06:00:00                  15.85         NaN          NaN   \n",
      "2024-10-01 07:00:00                  15.60         NaN          NaN   \n",
      "2024-10-01 08:00:00                  16.10         NaN          NaN   \n",
      "2024-10-01 09:00:00                  16.10         NaN          NaN   \n",
      "2024-10-01 10:00:00                  16.10         NaN          NaN   \n",
      "2024-10-01 11:00:00                  16.70         NaN          NaN   \n",
      "2024-10-01 12:00:00                  17.50         NaN          NaN   \n",
      "2024-10-01 13:00:00                  18.90         NaN          NaN   \n",
      "2024-10-01 14:00:00                  20.60         NaN          NaN   \n",
      "2024-10-01 15:00:00                  20.30         NaN          NaN   \n",
      "2024-10-01 16:00:00                  20.00         NaN          NaN   \n",
      "2024-10-01 17:00:00                  21.10         NaN          NaN   \n",
      "2024-10-01 18:00:00                  21.40         NaN          NaN   \n",
      "2024-10-01 19:00:00                  20.60         NaN          NaN   \n",
      "2024-10-01 20:00:00                  20.00         NaN          NaN   \n",
      "2024-10-01 21:00:00                  19.45         NaN          NaN   \n",
      "2024-10-01 22:00:00                  18.30         NaN          NaN   \n",
      "2024-10-01 23:00:00                  17.80         NaN          NaN   \n",
      "\n",
      "                     Mean_RH_pct  precip_depth_sum_mm  \n",
      "2024-10-01 00:00:00         90.0                  0.0  \n",
      "2024-10-01 01:00:00         90.0                  0.0  \n",
      "2024-10-01 02:00:00         90.0                  0.0  \n",
      "2024-10-01 03:00:00         90.0                  0.0  \n",
      "2024-10-01 04:00:00         75.0                  0.0  \n",
      "2024-10-01 05:00:00          NaN                  0.0  \n",
      "2024-10-01 06:00:00          NaN                  0.0  \n",
      "2024-10-01 07:00:00          NaN                  0.0  \n",
      "2024-10-01 08:00:00          NaN                  0.0  \n",
      "2024-10-01 09:00:00          NaN                  0.0  \n",
      "2024-10-01 10:00:00          NaN                  0.0  \n",
      "2024-10-01 11:00:00          NaN                  0.0  \n",
      "2024-10-01 12:00:00          NaN                  0.0  \n",
      "2024-10-01 13:00:00          NaN                  0.0  \n",
      "2024-10-01 14:00:00          NaN                  0.0  \n",
      "2024-10-01 15:00:00          NaN                  0.0  \n",
      "2024-10-01 16:00:00          NaN                  0.0  \n",
      "2024-10-01 17:00:00          NaN                  0.0  \n",
      "2024-10-01 18:00:00          NaN                  0.0  \n",
      "2024-10-01 19:00:00          NaN                  0.0  \n",
      "2024-10-01 20:00:00          NaN                  0.0  \n",
      "2024-10-01 21:00:00          NaN                  0.0  \n",
      "2024-10-01 22:00:00          NaN                  0.0  \n",
      "2024-10-01 23:00:00          NaN                  0.0  \n"
     ]
    }
   ],
   "source": [
    "print(combined_df.loc['2024-10-01'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('AT', 44, 23, datetime.date(2024, 12, 31))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Identify offsets to use so all datadata vectors are appropriately aligned \n",
    "offset_at = -(ref_date_minimum - ref_date_at).days\n",
    "offset_rh = -(ref_date_minimum - ref_date_rh).days\n",
    "offset_pr = -(ref_date_minimum - ref_date_pr).days\n",
    "\n",
    "# 3. Get vectors of data for each measure\n",
    "#These vectors all start on the same date\n",
    "vc_at1 = df_at.iloc[offset_at:(len(df_at.iloc[:,1])-offset_at),1]\n",
    "vc_rh1 = df_rh.iloc[offset_rh:(len(df_rh.iloc[:,1])-offset_rh),1]\n",
    "vc_rh2 = df_rh.iloc[offset_rh:(len(df_rh.iloc[:,2])-offset_rh),2]\n",
    "vc_rh3 = df_rh.iloc[offset_rh:(len(df_rh.iloc[:,3])-offset_rh),3]\n",
    "vc_pr1 = df_pr.iloc[offset_pr:(len(df_pr.iloc[:,1])-offset_pr),1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          9.4\n",
       "1          8.9\n",
       "2          9.7\n",
       "3         10.0\n",
       "4         11.7\n",
       "          ... \n",
       "271726    -1.7\n",
       "271727    -1.7\n",
       "271728    -1.1\n",
       "271729    -1.1\n",
       "271730    -0.6\n",
       "Name: air_temp_degrees_cels, Length: 271731, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_at.iloc[offset_at:(len(df_at.iloc[:,1])-offset_at),1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 52560 26280\n"
     ]
    }
   ],
   "source": [
    "# 4. Interleave vectors so we get a complete vector of all metrics\n",
    "#Start with the once a day vectors and loop over the number of days as defined by once a day vector\n",
    "# These set how many years of data are needed : nyears_row + nyears_col\n",
    "nyears_data_limit = nyears_row + nyears_col + 1\n",
    "nrows = nhours*ndays*nyears_row\n",
    "ncols = nhours*ndays*nyears_col \n",
    "\n",
    "print(nyears_data_limit, nrows, ncols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Interleave vectors so we get a complete vector of all metrics\n",
    "#Start with the once a day vectors and loop over the number of days as defined by once a day vector\n",
    "# These set how many years of data are needed : nyears_row + nyears_col\n",
    "nyears_data_limit = nyears_row + nyears_col + 1\n",
    "nrows = nhours*ndays*nyears_row\n",
    "ncols = nhours*ndays*nyears_col \n",
    "\n",
    "# set the number of metrics created each day\n",
    "# Metrics measured at hourly intervals are put into day metrics\n",
    "# And ordered from most recent hour to most distant hour from left to right\n",
    "# Day metrics are also ordered most recent day to most distant day left to right\n",
    "ndaily_metrics = 2*nhours + 3*1\n",
    "\n",
    "#Set the total number of days of data needed for the model\n",
    "num_days = nyears_data_limit*ndays\n",
    "#print(f\"Number of days: {num_days}\")\n",
    "\n",
    "is_first = 1\n",
    "\n",
    "#Construct the vector that holds all data values (valid and missing)\n",
    "for i_day in range(num_days):\n",
    "    i_day_hour_end = (i_day + 1)*nhours - 1\n",
    "    i_day_hour_start = i_day_hour_end - (nhours -1)\n",
    "    va = vc_pr1[(i_day_hour_start + offset_pr):(i_day_hour_end + offset_pr)]\n",
    "    vb = vc_rh1[(i_day + offset_rh):(i_day + offset_rh)]\n",
    "    vc = vc_rh2[(i_day + offset_rh):(i_day + offset_rh)]\n",
    "    vd = vc_rh3[(i_day + offset_rh):(i_day + offset_rh)]\n",
    "    ve = vc_at1[(i_day_hour_start + offset_pr):(i_day_hour_end + offset_pr)]\n",
    "\n",
    "    #vmt_full set = np.concatenate((vmt_one_day,va,vb,vc,vd,ve)) \n",
    "    #print(i_day)\n",
    "    if is_first == 0:\n",
    "        vmt_full_set = np.concatenate((vmt_full_set,va,vb,vc,vd,ve))\n",
    "        #print(len(vmt_full_set))\n",
    "    else:\n",
    "        is_first = 0\n",
    "        vmt_full_set = np.concatenate((va,vb,vc,vd,ve))\n",
    "        #print(len(vmt_full_set))\n",
    "\n",
    "#print(f\"Length of data vector: {len(vmt_full_set)}\")\n",
    "\n",
    "\n",
    "#Organize the data vector into an array which will be returned to the function call\n",
    "#Create an empty matrix in which data are organized\n",
    "nrows = nyears_row*ndays\n",
    "ncols = nyears_col*ndays*ndaily_metrics\n",
    "nrow_days =  nyears_row*ndays\n",
    "ncol_days =  nyears_col*ndays\n",
    "data_limit = len(vmt_full_set)\n",
    "#print(f\"Number of columns in matrix: {ncols}\")\n",
    "#print(f\"Limit of data: {data_limit}\")\n",
    "\n",
    "#Make template matrix to house data\n",
    "template_matrix = np.zeros((nrows, ncols), dtype=np.float16)\n",
    "\n",
    "# Fill the matrix\n",
    "for i in range(nrow_days):\n",
    "    vc_start = i*(ndaily_metrics)\n",
    "    vc_end = ncols + vc_start\n",
    "    \n",
    "    if vc_end < data_limit:\n",
    "        template_matrix[i] = vmt_full_set[(vc_start):(vc_end)]\n",
    "\n",
    "#   print(vc_start)\n",
    "#   print(vc_end)\n",
    "#   print(len(vmt_full_set[(vc_start):(vc_end)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the station ID and output directory\n",
    "station_id = \"74486094789\"\n",
    "output_dir = \"D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\\"\n",
    "\n",
    "# Export datasets to CSV\n",
    "train_data.to_csv(f\"{output_dir}{station_id}_train.csv\", index=False)\n",
    "val_data.to_csv(f\"{output_dir}{station_id}_validation.csv\", index=False)\n",
    "test_data.to_csv(f\"{output_dir}{station_id}_test.csv\", index=False)\n",
    "\n",
    "print(\"Datasets exported successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
