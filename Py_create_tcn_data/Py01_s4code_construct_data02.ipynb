{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data for a station and organize for precipitation model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths\n",
    "#Need to set up based on who is running\n",
    "usr = \"PK\"\n",
    "if usr == \"PK\":\n",
    "    ipd01 = \"\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\2024\\\\\"\n",
    "    ipd02 = \"D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\\"\n",
    "    \n",
    "if usr == \"JH\":\n",
    "    ipd01 = \"\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\2024\\\\\"\n",
    "    ipd02 = \"\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def date_to_integer(date_string, date_format='%Y-%m-%d'):\n",
    "    \"\"\"\n",
    "    Converts a date string to an integer representation.\n",
    "\n",
    "    Args:\n",
    "        date_string (str): The date string to convert.\n",
    "        date_format (str, optional): The format of the date string. Defaults to '%Y-%m-%d'.\n",
    "\n",
    "    Returns:\n",
    "        int: The integer representation of the date.\n",
    "    \"\"\"\n",
    "    date_object = datetime.datetime.strptime(date_string, date_format).date()\n",
    "    return int(date_object.strftime('%Y%m%d'))\n",
    "\n",
    "# Example usage\n",
    "# date_str = '2025-04-06'\n",
    "# date_int = date_to_integer(date_str)\n",
    "# print(date_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_make_matrix_pr04(stid_keys,nstations,ipd,nhours,ndays,nyears_row,nyears_col):\n",
    "\n",
    "    #Create a matrix for precipitation data that feeds\n",
    "    #The S4 model\n",
    "    # 1. Read in all data files for multiple stations\n",
    "    # 2. Find the common starting date that will be used to align the matrices\n",
    "    # 3. Keep only the values that fit the common starting date\n",
    "    # 4. Combine the data into one large matrix for the StationID \n",
    "\n",
    "    # 1. Read in data files and get start date v_date... for each\n",
    "    # Check that data exist for all metrics\n",
    "\n",
    "    #Create two objects of length 12\n",
    "    #data_list which contains the metrics for stations 1-4, the first is the target\n",
    "    #data_start_date which contains the most recent date for each file\n",
    "    vc_first_rcd = [0,3,6,9]\n",
    "    num_stations = len(vc_first_rcd)\n",
    "    vc_st_id = stid_keys.iloc[:,0]\n",
    "    vc_path = stid_keys.iloc[:,3]\n",
    "    vc_fnames = stid_keys.iloc[:,2]\n",
    "\n",
    "    data_list = []\n",
    "    data_start_date = []\n",
    "    \n",
    "    for i in range(0, num_stations):\n",
    "            #Air temperature (one variable recorded once an hour a day)\n",
    "            infile = vc_path.iloc[vc_first_rcd[i]] + \"\\\\\" + vc_fnames.iloc[vc_first_rcd[i]]\n",
    "            df_at = pd.read_csv(infile)\n",
    "            data_list.append(df_at)\n",
    "            dt_first = datetime.strptime(df_at.iloc[0,2], \"%Y-%m-%d\").date()\n",
    "            data_start_date.append(dt_first)\n",
    "            #print(dt_first)\n",
    "            \n",
    "            #Precipitation (one variable recorded once an hour a day)\n",
    "            infile = vc_path.iloc[vc_first_rcd[i]] + \"\\\\\" + vc_fnames.iloc[vc_first_rcd[i]+1] \n",
    "            df_pr = pd.read_csv(infile)\n",
    "            data_list.append(df_pr)\n",
    "            dt_first = datetime.strptime(df_pr.iloc[0,2], \"%Y-%m-%d\").date()\n",
    "            data_start_date.append(dt_first)\n",
    "            #print(dt_first)\n",
    "            \n",
    "            #Relative Humidity (3 variables recorded once a day)\n",
    "            infile = vc_path.iloc[vc_first_rcd[i]] + \"\\\\\" + vc_fnames.iloc[vc_first_rcd[i]+2]\n",
    "            df_rh = pd.read_csv(infile)\n",
    "            data_list.append(df_rh)\n",
    "            dt_first = datetime.strptime(df_rh.iloc[0,4], \"%Y-%m-%d\").date()\n",
    "            data_start_date.append(dt_first)\n",
    "            #print(dt_first)\n",
    " \n",
    "     # 2. Find the offsets needed to all datasets start at same starting date\n",
    "    v_date_minimum = min(data_start_date)\n",
    "    print(v_date_minimum)\n",
    "\n",
    "    #Identify offsets to use so all datadata vectors are appropriately aligned \n",
    "    upper_range = len(data_start_date)\n",
    "    \n",
    "    date_offsets_list = []\n",
    "    for i in range(0, upper_range):\n",
    "        date_offset = data_start_date[i] - v_date_minimum\n",
    "        #Temporarily set to 0, revise later\n",
    "        #date_offset = 0\n",
    "        date_offsets_list.append(date_offset)\n",
    "    \n",
    "    # 3. Get vectors of data for each measure\n",
    "    #These vectors all start on the same date\n",
    "\n",
    "    data_vectors_list = []\n",
    "    for i in vc_first_rcd:\n",
    "        tdt = data_list[i]\n",
    "        tof = date_offsets_list[i]\n",
    "        \n",
    "        i_start = tof.days\n",
    "        i_end = len(tdt.iloc[:,1]) - i_start\n",
    "        vc_at1 = tdt.iloc[i_start:i_end,1]\n",
    "        data_vectors_list.append(vc_at1)\n",
    "\n",
    "        tdt = data_list[i+1]\n",
    "        tof = date_offsets_list[i+1]\n",
    "        i_start = tof.days\n",
    "        i_end = len(tdt.iloc[:,1]) - i_start\n",
    "        vc_pr1 = tdt.iloc[i_start:i_end,1]\n",
    "        data_vectors_list.append(vc_pr1)\n",
    "            \n",
    "        tdt = data_list[i+2]\n",
    "        tof = date_offsets_list[i+2]\n",
    "        i_start = tof.days\n",
    "        i_end = len(tdt.iloc[:,1]) - i_start        \n",
    "        vc_rh1 = tdt.iloc[i_start:i_end,1]\n",
    "        data_vectors_list.append(vc_rh1)\n",
    "\n",
    "        tdt = data_list[i+2]\n",
    "        tof = date_offsets_list[i+2]\n",
    "        i_start = tof.days\n",
    "        i_end = len(tdt.iloc[:,1]) - i_start        \n",
    "        vc_rh2 = tdt.iloc[i_start:i_end,2]\n",
    "        data_vectors_list.append(vc_rh2)\n",
    "\n",
    "        tdt = data_list[i+2]\n",
    "        tof = date_offsets_list[i+2]\n",
    "        i_start = tof.days\n",
    "        i_end = len(tdt.iloc[:,1]) - i_start        \n",
    "        vc_rh3 = tdt.iloc[i_start:i_end,3]\n",
    "        data_vectors_list.append(vc_rh3)\n",
    "    \n",
    "    # 4. Interleave vectors so we get a complete vector of all metrics\n",
    "    #Start with the once a day vectors and loop over the number of days as defined by once a day vector\n",
    "    #These set how many years of data are needed : nyears_row + nyears_col\n",
    "    nyears_data_limit = nyears_row + nyears_col + 1\n",
    "    nrows = nhours*ndays*nyears_row\n",
    "    ncols = nhours*ndays*nyears_col \n",
    "        \n",
    "    # set the number of metrics created each day\n",
    "    # Metrics measured at hourly intervals are put into day metrics\n",
    "    # And ordered from most recent hour to most distant hour from left to right\n",
    "    # Day metrics are also ordered most recent day to most distant day left to right\n",
    "    ndaily_metrics = 2*nhours + 3*1\n",
    "\n",
    "    #Set the total number of days of data needed for the model\n",
    "    num_days = nyears_data_limit*ndays\n",
    "    #print(f\"Number of days: {num_days}\")\n",
    "\n",
    "    is_first = 1\n",
    "\n",
    "    #Construct the vector that holds all data values (valid and missing)\n",
    "    #Assumes that all vectors in data_vectors_list match on date start\n",
    "    #data are organized by day, target station, other stations\n",
    "    for i_day in range(num_days):\n",
    "        i_day_hour_end = (i_day + 1)*nhours - 1\n",
    "        i_day_hour_start = i_day_hour_end - (nhours -1)\n",
    "        \n",
    "        #Target Station\n",
    "        va2 = data_vectors_list[0][(i_day_hour_start):(i_day_hour_end)]  #AT\n",
    "        va1 = data_vectors_list[1][(i_day_hour_start):(i_day_hour_end)]  #PR\n",
    "        va3 = data_vectors_list[2][i_day:i_day]                          #RH1\n",
    "        va4 = data_vectors_list[3][i_day:i_day]                          #RH2\n",
    "        va5 = data_vectors_list[4][i_day:i_day]                          #RH3\n",
    "        #Station 1\n",
    "        vb2 = data_vectors_list[5][(i_day_hour_start):(i_day_hour_end)]  #AT\n",
    "        vb1 = data_vectors_list[6][(i_day_hour_start):(i_day_hour_end)]  #PR\n",
    "        vb3 = data_vectors_list[7][i_day:i_day]                          #RH1\n",
    "        vb4 = data_vectors_list[8][i_day:i_day]                          #RH2\n",
    "        vb5 = data_vectors_list[9][i_day:i_day]                          #RH3\n",
    "        #Station 2\n",
    "        vc2 = data_vectors_list[10][(i_day_hour_start):(i_day_hour_end)]  #AT\n",
    "        vc1 = data_vectors_list[11][(i_day_hour_start):(i_day_hour_end)]  #PR\n",
    "        vc3 = data_vectors_list[12][i_day:i_day]                          #RH1\n",
    "        vc4 = data_vectors_list[13][i_day:i_day]                          #RH2\n",
    "        vc5 = data_vectors_list[14][i_day:i_day]                          #RH3\n",
    "        #Station 3\n",
    "        vd2 = data_vectors_list[15][(i_day_hour_start):(i_day_hour_end)]  #AT\n",
    "        vd1 = data_vectors_list[16][(i_day_hour_start):(i_day_hour_end)]  #PR\n",
    "        vd3 = data_vectors_list[17][i_day:i_day]                          #RH1\n",
    "        vd4 = data_vectors_list[18][i_day:i_day]                          #RH2\n",
    "        vd5 = data_vectors_list[19][i_day:i_day]                          #RH3\n",
    "            \n",
    "\n",
    "        #vmt_full set = np.concatenate((vmt_one_day,va,vb,vc,vd,ve)) \n",
    "        #print(i_day)\n",
    "        if is_first == 0:\n",
    "            vmt_full_set = np.concatenate((vmt_full_set,va1,va2,va3,va4,va5,vb1,vb2,vb3,vb4,vb5,vc1,vc2,vc3,vc4,vc5,vd1,vd2,vd3,vd4,vd5))\n",
    "            #print(len(vmt_full_set))\n",
    "        else:\n",
    "            is_first = 0\n",
    "            vmt_full_set = np.concatenate((va1,va2,va3,va4,va5,vb1,vb2,vb3,vb4,vb5,vc1,vc2,vc3,vc4,vc5,vd1,vd2,vd3,vd4,vd5))\n",
    "            #print(len(vmt_full_set))\n",
    "\n",
    "    print(f\"Length of data vector: {len(vmt_full_set)}\")\n",
    "\n",
    "\n",
    "    #Organize the data vector into an array which will be returned to the function call\n",
    "    #Create an empty matrix in which data are organized\n",
    "    #nstations = 4\n",
    "    nrows = nyears_row*ndays\n",
    "    ncols = nstations*nyears_col*ndays*ndaily_metrics\n",
    "    nrow_days =  nyears_row*ndays\n",
    "    ncol_days =  nyears_col*ndays\n",
    "    data_limit = len(vmt_full_set)\n",
    "    #print(f\"Number of columns in matrix: {ncols}\")\n",
    "    #print(f\"Limit of data: {data_limit}\")\n",
    "\n",
    "    #Make template matrix to house data\n",
    "    template_matrix = np.zeros((nrows, ncols), dtype=np.float16)\n",
    "\n",
    "    # Fill the matrix\n",
    "    for i in range(nrow_days):\n",
    "        vc_start = i*(ndaily_metrics)\n",
    "        vc_end = ncols + vc_start\n",
    "            \n",
    "        if vc_end < data_limit:\n",
    "            template_matrix[i] = vmt_full_set[(vc_start):(vc_end)]\n",
    "\n",
    "            \n",
    "    return template_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_make_matrix_pr05(stid_keys,nstations,ipd,nhours,ndays,nyears_row,nyears_col,n_metrics):\n",
    "    #Create a matrix for precipitation data that feeds\n",
    "    #The S4 model\n",
    "    # 1. Read in all data files for multiple stations\n",
    "    # 2. Find the common starting date that will be used to align the matrices\n",
    "    # 3. Keep only the values that fit the common starting date\n",
    "    # 4. Combine the data into one large matrix for the StationID \n",
    "\n",
    "    # 1. Read in data files and get start date v_date... for each\n",
    "    # Check that data exist for all metrics\n",
    "\n",
    "    #Create two objects of length 12\n",
    "    #data_list which contains the metrics for stations 1-4, the first is the target\n",
    "    #data_start_date which contains the most recent date for each file\n",
    "    vc_first_rcd = [0,3,6,9]\n",
    "    num_stations = len(vc_first_rcd)\n",
    "    vc_st_id = stid_keys.iloc[:,0]\n",
    "    vc_path = stid_keys.iloc[:,3]\n",
    "    vc_fnames = stid_keys.iloc[:,2]\n",
    "\n",
    "    data_list = []\n",
    "    data_start_date = []\n",
    "    \n",
    "    for i in range(0, num_stations):\n",
    "            #Air temperature (one variable recorded once an hour a day)\n",
    "            infile = vc_path.iloc[vc_first_rcd[i]] + \"\\\\\" + vc_fnames.iloc[vc_first_rcd[i]]\n",
    "            df_at = pd.read_csv(infile)\n",
    "            data_list.append(df_at)\n",
    "            dt_first = datetime.strptime(df_at.iloc[0,2], \"%Y-%m-%d\").date()\n",
    "            data_start_date.append(dt_first)\n",
    "            #print(dt_first)\n",
    "            \n",
    "            #Precipitation (one variable recorded once an hour a day)\n",
    "            infile = vc_path.iloc[vc_first_rcd[i]] + \"\\\\\" + vc_fnames.iloc[vc_first_rcd[i]+1] \n",
    "            df_pr = pd.read_csv(infile)\n",
    "            data_list.append(df_pr)\n",
    "            dt_first = datetime.strptime(df_pr.iloc[0,2], \"%Y-%m-%d\").date()\n",
    "            data_start_date.append(dt_first)\n",
    "            #print(dt_first)\n",
    "            \n",
    "            #Relative Humidity (3 variables recorded once a day)\n",
    "            infile = vc_path.iloc[vc_first_rcd[i]] + \"\\\\\" + vc_fnames.iloc[vc_first_rcd[i]+2]\n",
    "            df_rh = pd.read_csv(infile)\n",
    "            data_list.append(df_rh)\n",
    "            dt_first = datetime.strptime(df_rh.iloc[0,4], \"%Y-%m-%d\").date()\n",
    "            data_start_date.append(dt_first)\n",
    "            #print(dt_first)\n",
    " \n",
    "     # 2. Find the offsets needed to all datasets start at same starting date\n",
    "    v_date_minimum = min(data_start_date)\n",
    "    #print(v_date_minimum)\n",
    "\n",
    "    #Identify offsets to use so all datadata vectors are appropriately aligned \n",
    "    upper_range = len(data_start_date)\n",
    "    \n",
    "    date_offsets_list = []\n",
    "    for i in range(0, upper_range):\n",
    "        date_offset = data_start_date[i] - v_date_minimum\n",
    "        #Temporarily set to 0, revise later\n",
    "        #date_offset = 0\n",
    "        date_offsets_list.append(date_offset)\n",
    "    \n",
    "    # 3. Get vectors of data for each measure\n",
    "    #These vectors all start on the same date\n",
    "\n",
    "    data_vectors_list = []\n",
    "    for i in vc_first_rcd:\n",
    "        tdt = data_list[i]\n",
    "        tof = date_offsets_list[i]\n",
    "        \n",
    "        i_start = tof.days\n",
    "        i_end = len(tdt.iloc[:,1]) - i_start\n",
    "        vc_at1 = tdt.iloc[i_start:i_end,1]\n",
    "        data_vectors_list.append(vc_at1)\n",
    "\n",
    "        tdt = data_list[i+1]\n",
    "        tof = date_offsets_list[i+1]\n",
    "        i_start = tof.days\n",
    "        i_end = len(tdt.iloc[:,1]) - i_start\n",
    "        vc_pr1 = tdt.iloc[i_start:i_end,1]\n",
    "        data_vectors_list.append(vc_pr1)\n",
    "            \n",
    "        tdt = data_list[i+2]\n",
    "        tof = date_offsets_list[i+2]\n",
    "        i_start = tof.days\n",
    "        i_end = len(tdt.iloc[:,1]) - i_start        \n",
    "        vc_rh1 = tdt.iloc[i_start:i_end,1]\n",
    "        #adjust from 1 a day to 24 a day measures\n",
    "        vc_rh1_24 = np.repeat(vc_rh1, nhours)\n",
    "        data_vectors_list.append(vc_rh1_24)\n",
    "    \n",
    "        tdt = data_list[i+2]\n",
    "        tof = date_offsets_list[i+2]\n",
    "        i_start = tof.days\n",
    "        i_end = len(tdt.iloc[:,1]) - i_start        \n",
    "        vc_rh2 = tdt.iloc[i_start:i_end,2]\n",
    "        #adjust from 1 a day to 24 a day measures\n",
    "        vc_rh2_24 = np.repeat(vc_rh2, nhours)\n",
    "        data_vectors_list.append(vc_rh2_24)\n",
    "\n",
    "        tdt = data_list[i+2]\n",
    "        tof = date_offsets_list[i+2]\n",
    "        i_start = tof.days\n",
    "        i_end = len(tdt.iloc[:,1]) - i_start        \n",
    "        vc_rh3 = tdt.iloc[i_start:i_end,3]\n",
    "        #adjust from 1 a day to 24 a day measures\n",
    "        vc_rh3_24 = np.repeat(vc_rh3, nhours)\n",
    "        data_vectors_list.append(vc_rh3_24)\n",
    "\n",
    "    for i in range(0,20):\n",
    "        print(f\"Length vector: {len(data_vectors_list[i])}\")\n",
    "        \n",
    "    # 4. Interleave vectors so we get a complete vector of all metrics\n",
    "    #Start with the once a day vectors and loop over the number of days as defined by once a day vector\n",
    "    #These set how many years of data are needed : nyears_row + nyears_col\n",
    "    nyears_data_limit = nyears_row + nyears_col + 1\n",
    "    nrows = nhours*ndays*nyears_row\n",
    "    ncols = nhours*ndays*nyears_col \n",
    "        \n",
    "    # Set the number of metrics created each day\n",
    "    # And ordered from most recent hour to most distant hour from left to right\n",
    "    # Day metrics are also ordered most recent day to most distant day left to right\n",
    "    ndaily_metrics = n_metrics*nhours\n",
    "\n",
    "    #Set the total number of days of data needed for the model\n",
    "    num_days = nyears_data_limit*ndays\n",
    "    num_days_hrs = nhours*num_days\n",
    "    #num_days_hrs = nhours*100\n",
    "\n",
    "    print(f\"Number of years total needed: {nyears_data_limit}\")\n",
    "    print(f\"Number of days total needed: {num_days}\")\n",
    "    print(f\"Number of days and hours: {num_days_hrs}\")\n",
    "    print(f\"Number of matrix columns per metric: {ncols}\")\n",
    "    print(f\"Number of matrix rows: {nrows}\")\n",
    "    #print(f\"tgt : {data_vectors_list[1][0:23]}\")\n",
    "    is_first = 1\n",
    "\n",
    "    #Construct the vector that holds all data values (valid and missing)\n",
    "    #Assumes that all vectors in data_vectors_list match on date start\n",
    "    #data are organized by day, target station, other stations\n",
    "    #for i_day in range(num_days_hrs):\n",
    "    for i_day in range(nrows):\n",
    "        #print(i_day)\n",
    "        i_day_hour_end_tgt = i_day + ncols\n",
    "        i_day_hour_start_tgt = i_day\n",
    "        \n",
    "        i_day_hour_end_pred = i_day_hour_end_tgt + 1\n",
    "        i_day_hour_start_pred = i_day_hour_start_tgt + 1\n",
    "        \n",
    "        #print(f\"start :{i_day_hour_start_tgt}\")\n",
    "        #print(f\"end :{i_day_hour_end_tgt}\")\n",
    "        \n",
    "        #Target Station\n",
    "        if nstations > 0:\n",
    "            va2 = data_vectors_list[0][(i_day_hour_start_pred):(i_day_hour_end_pred)]  #AT\n",
    "            va1 = data_vectors_list[1][(i_day_hour_start_tgt):(i_day_hour_end_tgt)]  #PR\n",
    "            va3 = data_vectors_list[2][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH1\n",
    "            va4 = data_vectors_list[3][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH2\n",
    "            va5 = data_vectors_list[4][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH3\n",
    "        #Station 1\n",
    "        if nstations > 1:\n",
    "            vb2 = data_vectors_list[5][(i_day_hour_start_pred):(i_day_hour_end_pred)]  #AT\n",
    "            vb1 = data_vectors_list[6][(i_day_hour_start_pred):(i_day_hour_end_pred)]  #PR\n",
    "            vb3 = data_vectors_list[7][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH1\n",
    "            vb4 = data_vectors_list[8][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH2\n",
    "            vb5 = data_vectors_list[9][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH3        \n",
    "        #Station 2\n",
    "        if nstations > 2:\n",
    "            vc2 = data_vectors_list[10][(i_day_hour_start_pred):(i_day_hour_end_pred)]  #AT\n",
    "            vc1 = data_vectors_list[11][(i_day_hour_start_pred):(i_day_hour_end_pred)]  #PR\n",
    "            vc3 = data_vectors_list[12][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH1\n",
    "            vc4 = data_vectors_list[13][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH2\n",
    "            vc5 = data_vectors_list[14][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH3        \n",
    "        #Station 3\n",
    "        if nstations > 3:\n",
    "            vd2 = data_vectors_list[15][(i_day_hour_start_pred):(i_day_hour_end_pred)]  #AT\n",
    "            vd1 = data_vectors_list[16][(i_day_hour_start_pred):(i_day_hour_end_pred)]  #PR\n",
    "            vd3 = data_vectors_list[17][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH1\n",
    "            vd4 = data_vectors_list[18][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH2\n",
    "            vd5 = data_vectors_list[19][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH3        \n",
    "\n",
    "        if is_first == 0:\n",
    "            if nstations == 1:\n",
    "                vc_row = np.concatenate((va1,va2,va3,va4,va5))\n",
    "            if nstations == 2:\n",
    "                vc_row = np.concatenate((va1,va2,va3,va4,va5,vb1,vb2,vb3,vb4,vb5))\n",
    "            if nstations == 3:\n",
    "                vc_row = np.concatenate((va1,va2,va3,va4,va5,vb1,vb2,vb3,vb4,vb5,vc1,vc2,vc3,vc4,vc5))\n",
    "            if nstations == 4:\n",
    "                vc_row = np.concatenate((va1,va2,va3,va4,va5,vb1,vb2,vb3,vb4,vb5,vc1,vc2,vc3,vc4,vc5,vd1,vd2,vd3,vd4,vd5))\n",
    "\n",
    "            vmt_full_set = np.vstack((vmt_full_set, vc_row))\n",
    "            print(vmt_full_set.shape)\n",
    "        else:\n",
    "            is_first = 0\n",
    "            \n",
    "            if nstations == 1:\n",
    "                vmt_full_set = np.concatenate((va1,va2,va3,va4,va5))\n",
    "            if nstations == 2:\n",
    "                vmt_full_set = np.concatenate((va1,va2,va3,va4,va5,vb1,vb2,vb3,vb4,vb5))\n",
    "            if nstations == 3:\n",
    "                vmt_full_set = np.concatenate((va1,va2,va3,va4,va5,vb1,vb2,vb3,vb4,vb5,vc1,vc2,vc3,vc4,vc5))\n",
    "            if nstations == 4:\n",
    "                vmt_full_set = np.concatenate((va1,va2,va3,va4,va5,vb1,vb2,vb3,vb4,vb5,vc1,vc2,vc3,vc4,vc5,vd1,vd2,vd3,vd4,vd5))\n",
    "\n",
    "    return vmt_full_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_make_matrix_pr06(stid_keys,nstations,ipd,nhours,ndays,nyears_row,nyears_col,n_metrics):\n",
    "    #Create a matrix for precipitation data that feeds\n",
    "    #The S4 model\n",
    "    # 1. Read in all data files for multiple stations\n",
    "    # 2. Find the common starting date that will be used to align the matrices\n",
    "    # 3. Keep only the values that fit the common starting date\n",
    "    # 4. Combine the data into one large matrix for the StationID \n",
    "\n",
    "    # 1. Read in data files and get start date v_date... for each\n",
    "    # Check that data exist for all metrics\n",
    "\n",
    "    #Create two objects of length 12\n",
    "    #data_list which contains the metrics for stations 1-4, the first is the target\n",
    "    #data_start_date which contains the most recent date for each file\n",
    "    vc_first_rcd = [0,3,6,9]\n",
    "    num_stations = len(vc_first_rcd)\n",
    "    vc_st_id = stid_keys.iloc[:,0]\n",
    "    vc_path = stid_keys.iloc[:,3]\n",
    "    vc_fnames = stid_keys.iloc[:,2]\n",
    "\n",
    "    data_list = []\n",
    "    data_start_date = []\n",
    "    \n",
    "    for i in range(0, num_stations):\n",
    "            #Air temperature (one variable recorded once an hour a day)\n",
    "            infile = vc_path.iloc[vc_first_rcd[i]] + \"\\\\\" + vc_fnames.iloc[vc_first_rcd[i]]\n",
    "            df_at = pd.read_csv(infile)\n",
    "            data_list.append(df_at)\n",
    "            dt_first = datetime.strptime(df_at.iloc[0,2], \"%Y-%m-%d\").date()\n",
    "            data_start_date.append(dt_first)\n",
    "            #print(dt_first)\n",
    "            \n",
    "            #Precipitation (one variable recorded once an hour a day)\n",
    "            infile = vc_path.iloc[vc_first_rcd[i]] + \"\\\\\" + vc_fnames.iloc[vc_first_rcd[i]+1] \n",
    "            df_pr = pd.read_csv(infile)\n",
    "            data_list.append(df_pr)\n",
    "            dt_first = datetime.strptime(df_pr.iloc[0,2], \"%Y-%m-%d\").date()\n",
    "            data_start_date.append(dt_first)\n",
    "            #print(dt_first)\n",
    "            \n",
    "            #Relative Humidity (3 variables recorded once a day)\n",
    "            infile = vc_path.iloc[vc_first_rcd[i]] + \"\\\\\" + vc_fnames.iloc[vc_first_rcd[i]+2]\n",
    "            df_rh = pd.read_csv(infile)\n",
    "            data_list.append(df_rh)\n",
    "            dt_first = datetime.strptime(df_rh.iloc[0,4], \"%Y-%m-%d\").date()\n",
    "            data_start_date.append(dt_first)\n",
    "            #print(dt_first)\n",
    " \n",
    "     # 2. Find the offsets needed to all datasets start at same starting date\n",
    "    v_date_minimum = min(data_start_date)\n",
    "    #print(v_date_minimum)\n",
    "\n",
    "    #Identify offsets to use so all datadata vectors are appropriately aligned \n",
    "    upper_range = len(data_start_date)\n",
    "    \n",
    "    date_offsets_list = []\n",
    "    for i in range(0, upper_range):\n",
    "        date_offset = data_start_date[i] - v_date_minimum\n",
    "        #Temporarily set to 0, revise later\n",
    "        #date_offset = 0\n",
    "        date_offsets_list.append(date_offset)\n",
    "    \n",
    "    # 3. Get vectors of data for each measure\n",
    "    #These vectors all start on the same date\n",
    "\n",
    "    data_vectors_list = []\n",
    "    for i in vc_first_rcd:\n",
    "        tdt = data_list[i]\n",
    "        tof = date_offsets_list[i]\n",
    "        \n",
    "        i_start = tof.days\n",
    "        i_end = len(tdt.iloc[:,1]) - i_start\n",
    "        vc_at1 = tdt.iloc[i_start:i_end,1]\n",
    "        data_vectors_list.append(vc_at1)\n",
    "\n",
    "        tdt = data_list[i+1]\n",
    "        tof = date_offsets_list[i+1]\n",
    "        i_start = tof.days\n",
    "        i_end = len(tdt.iloc[:,1]) - i_start\n",
    "        vc_pr1 = tdt.iloc[i_start:i_end,1]\n",
    "        data_vectors_list.append(vc_pr1)\n",
    "            \n",
    "        tdt = data_list[i+2]\n",
    "        tof = date_offsets_list[i+2]\n",
    "        i_start = tof.days\n",
    "        i_end = len(tdt.iloc[:,1]) - i_start        \n",
    "        vc_rh1 = tdt.iloc[i_start:i_end,1]\n",
    "        #adjust from 1 a day to 24 a day measures\n",
    "        vc_rh1_24 = np.repeat(vc_rh1, nhours)\n",
    "        data_vectors_list.append(vc_rh1_24)\n",
    "    \n",
    "        tdt = data_list[i+2]\n",
    "        tof = date_offsets_list[i+2]\n",
    "        i_start = tof.days\n",
    "        i_end = len(tdt.iloc[:,1]) - i_start        \n",
    "        vc_rh2 = tdt.iloc[i_start:i_end,2]\n",
    "        #adjust from 1 a day to 24 a day measures\n",
    "        vc_rh2_24 = np.repeat(vc_rh2, nhours)\n",
    "        data_vectors_list.append(vc_rh2_24)\n",
    "\n",
    "        tdt = data_list[i+2]\n",
    "        tof = date_offsets_list[i+2]\n",
    "        i_start = tof.days\n",
    "        i_end = len(tdt.iloc[:,1]) - i_start        \n",
    "        vc_rh3 = tdt.iloc[i_start:i_end,3]\n",
    "        #adjust from 1 a day to 24 a day measures\n",
    "        vc_rh3_24 = np.repeat(vc_rh3, nhours)\n",
    "        data_vectors_list.append(vc_rh3_24)\n",
    "\n",
    "    # for i in range(0,20):\n",
    "    #     print(f\"Length vector: {len(data_vectors_list[i])}\")\n",
    "        \n",
    "    # 4. Interleave vectors so we get a complete vector of all metrics\n",
    "    #Start with the once a day vectors and loop over the number of days as defined by once a day vector\n",
    "    #These set how many years of data are needed : nyears_row + nyears_col\n",
    "    nyears_data_limit = nyears_row + nyears_col + 1\n",
    "    nrows = nhours*ndays*nyears_row\n",
    "    ncols = nhours*ndays*nyears_col \n",
    "        \n",
    "    # Set the number of metrics created each day\n",
    "    # And ordered from most recent hour to most distant hour from left to right\n",
    "    # Day metrics are also ordered most recent day to most distant day left to right\n",
    "    ndaily_metrics = n_metrics*nhours\n",
    "\n",
    "    #Set the total number of days of data needed for the model\n",
    "    num_days = nyears_data_limit*ndays\n",
    "    num_days_hrs = nhours*num_days\n",
    "    #num_days_hrs = nhours*100\n",
    "\n",
    "    print(f\"Number of years total needed: {nyears_data_limit}\")\n",
    "    print(f\"Number of days total needed: {num_days}\")\n",
    "    print(f\"Number of days and hours: {num_days_hrs}\")\n",
    "    print(f\"Number of matrix columns per metric: {ncols}\")\n",
    "    print(f\"Number of total matrix columns: {ncols*n_metrics}\")\n",
    "    print(f\"Number of matrix rows: {nrows}\")\n",
    "    #print(f\"tgt : {data_vectors_list[1][0:23]}\")\n",
    "    vmt_full_set = np.empty((nrows,ncols*n_metrics*nstations),np.float16)\n",
    "    is_first = 1\n",
    "\n",
    "    #Construct the vector that holds all data values (valid and missing)\n",
    "    #Assumes that all vectors in data_vectors_list match on date start\n",
    "    #data are organized by day, target station, other stations\n",
    "    #for i_day in range(num_days_hrs):\n",
    "    for i_day in range(nrows):\n",
    "        #print(i_day)\n",
    "        i_day_hour_end_tgt = i_day + ncols\n",
    "        i_day_hour_start_tgt = i_day\n",
    "        \n",
    "        i_day_hour_end_pred = i_day_hour_end_tgt + 1\n",
    "        i_day_hour_start_pred = i_day_hour_start_tgt + 1\n",
    "        \n",
    "        #print(f\"start :{i_day_hour_start_tgt}\")\n",
    "        #print(f\"end :{i_day_hour_end_tgt}\")\n",
    "        \n",
    "        #Target Station\n",
    "        if nstations > 0:\n",
    "            va2 = data_vectors_list[0][(i_day_hour_start_pred):(i_day_hour_end_pred)]  #AT\n",
    "            va1 = data_vectors_list[1][(i_day_hour_start_tgt):(i_day_hour_end_tgt)]  #PR\n",
    "            va3 = data_vectors_list[2][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH1\n",
    "            va4 = data_vectors_list[3][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH2\n",
    "            va5 = data_vectors_list[4][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH3\n",
    "        #Station 1\n",
    "        if nstations > 1:\n",
    "            vb2 = data_vectors_list[5][(i_day_hour_start_pred):(i_day_hour_end_pred)]  #AT\n",
    "            vb1 = data_vectors_list[6][(i_day_hour_start_pred):(i_day_hour_end_pred)]  #PR\n",
    "            vb3 = data_vectors_list[7][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH1\n",
    "            vb4 = data_vectors_list[8][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH2\n",
    "            vb5 = data_vectors_list[9][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH3        \n",
    "        #Station 2\n",
    "        if nstations > 2:\n",
    "            vc2 = data_vectors_list[10][(i_day_hour_start_pred):(i_day_hour_end_pred)]  #AT\n",
    "            vc1 = data_vectors_list[11][(i_day_hour_start_pred):(i_day_hour_end_pred)]  #PR\n",
    "            vc3 = data_vectors_list[12][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH1\n",
    "            vc4 = data_vectors_list[13][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH2\n",
    "            vc5 = data_vectors_list[14][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH3        \n",
    "        #Station 3\n",
    "        if nstations > 3:\n",
    "            vd2 = data_vectors_list[15][(i_day_hour_start_pred):(i_day_hour_end_pred)]  #AT\n",
    "            vd1 = data_vectors_list[16][(i_day_hour_start_pred):(i_day_hour_end_pred)]  #PR\n",
    "            vd3 = data_vectors_list[17][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH1\n",
    "            vd4 = data_vectors_list[18][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH2\n",
    "            vd5 = data_vectors_list[19][(i_day_hour_start_pred):(i_day_hour_end_pred)]   #RH3        \n",
    "\n",
    "        if is_first == 0:\n",
    "            if nstations == 1:\n",
    "                vc_row = np.concatenate((va1,va2,va3,va4,va5))\n",
    "            if nstations == 2:\n",
    "                vc_row = np.concatenate((va1,va2,va3,va4,va5,vb1,vb2,vb3,vb4,vb5))\n",
    "            if nstations == 3:\n",
    "                vc_row = np.concatenate((va1,va2,va3,va4,va5,vb1,vb2,vb3,vb4,vb5,vc1,vc2,vc3,vc4,vc5))\n",
    "            if nstations == 4:\n",
    "                vc_row = np.concatenate((va1,va2,va3,va4,va5,vb1,vb2,vb3,vb4,vb5,vc1,vc2,vc3,vc4,vc5,vd1,vd2,vd3,vd4,vd5))\n",
    "\n",
    "            vmt_full_set[i_day] = vc_row\n",
    "            #vmt_full_set = np.vstack((vmt_full_set, vc_row))\n",
    "            # print(vmt_full_set.shape)\n",
    "        else:\n",
    "            is_first = 0\n",
    "            \n",
    "            if nstations == 1:\n",
    "                vmt_full_set[i_day] = np.concatenate((va1,va2,va3,va4,va5))\n",
    "            if nstations == 2:\n",
    "                vmt_full_set[i_day] = np.concatenate((va1,va2,va3,va4,va5,vb1,vb2,vb3,vb4,vb5))\n",
    "            if nstations == 3:\n",
    "                vmt_full_set[i_day] = np.concatenate((va1,va2,va3,va4,va5,vb1,vb2,vb3,vb4,vb5,vc1,vc2,vc3,vc4,vc5))\n",
    "            if nstations == 4:\n",
    "                vmt_full_set[i_day] = np.concatenate((va1,va2,va3,va4,va5,vb1,vb2,vb3,vb4,vb5,vc1,vc2,vc3,vc4,vc5,vd1,vd2,vd3,vd4,vd5))\n",
    "\n",
    "    return vmt_full_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temp to test fn06\n",
    "#Parameters governing data matrix\n",
    "# stid_keys = dt_analysis_set_keys\n",
    "# nstations = 3\n",
    "# ipd = ipd02\n",
    "# nhours = 24\n",
    "# ndays = 365\n",
    "# nyears_row = 3 \n",
    "# nyears_col = 2\n",
    "# n_metrics = 5\n",
    "\n",
    "#rtn_matrix = fn_make_matrix_pr06(stid_keys,nstations,ipd,nhours,ndays,nyears_row,nyears_col,n_metrics)\n",
    "#print(rtn_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = {\n",
    "#     'st_id': [72508014740,72508014740,72508014740, 72504094702,72504094702,72504094702,72510094746,72510094746,72510094746,72505004781,72505004781,72505004781],\n",
    "#     'target_station': [72508014740,72508014740,72508014740,72508014740,72508014740,72508014740,72508014740,72508014740,72508014740,72508014740,72508014740,72508014740],\n",
    "#     'fnames': ['72508014740_model_data_airtemp.csv', '72508014740_model_data_precip.csv', '72508014740_model_data_relhum.csv',\n",
    "#         '72504094702_model_data_airtemp.csv', '72504094702_model_data_precip.csv', '72504094702_model_data_relhum.csv',\n",
    "#         '72510094746_model_data_airtemp.csv', '72510094746_model_data_precip.csv', '72510094746_model_data_relhum.csv',\n",
    "#         '72505004781_model_data_airtemp.csv', '72505004781_model_data_precip.csv', '72505004781_model_data_relhum.csv'],\n",
    "#     'path': ['D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\',\n",
    "#             'D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\'],\n",
    "#     'vect_close_bkt2': [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4]\n",
    "# }\n",
    "\n",
    "# dt_analysis_set_keys = pd.DataFrame(data)\n",
    "# dt_analysis_set_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 1.  Select a station using ID to get its related stations\n",
    "# Get file of target stations and their related stations\n",
    "dt_trg_set = ipd02 + \"Target_station_analysis_set.csv\"\n",
    "lst_target_stations = [\n",
    "  \"74486094789\",\n",
    "  \"72518014735\",\n",
    "  \"72508014740\",\n",
    "  \"72526014860\",\n",
    "  \"72406093721\",\n",
    "  \"72401013740\",\n",
    "  \"72219013874\",\n",
    "  \"72423093821\",\n",
    "  \"72428014821\",\n",
    "  \"72327013897\"\n",
    "]\n",
    "\n",
    "#Select the target station for which predictions will be made\n",
    "#72508014740 is JFK airport\n",
    "st_id = lst_target_stations[2]\n",
    "num_st_keep = 4   #num of stations to keep for analysis\n",
    "analysis_set = dt_trg_set\n",
    "dt_analysis_set = pd.read_csv(analysis_set)\n",
    "dt_analysis_set['path'] = ipd02\n",
    "dt01 = dt_analysis_set[dt_analysis_set['target_station']==int(st_id)]\n",
    "#uniq_stat = dt01['vect_close_bkt2'].unique()[:2]\n",
    "uniq_station = dt01['st_id'].unique()[:num_st_keep]\n",
    "dt02 = dt01[dt01['st_id'].isin(uniq_station)]\n",
    "\n",
    "selected_columns = ['st_id','target_station',\n",
    "                    'fnames','path','vect_close_bkt2']\n",
    "#The output below is fed to STEP 2 to run the analysis\n",
    "dt_analysis_set_keys = dt02[selected_columns].copy()\n",
    "\n",
    "# dt_analysis_set_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\CodeLibrary\\Python\\weathermetrics\\data\\weathermetrics\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prajw\\AppData\\Local\\Temp\\ipykernel_22976\\4106003547.py:27: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_at = pd.read_csv(infile)\n",
      "C:\\Users\\prajw\\AppData\\Local\\Temp\\ipykernel_22976\\4106003547.py:35: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_pr = pd.read_csv(infile)\n",
      "C:\\Users\\prajw\\AppData\\Local\\Temp\\ipykernel_22976\\4106003547.py:27: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_at = pd.read_csv(infile)\n",
      "C:\\Users\\prajw\\AppData\\Local\\Temp\\ipykernel_22976\\4106003547.py:35: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_pr = pd.read_csv(infile)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of years total needed: 6\n",
      "Number of days total needed: 2190\n",
      "Number of days and hours: 52560\n",
      "Number of matrix columns per metric: 17520\n",
      "Number of total matrix columns: 87600\n",
      "Number of matrix rows: 26280\n"
     ]
    }
   ],
   "source": [
    "#STEP 2.  Use elements created in STEP 1 to get a data matrix for analysis\n",
    "#All the available stations which have all three metrics needed for model\n",
    "#vc_stid = dt_analysis_set_keys[['st_id']].drop_duplicates()\n",
    "\n",
    "#Organize the data based on the nature of the data so they all conform to the same size matrices\n",
    "#The metrics included are\n",
    "#  AT - air temperature in Celcius\n",
    "#  RH - relative humidity in Percent\n",
    "#  PR - precipitation in MM\n",
    "\n",
    "#Put them all in appropriate matrices and create list of metrics\n",
    "#Data files are of two types\n",
    "#    -  24 hour * 365 days * n years  (AT, PR)\n",
    "#    -  365 days * n years (RH)\n",
    "\n",
    "#Parameters governing data matrix\n",
    "in_data = dt_analysis_set_keys\n",
    "in_nstations = 4 #nstations\n",
    "in_path = ipd02\n",
    "nhours = 24\n",
    "ndays = 365\n",
    "nyears_row = 3\n",
    "nyears_col = 2\n",
    "n_metrics = 5\n",
    "\n",
    "print(ipd02)\n",
    "rtn_matrix = fn_make_matrix_pr06(in_data,in_nstations,in_path,nhours,ndays,nyears_row,nyears_col,n_metrics)\n",
    "# rtn_mtx_pqt = pd.DataFrame(rtn_matrix)\n",
    "# rtn_mtx_pqt.to_parquet(f\"{ipd02}{st_id}_2st_2RYr_2CYr.parquet\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26280, 43800)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rtn_matrix.shape\n",
    "rtn_mtx_pqt = pd.DataFrame(rtn_matrix)\n",
    "rtn_mtx_pqt.to_parquet(f\"{ipd02}{st_id}_{in_nstations}stations_{nyears_row}row_{nyears_col}col.parquet\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_make_matrix(vc_measure,offset_measure,nhours,ndays,nyears_col,nyears_row):\n",
    "    #the offset from the start of the vector to align the data points\n",
    "    vector = vc_measure\n",
    "\n",
    "    nrows = nhours*ndays*nyears_row   #the number of values put in each row of matrix days*hours*years\n",
    "    ncols = nhours*ndays*nyears_col   #the number of values put in each row of matrix days*hours*years\n",
    "    \n",
    "    # Create an empty matrix to store the results\n",
    "    matrix = np.zeros((nrows, ncols))\n",
    "\n",
    "    # Fill the matrix\n",
    "    for i in range(nrows):\n",
    "        matrix[i] = vector[(offset_measure + i):(offset_measure + i + ncols)]\n",
    "     \n",
    "    return matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-inf, 5, 20, 25, inf] [0, 1, 2, 3]\n",
      "Dataset:  (25982, 350400)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "category",
         "type": "unknown"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "1530e7c5-271d-4e15-b73f-c79a6c224a86",
       "rows": [
        [
         "0",
         "25656"
        ],
        [
         "1",
         "277"
        ],
        [
         "2",
         "14"
        ],
        [
         "3",
         "35"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 4
       }
      },
      "text/plain": [
       "0\n",
       "0    25656\n",
       "1      277\n",
       "2       14\n",
       "3       35\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform_matrix(df, bins, labels):\n",
    "    \"\"\"\n",
    "    Transforms the given matrix by binning the first column, dropping the first row and column,\n",
    "    and adding the binned column as the first column.\n",
    "\n",
    "    Parameters:\n",
    "        matrix (pd.DataFrame): The input matrix.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The transformed matrix.\n",
    "    \"\"\"\n",
    "    # Extract the first column (dependent variable)\n",
    "    firstcolumn = df.iloc[:, 0].astype('float32')\n",
    "\n",
    "    # Bin the first column\n",
    "    binned_column = pd.cut(firstcolumn, bins=bins, labels=labels)\n",
    "\n",
    "    # Drop the first first column\n",
    "    df = df.iloc[:, 1:]\n",
    "\n",
    "    # Add the binned column as the first column\n",
    "    df.insert(0, 0, binned_column.iloc[0:].values)\n",
    "\n",
    "    # Return the final matrix\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "# Example matrix (replace this with your actual matrix)\n",
    "jj   = rtn_matrix\n",
    "df = pd.DataFrame(jj).iloc[1:,] # Convert to DataFrame and drop the first row for binning\n",
    "# Drop rows where the first column has missing values\n",
    "# df = df.dropna(subset=[0])\n",
    "fc = df.iloc[:,0].astype('float32') # Ensure the first column is float for binning\n",
    "bins = [-float('inf'), 5, 20, 25, float('inf')]\n",
    "\n",
    "# bins = [-float('inf'), 1, float('inf')]\n",
    "# bins = [-float('inf'), 0] + list(pd.qcut(fc[fc > 0], q=4, retbins=True)[1][1:]) + [float('inf')]\n",
    "# labels = [0, 1]\n",
    "# print(bins, labels)\n",
    "\n",
    "\n",
    "# bins = [-float('inf'), 0] + list(pd.qcut(fc[fc > 0], q=4, retbins=True)[1][1:]) + [float('inf')]\n",
    "labels = [0, 1, 2, 3]\n",
    "print(bins, labels)\n",
    "\n",
    "df_transformed = transform_matrix(df, bins, labels)\n",
    "df_transformed = df_transformed[df_transformed[0]>= 0]\n",
    "print(\"Dataset: \", df_transformed.shape)\n",
    "df_transformed[0].value_counts(dropna=False).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: (18188, 350400)\n",
      "0\n",
      "0    17936\n",
      "1      211\n",
      "2        9\n",
      "3       32\n",
      "Name: count, dtype: int64\n",
      "Validation Set: (2598, 350400)\n",
      "0\n",
      "0    2570\n",
      "1      27\n",
      "2       0\n",
      "3       1\n",
      "Name: count, dtype: int64\n",
      "Test Set: (5196, 350400)\n",
      "0\n",
      "0    5150\n",
      "1      39\n",
      "2       5\n",
      "3       2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def split_time_series_data(df, train_ratio=0.7, val_ratio=0.1, test_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Splits a time series dataset into training, validation, and test sets.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input time series dataset, ordered in descending dates.\n",
    "        train_ratio (float): The proportion of data to use for training.\n",
    "        val_ratio (float): The proportion of data to use for validation.\n",
    "        test_ratio (float): The proportion of data to use for testing.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing three DataFrames (train, validation, test).\n",
    "    \"\"\"\n",
    "    assert train_ratio + val_ratio + test_ratio == 1.0, \"Ratios must sum to 1.0\"\n",
    "\n",
    "    # Calculate split indices\n",
    "    n = len(df)\n",
    "    test_end = int(n * test_ratio)    \n",
    "    val_end = test_end + int(n * val_ratio)\n",
    "    \n",
    "\n",
    "    # Split the data\n",
    "    test_data = df.iloc[:test_end].sort_index(ascending=False)\n",
    "    val_data = df.iloc[test_end:val_end].sort_index(ascending=False)\n",
    "    train_data = df.iloc[val_end:].sort_index(ascending=False)\n",
    "\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "# Example usage\n",
    "train_data, val_data, test_data = split_time_series_data(df_transformed)\n",
    "print(\"Training Set:\", train_data.shape)\n",
    "print(train_data[0].value_counts(dropna=False).sort_index())\n",
    "print(\"Validation Set:\", val_data.shape)\n",
    "print(val_data[0].value_counts(dropna=False).sort_index())\n",
    "print(\"Test Set:\", test_data.shape)\n",
    "print(test_data[0].value_counts(dropna=False).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_data.iloc[5, 0] = 2\n",
    "# print(\"Validation Set:\", val_data.shape)\n",
    "# print(val_data[0].value_counts(dropna=False).sort_index())\n",
    "# val_data.to_parquet(f\"{output_dir}{station_id}_validation.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets exported successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the station ID and output directory\n",
    "station_id = st_id\n",
    "#output_dir = \"D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\\"\n",
    "output_dir = ipd02\n",
    "\n",
    "\n",
    "# Export datasets to CSV\n",
    "train_data.to_parquet(f\"{output_dir}{station_id}_train.parquet\", index=False)\n",
    "val_data.to_parquet(f\"{output_dir}{station_id}_validation.parquet\", index=False)\n",
    "test_data.to_parquet(f\"{output_dir}{station_id}_test.parquet\", index=False)\n",
    "\n",
    "# train_data.to_csv(f\"{output_dir}{station_id}_train.csv\", index=False)\n",
    "# val_data.to_csv(f\"{output_dir}{station_id}_validation.csv\", index=False)\n",
    "# test_data.to_csv(f\"{output_dir}{station_id}_test.csv\", index=False)\n",
    "\n",
    "print(\"Datasets exported successfully.\")\n",
    "\n",
    "#python -m s4model --modelname 72508014740 --trainset ../data/weathermetrics/72508014740_train.csv --valset ../data/weathermetrics/72508014740_validation.csv --testset ../data/weathermetrics/72508014740_test.csv --tabulardata --dependent_variable 0 --epochs 30\n",
    "\n",
    "#python -m s4model --modelname 72508014740 --trainset \\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72508014740_train.parquet --valset \\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72508014740_validation.parquet --testset \\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72508014740_test.parquet --tabulardata --dependent_variable 0 --epochs 30\n",
    "\n",
    "#python -m s4model --modelname 72508014740 --trainset ../data/weathermetrics/72508014740_train.parquet --valset ../data/weathermetrics/72508014740_validation.parquet --testset ../data/weathermetrics/72508014740_test.parquet --tabulardata --dependent_variable 0 --epochs 30\n",
    "\n",
    "\n",
    "# python -m charts --df ../results/72508014740_Test_results_20250424_104121PM.csv --actual 0 --predicted Predicted \n",
    "# python -m charts --df ../results/72508014740_Test_results_20250427_100132PM.csv --actual 0 --predicted Predicted \n",
    "# python -m charts --df ../results/72508014740_Test_results_20250428_091259PM.csv --actual 0 --predicted Predicted \n",
    "# python -m charts --df ../results/72508014740_Test_results_20250428_094033PM.csv --actual 0 --predicted Predicted \n",
    "# python -m charts --df ../results/72508014740_Test_results_20250429_091538PM.csv --actual 0 --predicted Predicted\n",
    "# python -m charts --df ../results/72508014740_Test_results_20250601_102151PM.csv --actual 0 --predicted Predicted\n",
    "# python -m charts --df ../results/72508014740_Test_results_20250608_012954PM.csv --actual 0 --predicted Predicted\n",
    "# python -m charts --df ../results/72508014740_Test_results_20250608_015053PM.csv --actual 0 --predicted Predicted\n",
    "# python -m charts --df ../results/72508014740_Test_results_20250608_022058PM.csv --actual 0 --predicted Predicted\n",
    "# python -m charts --df ../results/72508014740_Test_results_20250608_100414PM.csv --actual 0 --predicted Predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# st_id\n",
    "# ipd02\n",
    "# python -m charts --df ../results/72508014740_Test_results_20250527_093455PM.csv --actual 0 --predicted Predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_first_non_nan(vector):\n",
    "#     return next((i for i, x in enumerate(vector) if not math.isnan(x)), None)\n",
    "\n",
    "# first_number_at1 = find_first_non_nan(vc_at1)\n",
    "# first_number_rh1 = find_first_non_nan(vc_rh1)\n",
    "# first_number_rh2 = find_first_non_nan(vc_rh2)\n",
    "# first_number_rh3 = find_first_non_nan(vc_rh3)\n",
    "# first_number_pr1 = find_first_non_nan(vc_pr1)\n",
    "\n",
    "# #Number of missing data points\n",
    "# print(f\"AT1 Number of missing cells: {np.count_nonzero(np.isnan(vc_at1))}\")\n",
    "# print(f\"AT1 Vector Length: {len(vc_at1)}\")\n",
    "\n",
    "# print(f\"RH1 Number of missing cells: {np.count_nonzero(np.isnan(vc_rh1))}\")\n",
    "# print(f\"RH1 Vector Length: {len(vc_rh1)}\")\n",
    "\n",
    "# print(f\"PR1 Number of missing cells: {np.count_nonzero(np.isnan(vc_pr1))}\")\n",
    "# print(f\"PR1 Vector Length: {len(vc_pr1)}\")\n",
    "\n",
    "# #Once a day metrics\n",
    "# v_data_first_once_day = max(\n",
    "#                    first_number_rh1,\n",
    "#                    first_number_rh2,\n",
    "#                    first_number_rh3)\n",
    "# print(v_data_first_once_day)\n",
    "\n",
    "# #24 times a day metrics\n",
    "# v_data_first_24h_day = max(first_number_at1,\n",
    "#                    first_number_pr1)\n",
    "# print(v_data_first_24h_day)\n",
    "\n",
    "\n",
    "#These vectors all start with a numeric value so missing values can be filled\n",
    "\n",
    "\n",
    "#print(first_number_at1)\n",
    "#print(first_number_rh1)\n",
    "#print(first_number_rh2)\n",
    "#print(first_number_rh3)\n",
    "#print(first_number_pr1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43800,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(jj)\n",
    "fc = df.iloc[:,0].astype('float32')\n",
    "print(fc.shape)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(fc, bins=50, color='blue', alpha=0.7, density=True)\n",
    "plt.title('Normalized Histogram of a')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 0.0\n",
      "Max: 94.5\n",
      "Percentiles: [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  94.5]\n"
     ]
    }
   ],
   "source": [
    "# Custom bins\n",
    "\n",
    "a = fc[~fc.isnull()]\n",
    "\n",
    "# Calculate min, max, and percentiles by 10 for 'a'\n",
    "min_val = a.min()\n",
    "max_val = a.max()\n",
    "percentiles = np.percentile(a, np.arange(0, 101, 10))\n",
    "\n",
    "print(f\"Min: {min_val}\")\n",
    "print(f\"Max: {max_val}\")\n",
    "print(f\"Percentiles: {percentiles}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
