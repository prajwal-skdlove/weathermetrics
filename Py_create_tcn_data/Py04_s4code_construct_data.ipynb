{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IV Load the data for a station and organize for precipitation model\n",
    "import os as os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths\n",
    "#Need to set up based on who is running\n",
    "usr = \"JH\"\n",
    "if usr == \"PK\":\n",
    "    ipd01 = \"\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\2024\\\\\"\n",
    "    ipd02 = \"D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\\"\n",
    "    ipd03 = \"\"\n",
    "    ipd04 = \"\"\n",
    "    \n",
    "if usr == \"JH\":\n",
    "    ipd01 = \"\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\2024\\\\\"\n",
    "    ipd02 = \"\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\\"\n",
    "    ipd03 = \"\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\Py_S4\\\\Py_S4_v02_JHH\\\\NCEI_data\\\\\"\n",
    "    ipd04 = \"\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\Py_S4\\\\Py_S4_v02_JHH\\\\NCEI_parquet_files\\\\\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_select_data_vectors_pr07(stid_keys,nstations,nhours,nmonths_row,nmonths_col):\n",
    "    #Get the matrix of data to input into marix\n",
    "    #The S4 model\n",
    "    # 1. Read in combined data files for multiple stations\n",
    "    # 2. Find the common dates that will be used to align the matrices\n",
    "    #    Keep only the values that fit the common starting date \n",
    "\n",
    "    # 1. Read in data files and get start date v_date... for each\n",
    "    # Check that data exist for all metrics\n",
    "\n",
    "    df_stid = stid_keys\n",
    "    df_stid2 = df_stid.iloc[:, [1]]\n",
    "    df_stid2 = df_stid2.drop_duplicates() # unique()\n",
    "    \n",
    "    n_rows_stid = min(len(df_stid2),nstations)\n",
    "    \n",
    "    #Matrix in which measures are stored\n",
    "    n_vmt_rows = nhours*30*max(nmonths_row,nmonths_col)\n",
    "    \n",
    "    data_list = []\n",
    "    date_max_list =[]\n",
    "    \n",
    "    #1. Read in data files\n",
    "    for i in range(0, n_rows_stid):\n",
    "    #for i in range(0, 28):\n",
    "        infile = ipd03 + str(df_stid2.iloc[i,0]) + \"_model_data_combined.csv\"\n",
    "        print(infile)\n",
    "        \n",
    "        #Eliminate fully blank lines\n",
    "        cleaned_lines = []\n",
    "        with open(infile, 'r', newline='') as infile:\n",
    "            reader = csv.reader(infile)\n",
    "            for row in reader:\n",
    "                # Example: Skip rows that are entirely empty or have only blank 'name'\n",
    "                if any(cell.strip() for cell in row) and (len(row) < 1 or row[0].strip() != ''): # Assuming 'name' is the first column\n",
    "                    cleaned_lines.append(row)\n",
    "        \n",
    "        df_metrics = pd.DataFrame(cleaned_lines[1:], columns=cleaned_lines[0]) # Assuming first row is header\n",
    "\n",
    "        df_metrics['str_hr'] = df_metrics['nHOUR']  #.apply(lambda x: f'{x:02d}')\n",
    "        df_metrics['date_hr'] = pd.to_datetime(df_metrics['Date_YYYYMMDD'] + ' ' + df_metrics['str_hr'])\n",
    "        df_metrics['month_number'] = df_metrics['date_hr'].dt.month\n",
    "        df_metrics['week_number'] = df_metrics['date_hr'].dt.isocalendar().week\n",
    "        df_metrics['stid'] = str(df_stid2.iloc[i,0])\n",
    "        data_list.append(df_metrics)\n",
    "\n",
    "    #2. Align dates across data files\n",
    "    for i in range(0, n_rows_stid):\n",
    "        df = data_list[i]\n",
    "        max_date = max(df['date_hr'])     \n",
    "        date_max_list.append(max_date)\n",
    "    \n",
    "    date_filter = min(date_max_list)\n",
    "\n",
    "    for i in range(0, n_rows_stid):\n",
    "        df = data_list[i]\n",
    "\n",
    "        filtered_df = df[df['date_hr'] < date_filter]\n",
    "        filtered_df = filtered_df.iloc[(len(filtered_df) - n_vmt_rows):len(filtered_df)]\n",
    "        \n",
    "        idx_col = filtered_df.columns\n",
    "        idx_col = str(df_stid2.iloc[i,0]) + '_' + idx_col\n",
    "        filtered_df.columns = idx_col\n",
    "\n",
    "        print(filtered_df.shape)\n",
    "        \n",
    "        data_list[i] = filtered_df\n",
    "        \n",
    "    return data_list[0:n_rows_stid]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_select_data_vectors_pr07xxx(stid_keys,nstations,nhours,nmonths_row,nmonths_col):\n",
    "    #Get the matrix of data to input into marix\n",
    "    #The S4 model\n",
    "    # 1. Read in combined data files for multiple stations\n",
    "    # 2. Find the common dates that will be used to align the matrices\n",
    "    #    Keep only the values that fit the common starting date \n",
    "\n",
    "    # 1. Read in data files and get start date v_date... for each\n",
    "    # Check that data exist for all metrics\n",
    "\n",
    "    df_stid = stid_keys\n",
    "    df_stid2 = df_stid.iloc[:, [0, 3, 4]]  #df_stid[df_stid['target_station'] == stid_target]\n",
    "    df_stid2 = df_stid2.drop_duplicates() # unique()\n",
    "\n",
    "    n_rows_stid = min(len(df_stid2),nstations)\n",
    "    \n",
    "    #Matrix in which measures are stored\n",
    "    n_vmt_rows = nhours*30*max(nmonths_row,nmonths_col)\n",
    "    #vmt_full_set = np.empty(n_vmt_rows,np.float16)\n",
    "    #is_first = 1\n",
    "    \n",
    "    data_list = []\n",
    "    date_max_list =[]\n",
    "\n",
    "    #1. Read in data files\n",
    "    for i in range(0, n_rows_stid):\n",
    "        infile = ipd03 + str(df_stid2.iloc[i,0]) + \"_model_data_combined.csv\"\n",
    "        print(infile)\n",
    "        df_metrics = pd.read_csv(infile)\n",
    "        df_metrics['str_hr'] = df_metrics['nHOUR'].apply(lambda x: f'{x:02d}')\n",
    "        df_metrics['date_hr'] = pd.to_datetime(df_metrics['Date_YYYYMMDD'] + ' ' + df_metrics['str_hr'])\n",
    "        df_metrics['month_number'] = df_metrics['date_hr'].dt.month\n",
    "        df_metrics['week_number'] = df_metrics['date_hr'].dt.isocalendar().week\n",
    "        df_metrics['stid'] = str(df_stid2.iloc[i,0])\n",
    "        data_list.append(df_metrics)\n",
    "\n",
    "    #2. Align dates across data files\n",
    "    for i in range(0, n_rows_stid):\n",
    "        df = data_list[i]\n",
    "        max_date = max(df['date_hr'])     \n",
    "        date_max_list.append(max_date)\n",
    "    \n",
    "    date_filter = min(date_max_list)\n",
    "\n",
    "    for i in range(0, n_rows_stid):\n",
    "        df = data_list[i]\n",
    "        #place holder for filtering to a specific end date\n",
    "        #date_filter = pd.to_datetime(\"2023-8-11\" + ' ' + \"17\")\n",
    "        filtered_df = df[df['date_hr'] < date_filter]\n",
    "        filtered_df = filtered_df.iloc[(len(filtered_df) - n_vmt_rows):len(filtered_df)]\n",
    "        \n",
    "        idx_col = filtered_df.columns\n",
    "        idx_col = str(df_stid2.iloc[i,0]) + '_' + idx_col\n",
    "        filtered_df.columns = idx_col\n",
    "\n",
    "        print(filtered_df.shape)\n",
    "        \n",
    "        data_list[i] = filtered_df\n",
    "        \n",
    "    return data_list[0:n_rows_stid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_create_data_matrix_pr07(lst_data,tgt_metric,nstations,nmetrics,nmonths_col):\n",
    "    #Create a matrix for precipitation data that feeds\n",
    "    #The S4 model\n",
    "    # 1. Get data list and select target metric\n",
    "    #Two choices now\n",
    "    #    - prec\n",
    "    #    - temp\n",
    "    # 2.  Create output and move data into approprate cells\n",
    "\n",
    "    # 1. get data list details and reduce to metrics\n",
    "    data_vectors_list = []\n",
    "    dt_rows = len(lst_data[0])\n",
    "    df00 = []\n",
    "\n",
    "    dt_rows_minus_1 = dt_rows - 1\n",
    "    \n",
    "    if tgt_metric == \"prec\": tgt_cols = [8,2,3,4,5,6,9,10]\n",
    "    if tgt_metric == \"temp\": tgt_cols = [8,3,2,4,5,6,9,10]\n",
    "    \n",
    "    #Reset the indices as all records are ordered by date and hour\n",
    "    for i in range(nstations):\n",
    "        df00.append(lst_data[i].iloc[:,tgt_cols])\n",
    "    \n",
    "    df01 = [df.reset_index(drop=True) for df in df00]\n",
    "    rs = pd.concat(df01, axis=1)\n",
    "    print(rs.shape)\n",
    "    #print(rs.iloc[1:10,:])\n",
    "    #Order from most recent to most distant in time\n",
    "    rs = rs.iloc[::-1]\n",
    "    #print(rs.iloc[1:10,:])\n",
    "    \n",
    "    #2. Create output matrix and move data into appropriate cells\n",
    "    \n",
    "    nmetrics_mon_week = nmetrics + 2\n",
    "    mt_cols = 1 + nstations*(nmetrics_mon_week)*nmonths_col*30\n",
    "    mt_rows = len(rs) #- nmonths_col*30*24 - 1\n",
    "    \n",
    "    mt_full_set = np.empty((mt_rows,mt_cols ),np.float16)\n",
    "\n",
    "    #Create vector to pull out the proper data columns\n",
    "    tgt_cols = []\n",
    "    for ns in range(1,nstations):    \n",
    "        lp = (ns-1)*8\n",
    "        temp = [1+lp,2+lp,3+lp,4+lp,5+lp,6+lp,7+lp]\n",
    "        tgt_cols = tgt_cols + temp\n",
    "    \n",
    "    #Create matrix of data\n",
    "    tgt_cols_num = len(tgt_cols)\n",
    "    #mt_rows = 10  #keep here for development until done\n",
    "    for i in range(0,mt_rows):\n",
    "        mt_full_set[i,range(0,tgt_cols_num)] = rs.iloc[i,tgt_cols].apply(pd.to_numeric, errors='coerce')\n",
    "   \n",
    "    print(mt_full_set.shape)\n",
    "    \n",
    "    #Add additional column sets\n",
    "    nc = nstations*nmetrics_mon_week\n",
    "    lpi = mt_rows - nmonths_col*30\n",
    "    #limit of rows in data set so all marix rows have data\n",
    "    lpj = nmonths_col*30 - nc  #number of colum sets in columns\n",
    "\n",
    "    for j in range(0,lpj):\n",
    "        for i in range(0,lpi):\n",
    "            row_get = i + j + 1\n",
    "            col_get_start = 0\n",
    "            col_get_end = nc\n",
    "\n",
    "            row_put = i\n",
    "            col_put_start = nc*(j+1)\n",
    "            col_put_end = col_put_start + nc\n",
    "\n",
    "            mt_full_set[row_put,col_put_start:col_put_end] = mt_full_set[row_get,col_get_start:col_get_end]\n",
    "            \n",
    "    return mt_full_set #rs #mt_full_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_create_data_matrix_pr07xxx(lst_data,tgt_metric,nstations,nmetrics,nmonths_col):\n",
    "    #Create a matrix for precipitation data that feeds\n",
    "    #The S4 model\n",
    "    # 1. Get data list and select target metric\n",
    "    #Two choices now\n",
    "    #    - prec\n",
    "    #    - temp\n",
    "    # 2.  Create output and move data into approprate cells\n",
    "\n",
    "    # 1. get data list details and reduce to metrics\n",
    "    data_vectors_list = []\n",
    "    dt_rows = len(lst_data[0])\n",
    "    df00 = []\n",
    "\n",
    "    dt_rows_minus_1 = dt_rows - 1\n",
    "    \n",
    "    if tgt_metric == \"prec\": tgt_cols = [8,2,3,4,5,6,9,10]\n",
    "    if tgt_metric == \"temp\": tgt_cols = [8,3,2,4,5,6,9,10]\n",
    "    \n",
    "    #Reset the indices as all records are ordered by date and hour\n",
    "    for i in range(nstations):\n",
    "        df00.append(lst_data[i].iloc[:,tgt_cols])\n",
    "    \n",
    "    df01 = [df.reset_index(drop=True) for df in df00]\n",
    "    rs = pd.concat(df01, axis=1)\n",
    "    print(rs.shape)\n",
    "    #print(rs.iloc[1:10,:])\n",
    "    #Order from most recent to most distant in time\n",
    "    rs = rs.iloc[::-1]\n",
    "    #print(rs.iloc[1:10,:])\n",
    "    \n",
    "    #2. Create output matrix and move data into appropratie cells\n",
    "    \n",
    "    nmetrics_mon_week = nmetrics + 2\n",
    "    mt_cols = 1 + nstations*(nmetrics_mon_week)*nmonths_col*30\n",
    "    mt_rows = len(rs) #- nmonths_col*30*24 - 1\n",
    "    \n",
    "    mt_full_set = np.empty((mt_rows,mt_cols ),np.float16)\n",
    "    \n",
    "    #rng = mt_rows\n",
    "    #Set up first column set of data\n",
    "    #tgt_cols = [1,2,3,4,5,7,8,9,10,11,13,14,15,16,17,19,20,21,22,23]\n",
    "    tgt_cols = [1,2,3,4,5,6,7,9,10,11,12,13,14,15,17,18,19,20,21,22,23,25,26,27,28,29,30,31]\n",
    "    \n",
    "    \n",
    "    tgt_cols_num = len(tgt_cols)\n",
    "    for i in range(0,mt_rows):\n",
    "        mt_full_set[i,range(0,tgt_cols_num)] = rs.iloc[i,tgt_cols]\n",
    "    \n",
    "\n",
    "    #Add additional column sets\n",
    "    nc = nstations*nmetrics_mon_week\n",
    "    lpi = mt_rows - nmonths_col*30\n",
    "    #limit of rows in data set so all marix rows have data\n",
    "    lpj = nmonths_col*30 - nc  #number of colum sets in columns\n",
    "    \n",
    "    for j in range(0,lpj):\n",
    "        for i in range(0,lpi):\n",
    "            row_get = i + j + 1\n",
    "            col_get_start = 0\n",
    "            col_get_end = nc\n",
    "\n",
    "            row_put = i\n",
    "            col_put_start = nc*(j+1)\n",
    "            col_put_end = col_put_start + nc\n",
    "\n",
    "            mt_full_set[row_put,col_put_start:col_put_end] = mt_full_set[row_get,col_get_start:col_get_end]\n",
    "    \n",
    "    return mt_full_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_create_tgt_matrix_pr07(mtx_data,tgt_metric,tgt_mod):\n",
    "    \n",
    "    mtx01 = mtx_data\n",
    "    \n",
    "    nrows = mtx01.shape[0]\n",
    "    ncols = mtx01.shape[1]\n",
    "    \n",
    "    #1. Modify tgt variable\n",
    "    # Shift the features down 1 row so they always occur before the tgt\n",
    "    vc_tgt = mtx01[0:(nrows-1),0].copy()\n",
    "    mt_feat = mtx01[1:(nrows),0:(ncols)].copy()\n",
    "    mt_feat[:,0]=vc_tgt\n",
    "    \n",
    "    #Modify the target variable depending on the metric (precipitation or temperature)\n",
    "    #Modify tgt depending on tgt_mod, options could be running totals, moving averages, \n",
    "        \n",
    "    #2. Check that all columns and rows have some type of data\n",
    "    \n",
    "    \n",
    "        \n",
    "    #return the matrix for analysis\n",
    "        \n",
    "    return mt_feat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_get_nearby_stations(stid_target, target_file,in_wd):\n",
    "\n",
    "    dt01 = target_file[target_file['tgt_stid']==stid_target]\n",
    "    dt01 = dt01[['feat_stid','dist_km']]\n",
    "    \n",
    "    vc_stid = dt01['feat_stid'].to_numpy()\n",
    "    vc_stid = np.insert(vc_stid,0,tgt_stid)\n",
    "    #vc_stid = [stid_target] + [vc_stid]\n",
    "    \n",
    "    #Need to create more combined data sets\n",
    "    lst_files = os.listdir(in_wd)\n",
    "    df_files = pd.DataFrame(lst_files,columns=[\"files\"])\n",
    "    df_files['stid'] = df_files['files'].str[:11]\n",
    "    df_files['istarget'] = df_files['stid'].str.contains(stid_target,case=False)\n",
    "    df_files['file_type'] = df_files['files'].str[23:31]\n",
    "    df_files['isintarget'] = df_files['stid'].isin(vc_stid)\n",
    "    \n",
    "    df_files = df_files[df_files['file_type']==\"combined\"]\n",
    "    df_files = df_files[df_files['isintarget']==True]\n",
    "    \n",
    "    mrg_df = pd.merge(df_files, dt01,how='left',left_on='stid',right_on='feat_stid')\n",
    "    mrg_df = mrg_df.drop(columns=['feat_stid','isintarget','file_type'])\n",
    "    mrg_df = mrg_df.sort_values(by='dist_km',na_position='first')\n",
    "    mrg_df = mrg_df.reset_index(drop=True)\n",
    "    mrg_df.columns = ['fnames','st_id','is_target_station','dist_km']\n",
    "    \n",
    "    return mrg_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "fnames",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "st_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "is_target_station",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "dist_km",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "0cd07f6c-de13-4813-a465-b0d17fc82f91",
       "rows": [
        [
         "0",
         "74486094789_model_data_combined.csv",
         "74486094789",
         "True",
         null
        ],
        [
         "1",
         "72503014732_model_data_combined.csv",
         "72503014732",
         "False",
         "18.4486301136753"
        ],
        [
         "2",
         "99728099999_model_data_combined.csv",
         "99728099999",
         "False",
         "19.1304795737379"
        ],
        [
         "3",
         "72055399999_model_data_combined.csv",
         "72055399999",
         "False",
         "21.8195607495791"
        ],
        [
         "4",
         "99727199999_model_data_combined.csv",
         "99727199999",
         "False",
         "22.210793862317"
        ],
        [
         "5",
         "72505394728_model_data_combined.csv",
         "72505394728",
         "False",
         "23.2929067375892"
        ],
        [
         "6",
         "99774399999_model_data_combined.csv",
         "99774399999",
         "False",
         "25.6017684203425"
        ],
        [
         "7",
         "99728999999_model_data_combined.csv",
         "99728999999",
         "False",
         "28.2313719527391"
        ],
        [
         "8",
         "74486454787_model_data_combined.csv",
         "74486454787",
         "False",
         "31.1941847078089"
        ],
        [
         "9",
         "72502014734_model_data_combined.csv",
         "72502014734",
         "False",
         "34.5736380004817"
        ],
        [
         "10",
         "72502594741_model_data_combined.csv",
         "72502594741",
         "False",
         "34.7322090728272"
        ],
        [
         "11",
         "72058100178_model_data_combined.csv",
         "72058100178",
         "False",
         "41.1426361626738"
        ],
        [
         "12",
         "72503794745_model_data_combined.csv",
         "72503794745",
         "False",
         "47.3759275096059"
        ],
        [
         "13",
         "72409454743_model_data_combined.csv",
         "72409454743",
         "False",
         "51.1131866502397"
        ],
        [
         "14",
         "72409754738_model_data_combined.csv",
         "72409754738",
         "False",
         "57.9118847134306"
        ],
        [
         "15",
         "72505004781_model_data_combined.csv",
         "72505004781",
         "False",
         "58.458610242145"
        ],
        [
         "16",
         "72408454760_model_data_combined.csv",
         "72408454760",
         "False",
         "58.9105057668071"
        ],
        [
         "17",
         "72224754785_model_data_combined.csv",
         "72224754785",
         "False",
         "76.4808285102006"
        ],
        [
         "18",
         "99729099999_model_data_combined.csv",
         "99729099999",
         "False",
         "77.1354530068429"
        ],
        [
         "19",
         "72501654790_model_data_combined.csv",
         "72501654790",
         "False",
         "78.2924460086958"
        ],
        [
         "20",
         "72504094702_model_data_combined.csv",
         "72504094702",
         "False",
         "79.3195150905245"
        ],
        [
         "21",
         "72409014780_model_data_combined.csv",
         "72409014780",
         "False",
         "83.7937677564474"
        ],
        [
         "22",
         "72508654734_model_data_combined.csv",
         "72508654734",
         "False",
         "84.9320353603073"
        ],
        [
         "23",
         "72040700462_model_data_combined.csv",
         "72040700462",
         "False",
         "91.0456611596941"
        ],
        [
         "24",
         "72407754779_model_data_combined.csv",
         "72407754779",
         "False",
         "91.6929903797762"
        ],
        [
         "25",
         "74000154793_model_data_combined.csv",
         "74000154793",
         "False",
         "95.6300251965931"
        ],
        [
         "26",
         "72409514792_model_data_combined.csv",
         "72409514792",
         "False",
         "97.8064627594545"
        ],
        [
         "27",
         "72409614706_model_data_combined.csv",
         "72409614706",
         "False",
         "99.1774139803395"
        ],
        [
         "28",
         "74486514719_model_data_combined.csv",
         "74486514719",
         "False",
         "99.3614295646764"
        ],
        [
         "29",
         "72503814714_model_data_combined.csv",
         "72503814714",
         "False",
         "99.8942895762404"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 30
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fnames</th>\n",
       "      <th>st_id</th>\n",
       "      <th>is_target_station</th>\n",
       "      <th>dist_km</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74486094789_model_data_combined.csv</td>\n",
       "      <td>74486094789</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72503014732_model_data_combined.csv</td>\n",
       "      <td>72503014732</td>\n",
       "      <td>False</td>\n",
       "      <td>18.448630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99728099999_model_data_combined.csv</td>\n",
       "      <td>99728099999</td>\n",
       "      <td>False</td>\n",
       "      <td>19.130480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72055399999_model_data_combined.csv</td>\n",
       "      <td>72055399999</td>\n",
       "      <td>False</td>\n",
       "      <td>21.819561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>99727199999_model_data_combined.csv</td>\n",
       "      <td>99727199999</td>\n",
       "      <td>False</td>\n",
       "      <td>22.210794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>72505394728_model_data_combined.csv</td>\n",
       "      <td>72505394728</td>\n",
       "      <td>False</td>\n",
       "      <td>23.292907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>99774399999_model_data_combined.csv</td>\n",
       "      <td>99774399999</td>\n",
       "      <td>False</td>\n",
       "      <td>25.601768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>99728999999_model_data_combined.csv</td>\n",
       "      <td>99728999999</td>\n",
       "      <td>False</td>\n",
       "      <td>28.231372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>74486454787_model_data_combined.csv</td>\n",
       "      <td>74486454787</td>\n",
       "      <td>False</td>\n",
       "      <td>31.194185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>72502014734_model_data_combined.csv</td>\n",
       "      <td>72502014734</td>\n",
       "      <td>False</td>\n",
       "      <td>34.573638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>72502594741_model_data_combined.csv</td>\n",
       "      <td>72502594741</td>\n",
       "      <td>False</td>\n",
       "      <td>34.732209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>72058100178_model_data_combined.csv</td>\n",
       "      <td>72058100178</td>\n",
       "      <td>False</td>\n",
       "      <td>41.142636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>72503794745_model_data_combined.csv</td>\n",
       "      <td>72503794745</td>\n",
       "      <td>False</td>\n",
       "      <td>47.375928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>72409454743_model_data_combined.csv</td>\n",
       "      <td>72409454743</td>\n",
       "      <td>False</td>\n",
       "      <td>51.113187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>72409754738_model_data_combined.csv</td>\n",
       "      <td>72409754738</td>\n",
       "      <td>False</td>\n",
       "      <td>57.911885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>72505004781_model_data_combined.csv</td>\n",
       "      <td>72505004781</td>\n",
       "      <td>False</td>\n",
       "      <td>58.458610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>72408454760_model_data_combined.csv</td>\n",
       "      <td>72408454760</td>\n",
       "      <td>False</td>\n",
       "      <td>58.910506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>72224754785_model_data_combined.csv</td>\n",
       "      <td>72224754785</td>\n",
       "      <td>False</td>\n",
       "      <td>76.480829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>99729099999_model_data_combined.csv</td>\n",
       "      <td>99729099999</td>\n",
       "      <td>False</td>\n",
       "      <td>77.135453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>72501654790_model_data_combined.csv</td>\n",
       "      <td>72501654790</td>\n",
       "      <td>False</td>\n",
       "      <td>78.292446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>72504094702_model_data_combined.csv</td>\n",
       "      <td>72504094702</td>\n",
       "      <td>False</td>\n",
       "      <td>79.319515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>72409014780_model_data_combined.csv</td>\n",
       "      <td>72409014780</td>\n",
       "      <td>False</td>\n",
       "      <td>83.793768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>72508654734_model_data_combined.csv</td>\n",
       "      <td>72508654734</td>\n",
       "      <td>False</td>\n",
       "      <td>84.932035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>72040700462_model_data_combined.csv</td>\n",
       "      <td>72040700462</td>\n",
       "      <td>False</td>\n",
       "      <td>91.045661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>72407754779_model_data_combined.csv</td>\n",
       "      <td>72407754779</td>\n",
       "      <td>False</td>\n",
       "      <td>91.692990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>74000154793_model_data_combined.csv</td>\n",
       "      <td>74000154793</td>\n",
       "      <td>False</td>\n",
       "      <td>95.630025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>72409514792_model_data_combined.csv</td>\n",
       "      <td>72409514792</td>\n",
       "      <td>False</td>\n",
       "      <td>97.806463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>72409614706_model_data_combined.csv</td>\n",
       "      <td>72409614706</td>\n",
       "      <td>False</td>\n",
       "      <td>99.177414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>74486514719_model_data_combined.csv</td>\n",
       "      <td>74486514719</td>\n",
       "      <td>False</td>\n",
       "      <td>99.361430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>72503814714_model_data_combined.csv</td>\n",
       "      <td>72503814714</td>\n",
       "      <td>False</td>\n",
       "      <td>99.894290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 fnames        st_id  is_target_station  \\\n",
       "0   74486094789_model_data_combined.csv  74486094789               True   \n",
       "1   72503014732_model_data_combined.csv  72503014732              False   \n",
       "2   99728099999_model_data_combined.csv  99728099999              False   \n",
       "3   72055399999_model_data_combined.csv  72055399999              False   \n",
       "4   99727199999_model_data_combined.csv  99727199999              False   \n",
       "5   72505394728_model_data_combined.csv  72505394728              False   \n",
       "6   99774399999_model_data_combined.csv  99774399999              False   \n",
       "7   99728999999_model_data_combined.csv  99728999999              False   \n",
       "8   74486454787_model_data_combined.csv  74486454787              False   \n",
       "9   72502014734_model_data_combined.csv  72502014734              False   \n",
       "10  72502594741_model_data_combined.csv  72502594741              False   \n",
       "11  72058100178_model_data_combined.csv  72058100178              False   \n",
       "12  72503794745_model_data_combined.csv  72503794745              False   \n",
       "13  72409454743_model_data_combined.csv  72409454743              False   \n",
       "14  72409754738_model_data_combined.csv  72409754738              False   \n",
       "15  72505004781_model_data_combined.csv  72505004781              False   \n",
       "16  72408454760_model_data_combined.csv  72408454760              False   \n",
       "17  72224754785_model_data_combined.csv  72224754785              False   \n",
       "18  99729099999_model_data_combined.csv  99729099999              False   \n",
       "19  72501654790_model_data_combined.csv  72501654790              False   \n",
       "20  72504094702_model_data_combined.csv  72504094702              False   \n",
       "21  72409014780_model_data_combined.csv  72409014780              False   \n",
       "22  72508654734_model_data_combined.csv  72508654734              False   \n",
       "23  72040700462_model_data_combined.csv  72040700462              False   \n",
       "24  72407754779_model_data_combined.csv  72407754779              False   \n",
       "25  74000154793_model_data_combined.csv  74000154793              False   \n",
       "26  72409514792_model_data_combined.csv  72409514792              False   \n",
       "27  72409614706_model_data_combined.csv  72409614706              False   \n",
       "28  74486514719_model_data_combined.csv  74486514719              False   \n",
       "29  72503814714_model_data_combined.csv  72503814714              False   \n",
       "\n",
       "      dist_km  \n",
       "0         NaN  \n",
       "1   18.448630  \n",
       "2   19.130480  \n",
       "3   21.819561  \n",
       "4   22.210794  \n",
       "5   23.292907  \n",
       "6   25.601768  \n",
       "7   28.231372  \n",
       "8   31.194185  \n",
       "9   34.573638  \n",
       "10  34.732209  \n",
       "11  41.142636  \n",
       "12  47.375928  \n",
       "13  51.113187  \n",
       "14  57.911885  \n",
       "15  58.458610  \n",
       "16  58.910506  \n",
       "17  76.480829  \n",
       "18  77.135453  \n",
       "19  78.292446  \n",
       "20  79.319515  \n",
       "21  83.793768  \n",
       "22  84.932035  \n",
       "23  91.045661  \n",
       "24  91.692990  \n",
       "25  95.630025  \n",
       "26  97.806463  \n",
       "27  99.177414  \n",
       "28  99.361430  \n",
       "29  99.894290  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#STEP 1.  Select a station using ID to get its related stations\n",
    "\n",
    "\n",
    "#Load df of target stations and those close by\n",
    "#74486094789 is JFK airport\n",
    "#Select the target station for which predictions will be made\n",
    "#Select the target station for which predictions will be made\n",
    "dt_trg_set2 = ipd02 + \"Station_Pairs_LE_100km_Info.csv\"\n",
    "dt_analysis_set2 = pd.read_csv(dt_trg_set2)\n",
    "#Governs the target station in subsequent modules\n",
    "tgt_stid =  \"74486094789\"\n",
    "\n",
    "#Nearby stations within 100km\n",
    "dt_analysis_set_keys = fn_get_nearby_stations(tgt_stid,dt_analysis_set2,ipd03)\n",
    "\n",
    "nstations_in_100km = len(dt_analysis_set_keys) #number of stations within 100 km\n",
    "num_st_keep = 20   #num of stations to keep for analysis\n",
    "nstations = min(nstations_in_100km,num_st_keep)\n",
    "\n",
    "#analysis_set = dt_trg_set\n",
    "#dt_analysis_set = pd.read_csv(analysis_set)\n",
    "#dt_analysis_set['path'] = ipd02\n",
    "#dt01 = dt_analysis_set[dt_analysis_set['target_station']==int(st_id)]\n",
    "#uniq_stat = dt01['vect_close_bkt2'].unique()[:2]\n",
    "#uniq_station = dt01['st_id'].unique()[:num_st_keep]\n",
    "#dt02 = dt01[dt01['st_id'].isin(uniq_station)]\n",
    "\n",
    "dt_analysis_set_keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\\CXA01\\Users\\jhugh\\Documents\\Py_S4\\Py_S4_v02_JHH\\NCEI_data\\74486094789_model_data_combined.csv\n",
      "\\\\CXA01\\Users\\jhugh\\Documents\\Py_S4\\Py_S4_v02_JHH\\NCEI_data\\72503014732_model_data_combined.csv\n",
      "\\\\CXA01\\Users\\jhugh\\Documents\\Py_S4\\Py_S4_v02_JHH\\NCEI_data\\99728099999_model_data_combined.csv\n",
      "\\\\CXA01\\Users\\jhugh\\Documents\\Py_S4\\Py_S4_v02_JHH\\NCEI_data\\72055399999_model_data_combined.csv\n",
      "\\\\CXA01\\Users\\jhugh\\Documents\\Py_S4\\Py_S4_v02_JHH\\NCEI_data\\99727199999_model_data_combined.csv\n",
      "\\\\CXA01\\Users\\jhugh\\Documents\\Py_S4\\Py_S4_v02_JHH\\NCEI_data\\72505394728_model_data_combined.csv\n",
      "\\\\CXA01\\Users\\jhugh\\Documents\\Py_S4\\Py_S4_v02_JHH\\NCEI_data\\99774399999_model_data_combined.csv\n",
      "\\\\CXA01\\Users\\jhugh\\Documents\\Py_S4\\Py_S4_v02_JHH\\NCEI_data\\99728999999_model_data_combined.csv\n",
      "\\\\CXA01\\Users\\jhugh\\Documents\\Py_S4\\Py_S4_v02_JHH\\NCEI_data\\74486454787_model_data_combined.csv\n",
      "\\\\CXA01\\Users\\jhugh\\Documents\\Py_S4\\Py_S4_v02_JHH\\NCEI_data\\72502014734_model_data_combined.csv\n",
      "\\\\CXA01\\Users\\jhugh\\Documents\\Py_S4\\Py_S4_v02_JHH\\NCEI_data\\72502594741_model_data_combined.csv\n",
      "\\\\CXA01\\Users\\jhugh\\Documents\\Py_S4\\Py_S4_v02_JHH\\NCEI_data\\72058100178_model_data_combined.csv\n",
      "\\\\CXA01\\Users\\jhugh\\Documents\\Py_S4\\Py_S4_v02_JHH\\NCEI_data\\72503794745_model_data_combined.csv\n",
      "\\\\CXA01\\Users\\jhugh\\Documents\\Py_S4\\Py_S4_v02_JHH\\NCEI_data\\72409454743_model_data_combined.csv\n",
      "\\\\CXA01\\Users\\jhugh\\Documents\\Py_S4\\Py_S4_v02_JHH\\NCEI_data\\72409754738_model_data_combined.csv\n",
      "\\\\CXA01\\Users\\jhugh\\Documents\\Py_S4\\Py_S4_v02_JHH\\NCEI_data\\72505004781_model_data_combined.csv\n",
      "\\\\CXA01\\Users\\jhugh\\Documents\\Py_S4\\Py_S4_v02_JHH\\NCEI_data\\72408454760_model_data_combined.csv\n",
      "\\\\CXA01\\Users\\jhugh\\Documents\\Py_S4\\Py_S4_v02_JHH\\NCEI_data\\72224754785_model_data_combined.csv\n",
      "\\\\CXA01\\Users\\jhugh\\Documents\\Py_S4\\Py_S4_v02_JHH\\NCEI_data\\99729099999_model_data_combined.csv\n",
      "\\\\CXA01\\Users\\jhugh\\Documents\\Py_S4\\Py_S4_v02_JHH\\NCEI_data\\72501654790_model_data_combined.csv\n",
      "(172800, 14)\n",
      "(172800, 14)\n",
      "(35683, 14)\n",
      "(73660, 14)\n",
      "(36525, 14)\n",
      "(172800, 14)\n",
      "(37570, 14)\n",
      "(40929, 14)\n",
      "(6241, 14)\n",
      "(172800, 14)\n",
      "(172800, 14)\n",
      "(70127, 14)\n",
      "(172800, 14)\n",
      "(6241, 14)\n",
      "(14195, 14)\n",
      "(172800, 14)\n",
      "(6276, 14)\n",
      "(6241, 14)\n",
      "(36777, 14)\n",
      "(6241, 14)\n",
      "lst data done\n",
      "(172800, 160)\n",
      "(172800, 8401)\n",
      "mtx data done\n",
      "s4 data done\n"
     ]
    }
   ],
   "source": [
    "#STEP 2.  Use elements created in STEP 1 to get a data matrix for analysis\n",
    "#All the available stations which have all three metrics needed for model\n",
    "#vc_stid = dt_analysis_set_keys[['st_id']].drop_duplicates()\n",
    "\n",
    "#Organize the data based on the nature of the data so they all conform to the same size matrices\n",
    "#The metrics included are\n",
    "#  AT - air temperature in Celcius\n",
    "#  RH - relative humidity in Percent\n",
    "#  PR - precipitation in MM\n",
    "\n",
    "#Put them all in appropriate matrices and create list of metrics\n",
    "#Data files are of two types\n",
    "#    -  24 hour * 365 days * n years  (AT, PR)\n",
    "#    -  365 days * n years (RH)\n",
    "\n",
    "#Parameters governing data matrix\n",
    "stid_target = tgt_stid  #\"72406093721\"\n",
    "stid_keys = dt_analysis_set_keys\n",
    "\n",
    "ipd = ipd02\n",
    "nhours = 24\n",
    "ndays = 365\n",
    "nmonths_row = 240\n",
    "nmonths_col = 2\n",
    "nmetrics = 5\n",
    "tgt_metric =  \"prec\" #\"prec\"  \"temp\"\n",
    "tgt_mod = \"No Mod\"\n",
    "\n",
    "#1. Get the relevant data in a list\n",
    "lst_data = fn_select_data_vectors_pr07(stid_keys,nstations,nhours,nmonths_row,nmonths_col)\n",
    "fn = f\"lst data done\"\n",
    "print(fn)\n",
    "\n",
    "#2. Construct a matrix from most recent to most distant from the list data\n",
    "mtx_data = fn_create_data_matrix_pr07(lst_data,tgt_metric,nstations,nmetrics,nmonths_col)\n",
    "fn = f\"mtx data done\"\n",
    "print(fn)\n",
    "\n",
    "#3.  Create a target variable and select feature variables\n",
    "#s4_data = fn_create_tgt_matrix_pr07(mtx_data,tgt_metric,tgt_mod)\n",
    "fn = f\"s4 data done\"\n",
    "print(fn)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\\CXA01\\Users\\jhugh\\Documents\\Py_S4\\Py_S4_v02_JHH\\NCEI_parquet_files\\\n",
      "(172800, 8401)\n",
      "\\\\CXA01\\Users\\jhugh\\Documents\\Py_S4\\Py_S4_v02_JHH\\NCEI_parquet_files\\prec_mtx_74486094789_stations_20_RowMn_240_ColMn_2.parquet\n"
     ]
    }
   ],
   "source": [
    "#Save the mtx_data to a folder\n",
    "#This lets us start at Step 3 for iterative runs\n",
    "print(ipd04)\n",
    "rtn_mtx_pqt = pd.DataFrame(mtx_data)\n",
    "print(rtn_mtx_pqt.shape)\n",
    "fn = f\"{ipd04}{tgt_metric}_mtx_{stid_target}_stations_{nstations}_RowMn_{nmonths_row}_ColMn_{nmonths_col}.parquet\"\n",
    "print(fn)\n",
    "rtn_mtx_pqt.to_parquet(fn, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  (172360, 5041)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "category",
         "type": "unknown"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "47196303-63ce-4b16-bbb1-2e471650a09d",
       "rows": [
        [
         "0",
         "166321"
        ],
        [
         "1",
         "6039"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 2
       }
      },
      "text/plain": [
       "0\n",
       "0    166321\n",
       "1      6039\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform_matrix(df, bins, labels):\n",
    "    \"\"\"\n",
    "    Transforms the given matrix by binning the first column, dropping the first row and column,\n",
    "    and adding the binned column as the first column.\n",
    "\n",
    "    Parameters:\n",
    "        matrix (pd.DataFrame): The input matrix.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The transformed matrix.\n",
    "    \"\"\"\n",
    "    # Extract the first column (dependent variable)\n",
    "    firstcolumn = df.iloc[:, 0].astype('float32')\n",
    "\n",
    "    # Bin the first column\n",
    "    binned_column = pd.cut(firstcolumn, bins=bins, labels=labels)\n",
    "\n",
    "    # Drop the first first column\n",
    "    df = df.iloc[:, 1:]\n",
    "\n",
    "    # Add the binned column as the first column\n",
    "    df.insert(0, 0, binned_column.iloc[0:].values)\n",
    "\n",
    "    # Return the final matrix\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "# Example matrix (replace this with your actual matrix)\n",
    "df = pd.DataFrame(s4_data).iloc[1:,] # Convert to DataFrame and drop the first row for binning\n",
    "# Drop rows where the first column has missing values\n",
    "# df = df.dropna(subset=[0])\n",
    "fc = df.iloc[:,0].astype('float32') # Ensure the first column is float for binning\n",
    "#DEL#bins = [-float('inf'), 10, 20, 25, float('inf')]\n",
    "\n",
    "bins = [-float('inf'), 1, float('inf')]\n",
    "# bins = [-float('inf'), 0] + list(pd.qcut(fc[fc > 0], q=4, retbins=True)[1][1:]) + [float('inf')]\n",
    "labels = [0, 1]\n",
    "# print(bins, labels)\n",
    "\n",
    "\n",
    "# bins = [-float('inf'), 0] + list(pd.qcut(fc[fc > 0], q=4, retbins=True)[1][1:]) + [float('inf')]\n",
    "#labels = [0, 1, 2, 3]\n",
    "# print(bins, labels)\n",
    "\n",
    "df_transformed = transform_matrix(df, bins, labels)\n",
    "df_transformed = df_transformed[df_transformed[0]>= 0]\n",
    "print(\"Dataset: \", df_transformed.shape)\n",
    "df_transformed[0].value_counts(dropna=False).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: (120652, 5041)\n",
      "0\n",
      "0    116260\n",
      "1      4392\n",
      "Name: count, dtype: int64\n",
      "Validation Set: (17236, 5041)\n",
      "0\n",
      "0    16626\n",
      "1      610\n",
      "Name: count, dtype: int64\n",
      "Test Set: (34472, 5041)\n",
      "0\n",
      "0    33435\n",
      "1     1037\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def split_time_series_data(df, train_ratio=0.7, val_ratio=0.1, test_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Splits a time series dataset into training, validation, and test sets.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input time series dataset, ordered in descending dates.\n",
    "        train_ratio (float): The proportion of data to use for training.\n",
    "        val_ratio (float): The proportion of data to use for validation.\n",
    "        test_ratio (float): The proportion of data to use for testing.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing three DataFrames (train, validation, test).\n",
    "    \"\"\"\n",
    "    assert train_ratio + val_ratio + test_ratio == 1.0, \"Ratios must sum to 1.0\"\n",
    "\n",
    "    # Calculate split indices\n",
    "    n = len(df)\n",
    "    test_end = int(n * test_ratio)    \n",
    "    val_end = test_end + int(n * val_ratio)\n",
    "    \n",
    "\n",
    "    # Split the data\n",
    "    test_data = df.iloc[:test_end].sort_index(ascending=False)\n",
    "    val_data = df.iloc[test_end:val_end].sort_index(ascending=False)\n",
    "    train_data = df.iloc[val_end:].sort_index(ascending=False)\n",
    "\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "# Example usage\n",
    "train_data, val_data, test_data = split_time_series_data(df_transformed)\n",
    "print(\"Training Set:\", train_data.shape)\n",
    "print(train_data[0].value_counts(dropna=False).sort_index())\n",
    "print(\"Validation Set:\", val_data.shape)\n",
    "print(val_data[0].value_counts(dropna=False).sort_index())\n",
    "print(\"Test Set:\", test_data.shape)\n",
    "print(test_data[0].value_counts(dropna=False).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets exported successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the station ID and output directory\n",
    "station_id = st_id\n",
    "output_dir = ipd02\n",
    "\n",
    "\n",
    "# Export datasets to CSV\n",
    "train_data.to_parquet(f\"{output_dir}{station_id}_train.parquet\", index=False)\n",
    "val_data.to_parquet(f\"{output_dir}{station_id}_validation.parquet\", index=False)\n",
    "test_data.to_parquet(f\"{output_dir}{station_id}_test.parquet\", index=False)\n",
    "\n",
    "# train_data.to_csv(f\"{output_dir}{station_id}_train.csv\", index=False)\n",
    "# val_data.to_csv(f\"{output_dir}{station_id}_validation.csv\", index=False)\n",
    "# test_data.to_csv(f\"{output_dir}{station_id}_test.csv\", index=False)\n",
    "\n",
    "print(\"Datasets exported successfully.\")\n",
    "\n",
    "#python -m s4model --modelname 72406093721 --trainset C:\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72406093721_train.parquet --valset C:\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72406093721_validation.parquet --testset C:\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72406093721_test.parquet --tabulardata --dependent_variable 0 --epochs 30\n",
    "\n",
    "# python -m charts --df ../results/72508014740_Test_results_20250424_104121PM.csv --actual 0 --predicted Predicted \n",
    "# python -m charts --df ../results/72508014740_Test_results_20250427_100132PM.csv --actual 0 --predicted Predicted \n",
    "# python -m charts --df ../results/72508014740_Test_results_20250428_091259PM.csv --actual 0 --predicted Predicted \n",
    "# python -m charts --df ../results/72508014740_Test_results_20250428_094033PM.csv --actual 0 --predicted Predicted \n",
    "# python -m charts --df ../results/72508014740_Test_results_20250429_091538PM.csv --actual 0 --predicted Predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2106272906.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[23], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    python -m s4model --modelname 72406093721 --trainset C:\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72406093721_train.parquet --valset C:\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72406093721_validation.parquet --testset C:\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72406093721_test.parquet --tabulardata --dependent_variable 0 --epochs 30\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Code to run at cmd\n",
    "#python -m s4model --modelname 72406093721 --trainset C:\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72406093721_train.parquet --valset C:\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72406093721_validation.parquet --testset C:\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72406093721_test.parquet --tabulardata --dependent_variable 0 --epochs 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_first_non_nan(vector):\n",
    "#     return next((i for i, x in enumerate(vector) if not math.isnan(x)), None)\n",
    "\n",
    "# first_number_at1 = find_first_non_nan(vc_at1)\n",
    "# first_number_rh1 = find_first_non_nan(vc_rh1)\n",
    "# first_number_rh2 = find_first_non_nan(vc_rh2)\n",
    "# first_number_rh3 = find_first_non_nan(vc_rh3)\n",
    "# first_number_pr1 = find_first_non_nan(vc_pr1)\n",
    "\n",
    "# #Number of missing data points\n",
    "# print(f\"AT1 Number of missing cells: {np.count_nonzero(np.isnan(vc_at1))}\")\n",
    "# print(f\"AT1 Vector Length: {len(vc_at1)}\")\n",
    "\n",
    "# print(f\"RH1 Number of missing cells: {np.count_nonzero(np.isnan(vc_rh1))}\")\n",
    "# print(f\"RH1 Vector Length: {len(vc_rh1)}\")\n",
    "\n",
    "# print(f\"PR1 Number of missing cells: {np.count_nonzero(np.isnan(vc_pr1))}\")\n",
    "# print(f\"PR1 Vector Length: {len(vc_pr1)}\")\n",
    "\n",
    "# #Once a day metrics\n",
    "# v_data_first_once_day = max(\n",
    "#                    first_number_rh1,\n",
    "#                    first_number_rh2,\n",
    "#                    first_number_rh3)\n",
    "# print(v_data_first_once_day)\n",
    "\n",
    "# #24 times a day metrics\n",
    "# v_data_first_24h_day = max(first_number_at1,\n",
    "#                    first_number_pr1)\n",
    "# print(v_data_first_24h_day)\n",
    "\n",
    "\n",
    "#These vectors all start with a numeric value so missing values can be filled\n",
    "\n",
    "\n",
    "#print(first_number_at1)\n",
    "#print(first_number_rh1)\n",
    "#print(first_number_rh2)\n",
    "#print(first_number_rh3)\n",
    "#print(first_number_pr1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(jj)\n",
    "fc = df.iloc[:,0].astype('float32')\n",
    "print(fc.shape)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(fc, bins=50, color='blue', alpha=0.7, density=True)\n",
    "plt.title('Normalized Histogram of a')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom bins\n",
    "\n",
    "a = fc[~fc.isnull()]\n",
    "\n",
    "# Calculate min, max, and percentiles by 10 for 'a'\n",
    "min_val = a.min()\n",
    "max_val = a.max()\n",
    "percentiles = np.percentile(a, np.arange(0, 101, 10))\n",
    "\n",
    "print(f\"Min: {min_val}\")\n",
    "print(f\"Max: {max_val}\")\n",
    "print(f\"Percentiles: {percentiles}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s4_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
