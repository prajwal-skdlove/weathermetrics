{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IV Load the data for a station and organize for precipitation model\n",
    "import os as os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths\n",
    "#Need to set up based on who is running\n",
    "usr = \"JH\"\n",
    "if usr == \"PK\":\n",
    "    ipd01 = \"\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\2024\\\\\"\n",
    "    ipd02 = \"D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\\"\n",
    "    ipd03 = \"\"\n",
    "    ipd04 = \"\"\n",
    "    \n",
    "if usr == \"JH\":\n",
    "    ipd01 = \"\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\2024\\\\\"\n",
    "    ipd02 = \"\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\\"\n",
    "    ipd03 = \"\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\Py_S4\\\\Py_S4_v02_JHH\\\\NCEI_data\\\\\"\n",
    "    ipd04 = \"\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\Py_S4\\\\Py_S4_v02_JHH\\\\NCEI_parquet_files\\\\\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_select_data_vectors_pr07(stid_keys,nstations,nhours,nmonths_row,nmonths_col):\n",
    "    #Get the matrix of data to input into marix\n",
    "    #The S4 model\n",
    "    # 1. Read in combined data files for multiple stations\n",
    "    # 2. Find the common dates that will be used to align the matrices\n",
    "    #    Keep only the values that fit the common starting date \n",
    "\n",
    "    # 1. Read in data files and get start date v_date... for each\n",
    "    # Check that data exist for all metrics\n",
    "\n",
    "    df_stid = stid_keys\n",
    "    df_stid2 = df_stid.iloc[:, [1]]\n",
    "    df_stid2 = df_stid2.drop_duplicates() # unique()\n",
    "    \n",
    "    n_rows_stid = min(len(df_stid2),nstations)\n",
    "    \n",
    "    #Matrix in which measures are stored\n",
    "    n_vmt_rows = nhours*30*max(nmonths_row,nmonths_col)\n",
    "    \n",
    "    data_list = []\n",
    "    date_max_list =[]\n",
    "    \n",
    "    #1. Read in data files\n",
    "    for i in range(0, n_rows_stid):\n",
    "    #for i in range(0, 28):\n",
    "        infile = ipd03 + str(df_stid2.iloc[i,0]) + \"_model_data_combined.csv\"\n",
    "        print(infile)\n",
    "        \n",
    "        #Eliminate fully blank lines\n",
    "        cleaned_lines = []\n",
    "        with open(infile, 'r', newline='') as infile:\n",
    "            reader = csv.reader(infile)\n",
    "            for row in reader:\n",
    "                # Example: Skip rows that are entirely empty or have only blank 'name'\n",
    "                if any(cell.strip() for cell in row) and (len(row) < 1 or row[0].strip() != ''): # Assuming 'name' is the first column\n",
    "                    cleaned_lines.append(row)\n",
    "        \n",
    "        df_metrics = pd.DataFrame(cleaned_lines[1:], columns=cleaned_lines[0]) # Assuming first row is header\n",
    "\n",
    "        df_metrics['str_hr'] = df_metrics['nHOUR']  #.apply(lambda x: f'{x:02d}')\n",
    "        df_metrics['date_hr'] = pd.to_datetime(df_metrics['Date_YYYYMMDD'] + ' ' + df_metrics['str_hr'])\n",
    "        df_metrics['month_number'] = df_metrics['date_hr'].dt.month\n",
    "        df_metrics['week_number'] = df_metrics['date_hr'].dt.isocalendar().week\n",
    "        df_metrics['stid'] = str(df_stid2.iloc[i,0])\n",
    "        data_list.append(df_metrics)\n",
    "\n",
    "    #2. Align dates across data files\n",
    "    for i in range(0, n_rows_stid):\n",
    "        df = data_list[i]\n",
    "        max_date = max(df['date_hr'])     \n",
    "        date_max_list.append(max_date)\n",
    "    \n",
    "    date_filter = min(date_max_list)\n",
    "\n",
    "    for i in range(0, n_rows_stid):\n",
    "        df = data_list[i]\n",
    "\n",
    "        filtered_df = df[df['date_hr'] < date_filter]\n",
    "        filtered_df = filtered_df.iloc[(len(filtered_df) - n_vmt_rows):len(filtered_df)]\n",
    "        \n",
    "        idx_col = filtered_df.columns\n",
    "        idx_col = str(df_stid2.iloc[i,0]) + '_' + idx_col\n",
    "        filtered_df.columns = idx_col\n",
    "\n",
    "        print(filtered_df.shape)\n",
    "        \n",
    "        data_list[i] = filtered_df\n",
    "        \n",
    "    return data_list[0:n_rows_stid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_select_data_vectors_pr07xxx(stid_keys,nstations,nhours,nmonths_row,nmonths_col):\n",
    "    #Get the matrix of data to input into marix\n",
    "    #The S4 model\n",
    "    # 1. Read in combined data files for multiple stations\n",
    "    # 2. Find the common dates that will be used to align the matrices\n",
    "    #    Keep only the values that fit the common starting date \n",
    "\n",
    "    # 1. Read in data files and get start date v_date... for each\n",
    "    # Check that data exist for all metrics\n",
    "\n",
    "    df_stid = stid_keys\n",
    "    df_stid2 = df_stid.iloc[:, [0, 3, 4]]  #df_stid[df_stid['target_station'] == stid_target]\n",
    "    df_stid2 = df_stid2.drop_duplicates() # unique()\n",
    "\n",
    "    n_rows_stid = min(len(df_stid2),nstations)\n",
    "    \n",
    "    #Matrix in which measures are stored\n",
    "    n_vmt_rows = nhours*30*max(nmonths_row,nmonths_col)\n",
    "    #vmt_full_set = np.empty(n_vmt_rows,np.float16)\n",
    "    #is_first = 1\n",
    "    \n",
    "    data_list = []\n",
    "    date_max_list =[]\n",
    "\n",
    "    #1. Read in data files\n",
    "    for i in range(0, n_rows_stid):\n",
    "        infile = ipd03 + str(df_stid2.iloc[i,0]) + \"_model_data_combined.csv\"\n",
    "        print(infile)\n",
    "        df_metrics = pd.read_csv(infile)\n",
    "        df_metrics['str_hr'] = df_metrics['nHOUR'].apply(lambda x: f'{x:02d}')\n",
    "        df_metrics['date_hr'] = pd.to_datetime(df_metrics['Date_YYYYMMDD'] + ' ' + df_metrics['str_hr'])\n",
    "        df_metrics['month_number'] = df_metrics['date_hr'].dt.month\n",
    "        df_metrics['week_number'] = df_metrics['date_hr'].dt.isocalendar().week\n",
    "        df_metrics['stid'] = str(df_stid2.iloc[i,0])\n",
    "        data_list.append(df_metrics)\n",
    "\n",
    "    #2. Align dates across data files\n",
    "    for i in range(0, n_rows_stid):\n",
    "        df = data_list[i]\n",
    "        max_date = max(df['date_hr'])     \n",
    "        date_max_list.append(max_date)\n",
    "    \n",
    "    date_filter = min(date_max_list)\n",
    "\n",
    "    for i in range(0, n_rows_stid):\n",
    "        df = data_list[i]\n",
    "        #place holder for filtering to a specific end date\n",
    "        #date_filter = pd.to_datetime(\"2023-8-11\" + ' ' + \"17\")\n",
    "        filtered_df = df[df['date_hr'] < date_filter]\n",
    "        filtered_df = filtered_df.iloc[(len(filtered_df) - n_vmt_rows):len(filtered_df)]\n",
    "        \n",
    "        idx_col = filtered_df.columns\n",
    "        idx_col = str(df_stid2.iloc[i,0]) + '_' + idx_col\n",
    "        filtered_df.columns = idx_col\n",
    "\n",
    "        print(filtered_df.shape)\n",
    "        \n",
    "        data_list[i] = filtered_df\n",
    "        \n",
    "    return data_list[0:n_rows_stid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_create_data_matrix_pr07(lst_data,tgt_metric,nstations,nmetrics,nmonths_col,ndays_col):\n",
    "    #Create a matrix for precipitation data that feeds\n",
    "    #The S4 model\n",
    "    # 1. Get data list and select target metric\n",
    "    #Two choices now\n",
    "    #    - prec\n",
    "    #    - temp\n",
    "    # 2.  Create output and move data into approprate cells\n",
    "\n",
    "    # 1. get data list details and reduce to metrics\n",
    "    data_vectors_list = []\n",
    "    dt_rows = len(lst_data[0])\n",
    "    df00 = []\n",
    "\n",
    "    dt_rows_minus_1 = dt_rows - 1\n",
    "    \n",
    "    if tgt_metric == \"prec\": tgt_cols = [10,2,3,4,5,6,8,9,11,12]\n",
    "    if tgt_metric == \"temp\": tgt_cols = [10,3,2,4,5,6,8,9,11,12]\n",
    "    non_metrics = 4  #These are the non sensor data brought in to analysis (e.g., day of year, month, ...)\n",
    "    \n",
    "    #Reset the indices as all records are ordered by date and hour\n",
    "    #But stations have missing records so not everything aligns\n",
    "    #Need to use the target station (station 0) as the standard and left join th other fields\n",
    "    for i in range(nstations):\n",
    "        tdt = lst_data[i].iloc[:,tgt_cols]\n",
    "        tdt = tdt.iloc[::-1]\n",
    "        tdt.reset_index(drop=True, inplace=True)\n",
    "        first_column_name = tdt.columns[0]\n",
    "        tdt.rename(columns={first_column_name: 'key_date_hr'}, inplace=True)\n",
    "        df00.append(tdt)\n",
    "        fn = f\"1. Done with setting up station -  {i}\"\n",
    "        print(fn)\n",
    "        #print(tdt)\n",
    "        \n",
    "    \n",
    "    #Take the list of station data, df00, and paste side by side\n",
    "    #to produce rs which is a large matrix of data for all stations \n",
    "    join_key = 'key_date_hr'\n",
    "    rs = reduce(lambda left, right: pd.merge(left, right, on=join_key, how='left'), df00)\n",
    "    \n",
    "    #rs = pd.concat(df00, axis=1)  #old way that did merge based on record index\n",
    "    #rs has each row being a dayXhour for all stations for the stations full metrics\n",
    "    #full metrics is len(tgt_cols) - 1\n",
    "    fn = f\"2. Shape of combined data including all stations\"\n",
    "    print(fn)\n",
    "    print(rs.shape)\n",
    "    \n",
    "    \n",
    "    #2. Create output matrix and move data into appropriate cells\n",
    "    \n",
    "    nmetrics_and_non_metrics = nmetrics + non_metrics\n",
    "    fn = f\"nmetrics_and_non_metrics {nmetrics_and_non_metrics}\"\n",
    "    print(fn)\n",
    "    ndataelements = nstations*(nmetrics_and_non_metrics)\n",
    "    fn = f\"ndataelements - stations by metrics {ndataelements}\"\n",
    "    print(fn)\n",
    "    mt_cols = 1 + ndataelements*ndays_col*24\n",
    "    fn = f\"mt_cols 1 + #data elements*ndays_col*24 {mt_cols}\"\n",
    "    print(fn)\n",
    "    mt_rows = len(rs)\n",
    "    \n",
    "    #mt_rows = 1000  #keep here for development until done\n",
    "    mt_full_set = np.empty((mt_rows,mt_cols ),np.float16)\n",
    "    \n",
    "    #tgt_cols = range(1,181)\n",
    "    \n",
    "    #Create matrix of data\n",
    "    #tgt_cols_num = len(tgt_cols)\n",
    "    for i in range(0,mt_rows):\n",
    "        mt_full_set[i,range(0,ndataelements)] = rs.iloc[i,range(1,ndataelements+1)].apply(pd.to_numeric, errors='coerce')\n",
    " \n",
    "    fn = f\"3. Full mt setup with initial data for the dayXhour only\"\n",
    "    print(fn)  \n",
    "    print(mt_full_set.shape)\n",
    "    \n",
    "    #Add additional column sets\n",
    "    lpi = mt_rows - ndays_col*24 - 1  #number of rows in matrix - the data elements in a column... \n",
    "                                    #ensures all records have data\n",
    "    #limit of rows in data set so all matrix rows have data\n",
    "    lpj = ndays_col*24 -1   #number of colum sets in columns\n",
    "    \n",
    "    #The row I am operating on\n",
    "    for i in range(0,lpi):\n",
    "        #The column I am going to fill\n",
    "        for j in range(0,lpj):\n",
    "        \n",
    "            row_get = i + j + 1              #For every new column in current row we have to go down one more row\n",
    "            col_get_start = 0                #data retrieved starting column 1\n",
    "            col_get_end = ndataelements - 1\n",
    "            \n",
    "            row_put = i\n",
    "            col_put_start = ndataelements*(row_get - row_put)\n",
    "            col_put_end = col_put_start + ndataelements - 1\n",
    "            \n",
    "            mt_full_set[row_put,col_put_start:col_put_end] = mt_full_set[row_get,col_get_start:col_get_end]\n",
    "     \n",
    "    fn = f\"4. mt_full_set with ndays of data added to the columns\"\n",
    "    print(fn)  \n",
    "    print(mt_full_set.shape)\n",
    "           \n",
    "    return mt_full_set #rs #mt_full_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_create_data_matrix_pr07xxx(lst_data,tgt_metric,nstations,nmetrics,nmonths_col):\n",
    "    #Create a matrix for precipitation data that feeds\n",
    "    #The S4 model\n",
    "    # 1. Get data list and select target metric\n",
    "    #Two choices now\n",
    "    #    - prec\n",
    "    #    - temp\n",
    "    # 2.  Create output and move data into approprate cells\n",
    "\n",
    "    # 1. get data list details and reduce to metrics\n",
    "    data_vectors_list = []\n",
    "    dt_rows = len(lst_data[0])\n",
    "    df00 = []\n",
    "\n",
    "    dt_rows_minus_1 = dt_rows - 1\n",
    "    \n",
    "    if tgt_metric == \"prec\": tgt_cols = [8,2,3,4,5,6,9,10]\n",
    "    if tgt_metric == \"temp\": tgt_cols = [8,3,2,4,5,6,9,10]\n",
    "    \n",
    "    #Reset the indices as all records are ordered by date and hour\n",
    "    for i in range(nstations):\n",
    "        df00.append(lst_data[i].iloc[:,tgt_cols])\n",
    "    \n",
    "    df01 = [df.reset_index(drop=True) for df in df00]\n",
    "    rs = pd.concat(df01, axis=1)\n",
    "    print(rs.shape)\n",
    "    #print(rs.iloc[1:10,:])\n",
    "    #Order from most recent to most distant in time\n",
    "    rs = rs.iloc[::-1]\n",
    "    #print(rs.iloc[1:10,:])\n",
    "    \n",
    "    #2. Create output matrix and move data into appropratie cells\n",
    "    \n",
    "    nmetrics_mon_week = nmetrics + 2\n",
    "    mt_cols = 1 + nstations*(nmetrics_mon_week)*nmonths_col*30\n",
    "    mt_rows = len(rs) #- nmonths_col*30*24 - 1\n",
    "    \n",
    "    mt_full_set = np.empty((mt_rows,mt_cols ),np.float16)\n",
    "    \n",
    "    #rng = mt_rows\n",
    "    #Set up first column set of data\n",
    "    #tgt_cols = [1,2,3,4,5,7,8,9,10,11,13,14,15,16,17,19,20,21,22,23]\n",
    "    tgt_cols = [1,2,3,4,5,6,7,9,10,11,12,13,14,15,17,18,19,20,21,22,23,25,26,27,28,29,30,31]\n",
    "    \n",
    "    \n",
    "    tgt_cols_num = len(tgt_cols)\n",
    "    for i in range(0,mt_rows):\n",
    "        mt_full_set[i,range(0,tgt_cols_num)] = rs.iloc[i,tgt_cols]\n",
    "    \n",
    "\n",
    "    #Add additional column sets\n",
    "    nc = nstations*nmetrics_mon_week\n",
    "    lpi = mt_rows - nmonths_col*30\n",
    "    #limit of rows in data set so all marix rows have data\n",
    "    lpj = nmonths_col*30 - nc  #number of colum sets in columns\n",
    "    \n",
    "    for j in range(0,lpj):\n",
    "        for i in range(0,lpi):\n",
    "            row_get = i + j + 1\n",
    "            col_get_start = 0\n",
    "            col_get_end = nc\n",
    "\n",
    "            row_put = i\n",
    "            col_put_start = nc*(j+1)\n",
    "            col_put_end = col_put_start + nc\n",
    "\n",
    "            mt_full_set[row_put,col_put_start:col_put_end] = mt_full_set[row_get,col_get_start:col_get_end]\n",
    "    \n",
    "    return mt_full_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions to identify stations in network that have rain sensor and rain\n",
    "def cnt_ge0(row):\n",
    "        cnt_gt_0 = sum(row >= 0)\n",
    "        return cnt_gt_0\n",
    "\n",
    "def cnt_gt0(row):\n",
    "        cnt_gt_0 = sum(row > 0)\n",
    "        return cnt_gt_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_prec_ex01(mtx01,nstations,nmetrics,dayscol):\n",
    "    \n",
    "    df = pd.DataFrame(mtx01)\n",
    "    ncols_orig = df.shape[1]\n",
    "    print(f\"Number of mxta columns {ncols_orig}\")\n",
    "    \n",
    "    \n",
    "    #Does target have a finite value\n",
    "    df['tgt_isfinite'] = np.isfinite(df.iloc[:,0])\n",
    "    \n",
    "    #moving average of target value\n",
    "    window_hrs = 6\n",
    "    nm1 = 'tgt_roll_sum_per' + str(window_hrs)\n",
    "    df[nm1] = df.iloc[:,0].rolling(window=window_hrs, min_periods=1).sum()\n",
    "    nm2 = 'tgt_roll_round_int' + str(window_hrs)\n",
    "    df[nm2] = round(df[nm1],0)\n",
    "    \n",
    "    #nstations = 20\n",
    "    #dayscol = 7\n",
    "    #nmetrics = 9\n",
    "    curr_per = 1   #current period for which average is to be taken, must be 1 or greater\n",
    "    tot_per = (dayscol*24 - 1)  #days by 24 hours minus 1\n",
    "    \n",
    "\n",
    "    for iper in range(1,tot_per,1):\n",
    "        vc_start = iper*nstations*nmetrics\n",
    "        vc_end = vc_start+ nstations*nmetrics - 1\n",
    "        shift_right = 0\n",
    "        vc_num = np.arange(vc_start,vc_end,nmetrics) + shift_right\n",
    "\n",
    "        nm = 'avg'+ str(iper)\n",
    "        df[nm] = df.iloc[:,vc_num].mean(axis=1,skipna=True)\n",
    "\n",
    "        nm = 'cnt_ge_zero'+ str(iper)\n",
    "        jjge0 = df.iloc[:,vc_num].apply(cnt_ge0,axis=1)\n",
    "        df[nm] = jjge0\n",
    "\n",
    "        nm = 'cnt_gt_zero'+ str(iper)\n",
    "        jjgt0 = df.iloc[:,vc_num].apply(cnt_gt0,axis=1)\n",
    "        df[nm] = jjgt0\n",
    "    \n",
    "        ncols_final = df.shape[1]\n",
    "        new_cols = range(ncols_orig+1,ncols_final,1)\n",
    "        old_cols = range(180,ncols_orig,1)\n",
    "        vc_keep = [0]+list(new_cols) + list(old_cols)\n",
    "    \n",
    "    rtn_df = df  #df.iloc[:,vc_keep]\n",
    "    rtn_df = rtn_df.iloc[:,vc_keep]\n",
    "        \n",
    "    return rtn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_create_tgt_matrix_pr07(mtx_data,tgt_metric,tgt_mod,nstations,nmetrics,ndays_col):\n",
    "    \n",
    "    mtx01 = mtx_data\n",
    "    nrows = mtx01.shape[0]\n",
    "    ncols = mtx01.shape[1]\n",
    "        \n",
    "    if tgt_metric == \"prec\" and tgt_mod == \"prec_ex01\": \n",
    "        rtn_matrix = fn_prec_ex01(mtx01,nstations,nmetrics,ndays_col)\n",
    "    \n",
    "    #nrows = rtn_matrix.shape[0]\n",
    "    #ncols = rtn_matrix.shape[1]\n",
    "    \n",
    "    #1. Modify tgt variable\n",
    "    # Shift the features down 1 row so they always occur before the tgt\n",
    "    #vc_tgt = mtx01[0:(nrows-1),0].copy()\n",
    "    #mt_feat = mtx01[1:(nrows),0:(ncols)].copy()\n",
    "    #mt_feat[:,0]=vc_tgt\n",
    "    \n",
    "    #Modify the target variable depending on the metric (precipitation or temperature)\n",
    "    #Modify tgt depending on tgt_mod, options could be running totals, moving averages, \n",
    "        \n",
    "    #2. Check that all columns and rows have some type of data\n",
    "    \n",
    "    \n",
    "        \n",
    "    #return the matrix for analysis\n",
    "        \n",
    "    return rtn_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_get_nearby_stations(stid_target, target_file,in_wd):\n",
    "\n",
    "    dt01 = target_file[target_file['tgt_stid']==stid_target]\n",
    "    dt01 = dt01[['feat_stid','dist_km']]\n",
    "    \n",
    "    vc_stid = dt01['feat_stid'].to_numpy()\n",
    "    vc_stid = np.insert(vc_stid,0,tgt_stid)\n",
    "    #vc_stid = [stid_target] + [vc_stid]\n",
    "    \n",
    "    #Need to create more combined data sets\n",
    "    lst_files = os.listdir(in_wd)\n",
    "    df_files = pd.DataFrame(lst_files,columns=[\"files\"])\n",
    "    df_files['stid'] = df_files['files'].str[:11]\n",
    "    df_files['istarget'] = df_files['stid'].str.contains(stid_target,case=False)\n",
    "    df_files['file_type'] = df_files['files'].str[23:31]\n",
    "    df_files['isintarget'] = df_files['stid'].isin(vc_stid)\n",
    "    \n",
    "    df_files = df_files[df_files['file_type']==\"combined\"]\n",
    "    df_files = df_files[df_files['isintarget']==True]\n",
    "    \n",
    "    mrg_df = pd.merge(df_files, dt01,how='left',left_on='stid',right_on='feat_stid')\n",
    "    mrg_df = mrg_df.drop(columns=['feat_stid','isintarget','file_type'])\n",
    "    mrg_df = mrg_df.sort_values(by='dist_km',na_position='first')\n",
    "    mrg_df = mrg_df.reset_index(drop=True)\n",
    "    mrg_df.columns = ['fnames','st_id','is_target_station','dist_km']\n",
    "    \n",
    "    return mrg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 1.  Select a station using ID to get its related stations\n",
    "\n",
    "\n",
    "#Load df of target stations and those close by\n",
    "#74486094789 is JFK airport\n",
    "#74486454787 is Farmingdale\n",
    "#72505004781 is Macarthur\n",
    "#72502014734 is Newark\n",
    "#Select the target station for which predictions will be made\n",
    "#Select the target station for which predictions will be made\n",
    "dt_trg_set2 = ipd02 + \"Station_Pairs_LE_100km_Info.csv\"\n",
    "dt_analysis_set2 = pd.read_csv(dt_trg_set2)\n",
    "#Governs the target station in subsequent modules\n",
    "tgt_stid =  \"74486094789\"\n",
    "\n",
    "#Nearby stations within 100km\n",
    "dt_analysis_set_keys = fn_get_nearby_stations(tgt_stid,dt_analysis_set2,ipd03)\n",
    "\n",
    "nstations_in_100km = len(dt_analysis_set_keys) #number of stations within 100 km\n",
    "num_st_keep = 20   #num of stations to keep for analysis\n",
    "nstations = min(nstations_in_100km,num_st_keep)\n",
    "\n",
    "#analysis_set = dt_trg_set\n",
    "#dt_analysis_set = pd.read_csv(analysis_set)\n",
    "#dt_analysis_set['path'] = ipd02\n",
    "#dt01 = dt_analysis_set[dt_analysis_set['target_station']==int(st_id)]\n",
    "#uniq_stat = dt01['vect_close_bkt2'].unique()[:2]\n",
    "#uniq_station = dt01['st_id'].unique()[:num_st_keep]\n",
    "#dt02 = dt01[dt01['st_id'].isin(uniq_station)]\n",
    "\n",
    "dt_analysis_set_keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 2.  Use elements created in STEP 1 to get a data matrix for analysis\n",
    "#All the available stations which have all three metrics needed for model\n",
    "#vc_stid = dt_analysis_set_keys[['st_id']].drop_duplicates()\n",
    "\n",
    "#Organize the data based on the nature of the data so they all conform to the same size matrices\n",
    "#The metrics included are\n",
    "#  AT - air temperature in Celcius\n",
    "#  RH - relative humidity in Percent\n",
    "#  PR - precipitation in MM\n",
    "\n",
    "#Put them all in appropriate matrices and create list of metrics\n",
    "#Data files are of two types\n",
    "#    -  24 hour * 365 days * n years  (AT, PR)\n",
    "#    -  365 days * n years (RH)\n",
    "\n",
    "#Parameters governing data matrix\n",
    "stid_target = tgt_stid\n",
    "stid_keys = dt_analysis_set_keys\n",
    "\n",
    "ipd = ipd02\n",
    "nhours = 24\n",
    "ndays = 365\n",
    "nmonths_row = 240\n",
    "nmonths_col = 2 #governs the number of months predicting that are needed\n",
    "ndays_col = 7  #governs the number of days predicting that are needed\n",
    "nmetrics = 5\n",
    "tgt_metric =  \"prec\" #\"prec\"  \"temp\"\n",
    "tgt_mod = \"prec_ex01\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#1. Get the relevant data in a list\n",
    "lst_data = fn_select_data_vectors_pr07(stid_keys,nstations,nhours,nmonths_row,nmonths_col)\n",
    "fn = f\"lst data done\"\n",
    "print(fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#2. Construct a matrix from most recent to most distant from the list data\n",
    "mtx_data = fn_create_data_matrix_pr07(lst_data,tgt_metric,nstations,nmetrics,nmonths_col,ndays_col)\n",
    "fn = f\"mtx data done\"\n",
    "print(fn)\n",
    "\n",
    "#Save the mtx_data to a folder\n",
    "#This lets us start at Step 3 for iterative runs\n",
    "print(ipd04)\n",
    "rtn_mtx_pqt = pd.DataFrame(mtx_data)\n",
    "print(rtn_mtx_pqt.shape)\n",
    "fn = f\"{ipd04}{tgt_metric}_mtx_{stid_target}_stations_{nstations}_RowMn_{nmonths_row}_ColDy_{ndays_col}.parquet\"\n",
    "print(fn)\n",
    "rtn_mtx_pqt.to_parquet(fn, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(172800, 30241)\n"
     ]
    }
   ],
   "source": [
    "#3.  Read in mtx_data file\n",
    "\n",
    "#Get the mtx_data from a folder\n",
    "#This lets us start at Step 3 for iterative runs\n",
    "fn = '\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\Py_S4\\\\Py_S4_v02_JHH\\\\NCEI_parquet_files\\\\prec_mtx_74486094789_stations_20_RowMn_240_ColDy_7.parquet'\n",
    "\n",
    "# Read the Parquet file into a Pandas DataFrame\n",
    "mtx_data = pd.read_parquet(fn)\n",
    "print(mtx_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.  Create a target variable and select feature variables\n",
    "\n",
    "nmetrics = 9\n",
    "\n",
    "s4_data = fn_create_tgt_matrix_pr07(mtx_data,\"prec\",\"prec_ex01\",nstations,nmetrics,ndays_col)\n",
    "print(f\"Shape of s4_data {s4_data.shape}\")\n",
    "fn = f\"s4 data done\"\n",
    "print(fn)\n",
    "\n",
    "\n",
    "#Save the s4_data to a folder\n",
    "#This lets us start at Step 3 for iterative runs\n",
    "print(f\"Done temp\")\n",
    "print(ipd04)\n",
    "rtn_s4_data_pqt = pd.DataFrame(s4_data)\n",
    "print(rtn_s4_data_pqt.shape)\n",
    "fn = f\"{ipd04}{tgt_metric}_s4data_{stid_target}_stations_{nstations}_RowMn_{nmonths_row}_ColDy_{ndays_col}.parquet\"\n",
    "print(fn)\n",
    "rtn_s4_data_pqt.to_parquet(fn, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jj = s4_data.iloc[0:1000,:]\n",
    "jj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  (172360, 5041)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "category",
         "type": "unknown"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "47196303-63ce-4b16-bbb1-2e471650a09d",
       "rows": [
        [
         "0",
         "166321"
        ],
        [
         "1",
         "6039"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 2
       }
      },
      "text/plain": [
       "0\n",
       "0    166321\n",
       "1      6039\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform_matrix(df, bins, labels):\n",
    "    \"\"\"\n",
    "    Transforms the given matrix by binning the first column, dropping the first row and column,\n",
    "    and adding the binned column as the first column.\n",
    "\n",
    "    Parameters:\n",
    "        matrix (pd.DataFrame): The input matrix.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The transformed matrix.\n",
    "    \"\"\"\n",
    "    # Extract the first column (dependent variable)\n",
    "    firstcolumn = df.iloc[:, 0].astype('float32')\n",
    "\n",
    "    # Bin the first column\n",
    "    binned_column = pd.cut(firstcolumn, bins=bins, labels=labels)\n",
    "\n",
    "    # Drop the first first column\n",
    "    df = df.iloc[:, 1:]\n",
    "\n",
    "    # Add the binned column as the first column\n",
    "    df.insert(0, 0, binned_column.iloc[0:].values)\n",
    "\n",
    "    # Return the final matrix\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "# Example matrix (replace this with your actual matrix)\n",
    "df = pd.DataFrame(s4_data).iloc[1:,] # Convert to DataFrame and drop the first row for binning\n",
    "# Drop rows where the first column has missing values\n",
    "# df = df.dropna(subset=[0])\n",
    "fc = df.iloc[:,0].astype('float32') # Ensure the first column is float for binning\n",
    "#DEL#bins = [-float('inf'), 10, 20, 25, float('inf')]\n",
    "\n",
    "bins = [-float('inf'), 1, float('inf')]\n",
    "# bins = [-float('inf'), 0] + list(pd.qcut(fc[fc > 0], q=4, retbins=True)[1][1:]) + [float('inf')]\n",
    "labels = [0, 1]\n",
    "# print(bins, labels)\n",
    "\n",
    "\n",
    "# bins = [-float('inf'), 0] + list(pd.qcut(fc[fc > 0], q=4, retbins=True)[1][1:]) + [float('inf')]\n",
    "#labels = [0, 1, 2, 3]\n",
    "# print(bins, labels)\n",
    "\n",
    "df_transformed = transform_matrix(df, bins, labels)\n",
    "df_transformed = df_transformed[df_transformed[0]>= 0]\n",
    "print(\"Dataset: \", df_transformed.shape)\n",
    "df_transformed[0].value_counts(dropna=False).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: (120652, 5041)\n",
      "0\n",
      "0    116260\n",
      "1      4392\n",
      "Name: count, dtype: int64\n",
      "Validation Set: (17236, 5041)\n",
      "0\n",
      "0    16626\n",
      "1      610\n",
      "Name: count, dtype: int64\n",
      "Test Set: (34472, 5041)\n",
      "0\n",
      "0    33435\n",
      "1     1037\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def split_time_series_data(df, train_ratio=0.7, val_ratio=0.1, test_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Splits a time series dataset into training, validation, and test sets.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input time series dataset, ordered in descending dates.\n",
    "        train_ratio (float): The proportion of data to use for training.\n",
    "        val_ratio (float): The proportion of data to use for validation.\n",
    "        test_ratio (float): The proportion of data to use for testing.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing three DataFrames (train, validation, test).\n",
    "    \"\"\"\n",
    "    assert train_ratio + val_ratio + test_ratio == 1.0, \"Ratios must sum to 1.0\"\n",
    "\n",
    "    # Calculate split indices\n",
    "    n = len(df)\n",
    "    test_end = int(n * test_ratio)    \n",
    "    val_end = test_end + int(n * val_ratio)\n",
    "    \n",
    "\n",
    "    # Split the data\n",
    "    test_data = df.iloc[:test_end].sort_index(ascending=False)\n",
    "    val_data = df.iloc[test_end:val_end].sort_index(ascending=False)\n",
    "    train_data = df.iloc[val_end:].sort_index(ascending=False)\n",
    "\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "# Example usage\n",
    "train_data, val_data, test_data = split_time_series_data(df_transformed)\n",
    "print(\"Training Set:\", train_data.shape)\n",
    "print(train_data[0].value_counts(dropna=False).sort_index())\n",
    "print(\"Validation Set:\", val_data.shape)\n",
    "print(val_data[0].value_counts(dropna=False).sort_index())\n",
    "print(\"Test Set:\", test_data.shape)\n",
    "print(test_data[0].value_counts(dropna=False).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets exported successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the station ID and output directory\n",
    "station_id = st_id\n",
    "output_dir = ipd02\n",
    "\n",
    "\n",
    "# Export datasets to CSV\n",
    "train_data.to_parquet(f\"{output_dir}{station_id}_train.parquet\", index=False)\n",
    "val_data.to_parquet(f\"{output_dir}{station_id}_validation.parquet\", index=False)\n",
    "test_data.to_parquet(f\"{output_dir}{station_id}_test.parquet\", index=False)\n",
    "\n",
    "# train_data.to_csv(f\"{output_dir}{station_id}_train.csv\", index=False)\n",
    "# val_data.to_csv(f\"{output_dir}{station_id}_validation.csv\", index=False)\n",
    "# test_data.to_csv(f\"{output_dir}{station_id}_test.csv\", index=False)\n",
    "\n",
    "print(\"Datasets exported successfully.\")\n",
    "\n",
    "#python -m s4model --modelname 72406093721 --trainset C:\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72406093721_train.parquet --valset C:\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72406093721_validation.parquet --testset C:\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72406093721_test.parquet --tabulardata --dependent_variable 0 --epochs 30\n",
    "\n",
    "# python -m charts --df ../results/72508014740_Test_results_20250424_104121PM.csv --actual 0 --predicted Predicted \n",
    "# python -m charts --df ../results/72508014740_Test_results_20250427_100132PM.csv --actual 0 --predicted Predicted \n",
    "# python -m charts --df ../results/72508014740_Test_results_20250428_091259PM.csv --actual 0 --predicted Predicted \n",
    "# python -m charts --df ../results/72508014740_Test_results_20250428_094033PM.csv --actual 0 --predicted Predicted \n",
    "# python -m charts --df ../results/72508014740_Test_results_20250429_091538PM.csv --actual 0 --predicted Predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2106272906.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[23], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    python -m s4model --modelname 72406093721 --trainset C:\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72406093721_train.parquet --valset C:\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72406093721_validation.parquet --testset C:\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72406093721_test.parquet --tabulardata --dependent_variable 0 --epochs 30\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Code to run at cmd\n",
    "#python -m s4model --modelname 72406093721 --trainset C:\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72406093721_train.parquet --valset C:\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72406093721_validation.parquet --testset C:\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72406093721_test.parquet --tabulardata --dependent_variable 0 --epochs 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_first_non_nan(vector):\n",
    "#     return next((i for i, x in enumerate(vector) if not math.isnan(x)), None)\n",
    "\n",
    "# first_number_at1 = find_first_non_nan(vc_at1)\n",
    "# first_number_rh1 = find_first_non_nan(vc_rh1)\n",
    "# first_number_rh2 = find_first_non_nan(vc_rh2)\n",
    "# first_number_rh3 = find_first_non_nan(vc_rh3)\n",
    "# first_number_pr1 = find_first_non_nan(vc_pr1)\n",
    "\n",
    "# #Number of missing data points\n",
    "# print(f\"AT1 Number of missing cells: {np.count_nonzero(np.isnan(vc_at1))}\")\n",
    "# print(f\"AT1 Vector Length: {len(vc_at1)}\")\n",
    "\n",
    "# print(f\"RH1 Number of missing cells: {np.count_nonzero(np.isnan(vc_rh1))}\")\n",
    "# print(f\"RH1 Vector Length: {len(vc_rh1)}\")\n",
    "\n",
    "# print(f\"PR1 Number of missing cells: {np.count_nonzero(np.isnan(vc_pr1))}\")\n",
    "# print(f\"PR1 Vector Length: {len(vc_pr1)}\")\n",
    "\n",
    "# #Once a day metrics\n",
    "# v_data_first_once_day = max(\n",
    "#                    first_number_rh1,\n",
    "#                    first_number_rh2,\n",
    "#                    first_number_rh3)\n",
    "# print(v_data_first_once_day)\n",
    "\n",
    "# #24 times a day metrics\n",
    "# v_data_first_24h_day = max(first_number_at1,\n",
    "#                    first_number_pr1)\n",
    "# print(v_data_first_24h_day)\n",
    "\n",
    "\n",
    "#These vectors all start with a numeric value so missing values can be filled\n",
    "\n",
    "\n",
    "#print(first_number_at1)\n",
    "#print(first_number_rh1)\n",
    "#print(first_number_rh2)\n",
    "#print(first_number_rh3)\n",
    "#print(first_number_pr1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(jj)\n",
    "fc = df.iloc[:,0].astype('float32')\n",
    "print(fc.shape)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(fc, bins=50, color='blue', alpha=0.7, density=True)\n",
    "plt.title('Normalized Histogram of a')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom bins\n",
    "\n",
    "a = fc[~fc.isnull()]\n",
    "\n",
    "# Calculate min, max, and percentiles by 10 for 'a'\n",
    "min_val = a.min()\n",
    "max_val = a.max()\n",
    "percentiles = np.percentile(a, np.arange(0, 101, 10))\n",
    "\n",
    "print(f\"Min: {min_val}\")\n",
    "print(f\"Max: {max_val}\")\n",
    "print(f\"Percentiles: {percentiles}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s4_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
