{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IV Load the data for a station and organize for precipitation model\n",
    "#Check for update\n",
    "import os as os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current user: prajwal\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    usr = os.environ.get('USER') or os.environ.get('USERNAME') or getpass.getuser()\n",
    "    if usr is None or str(usr).strip() == \"\":\n",
    "        raise ValueError(\"username is empty\")\n",
    "except Exception as e:\n",
    "    try:\n",
    "        usr = getpass.getuser()\n",
    "    except Exception:\n",
    "        usr = \"unknown\"\n",
    "    print(f\"Warning: could not determine username ({e}); using '{usr}'\")\n",
    "print(f\"Current user: {usr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths\n",
    "#Need to set up based on who is running\n",
    "# usr = \"JH\"\n",
    "if usr == \"prajwal\":\n",
    "    #ipd01 = \"\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\2024\\\\\"\n",
    "    ipd02 = \"D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\\"\n",
    "    ipd03 = \"D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\\"\n",
    "    ipd04 = \"D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\\"\n",
    "    \n",
    "if usr == \"JH\":\n",
    "    #ipd01 = \"\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\2024\\\\\"\n",
    "    #ipd02 = \"\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\\"\n",
    "    ipd02 = \"\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\Py_S4\\\\NCEI_Station_Info\\\\\"\n",
    "    ipd03 = \"\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\Py_S4\\\\NCEI_data\\\\\"\n",
    "    ipd04 = \"\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\Py_S4\\\\NCEI_parquet_files\\\\\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_select_data_vectors_pr07(stid_keys,nstations,nhours,nmonths_row,nmonths_col):\n",
    "    #Get the matrix of data to input into marix\n",
    "    #The S4 model\n",
    "    # 1. Read in combined data files for multiple stations\n",
    "    # 2. Find the common dates that will be used to align the matrices\n",
    "    #    Keep only the values that fit the common starting date \n",
    "\n",
    "    # 1. Read in data files and get start date v_date... for each\n",
    "    # Check that data exist for all metrics\n",
    "\n",
    "    df_stid = stid_keys\n",
    "    df_stid2 = df_stid.iloc[:, [0]]\n",
    "    df_stid2 = df_stid2.drop_duplicates() # unique()\n",
    "    \n",
    "    n_rows_stid = min(len(df_stid2),nstations)\n",
    "    #print(f\"n rows stid {n_rows_stid}\")\n",
    "    #Matrix in which measures are stored\n",
    "    n_vmt_rows = nhours*30*max(nmonths_row,nmonths_col)\n",
    "    \n",
    "    data_list = []\n",
    "    date_max_list =[]\n",
    "    \n",
    "    #1. Read in data files\n",
    "    for i in range(0, n_rows_stid):\n",
    "    #for i in range(0, 28):\n",
    "        infile = ipd03 + str(df_stid2.iloc[i,0]) #+ \"_model_data_combined.csv\"\n",
    "        print(infile)\n",
    "        \n",
    "        #Eliminate fully blank lines\n",
    "        cleaned_lines = []\n",
    "        with open(infile, 'r', newline='') as infile:\n",
    "            reader = csv.reader(infile)\n",
    "            for row in reader:\n",
    "                # Example: Skip rows that are entirely empty or have only blank 'name'\n",
    "                if any(cell.strip() for cell in row) and (len(row) < 1 or row[0].strip() != ''): # Assuming 'name' is the first column\n",
    "                    cleaned_lines.append(row)\n",
    "        \n",
    "        df_metrics = pd.DataFrame(cleaned_lines[1:], columns=cleaned_lines[0]) # Assuming first row is header\n",
    "\n",
    "        df_metrics['str_hr'] = df_metrics['nHOUR']  #.apply(lambda x: f'{x:02d}')\n",
    "        df_metrics['date_hr'] = pd.to_datetime(df_metrics['Date_YYYYMMDD'] + ' ' + df_metrics['str_hr'])\n",
    "        df_metrics['month_number'] = df_metrics['date_hr'].dt.month\n",
    "        df_metrics['week_number'] = df_metrics['date_hr'].dt.isocalendar().week\n",
    "        df_metrics['stid'] = str(df_stid2.iloc[i,0])\n",
    "        data_list.append(df_metrics)\n",
    "\n",
    "    #2. Align dates across data files\n",
    "    for i in range(0, n_rows_stid):\n",
    "        df = data_list[i]\n",
    "        max_date = max(df['date_hr'])     \n",
    "        date_max_list.append(max_date)\n",
    "    \n",
    "    date_filter = min(date_max_list)\n",
    "\n",
    "    for i in range(0, n_rows_stid):\n",
    "        df = data_list[i]\n",
    "\n",
    "        filtered_df = df[df['date_hr'] < date_filter]\n",
    "        filtered_df = filtered_df.iloc[(len(filtered_df) - n_vmt_rows):len(filtered_df)]\n",
    "        \n",
    "        idx_col = filtered_df.columns\n",
    "        idx_col = str(df_stid2.iloc[i,0]) + '_' + idx_col\n",
    "        filtered_df.columns = idx_col\n",
    "\n",
    "        print(filtered_df.shape)\n",
    "        \n",
    "        data_list[i] = filtered_df\n",
    "        \n",
    "    return data_list[0:n_rows_stid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_create_data_matrix_pr07(lst_data,tgt_metric,nstations,nmetrics,nmonths_col,ndays_col):\n",
    "    #Create a matrix for precipitation data that feeds\n",
    "    #The S4 model\n",
    "    # 1. Get data list and select target metric\n",
    "    #Two choices now\n",
    "    #    - prec\n",
    "    #    - temp\n",
    "    # 2.  Create output and move data into approprate cells\n",
    "\n",
    "    # 1. get data list details and reduce to metrics\n",
    "    data_vectors_list = []\n",
    "    dt_rows = len(lst_data[0])\n",
    "    df00 = []\n",
    "\n",
    "    dt_rows_minus_1 = dt_rows - 1\n",
    "    \n",
    "    if tgt_metric == \"prec\": tgt_cols = [10,2,3,4,5,6,8,9,11,12]\n",
    "    if tgt_metric == \"temp\": tgt_cols = [10,3,2,4,5,6,8,9,11,12]\n",
    "    non_metrics = 4  #These are the non sensor data brought in to analysis (e.g., day of year, month, ...)\n",
    "    \n",
    "    #Reset the indices as all records are ordered by date and hour\n",
    "    #But stations have missing records so not everything aligns\n",
    "    #Need to use the target station (station 0) as the standard and left join th other fields\n",
    "    print(f\"number of stations in data list {len(lst_data)}\")\n",
    "    n_data_files = len(lst_data)-1\n",
    "    \n",
    "    for i in range(n_data_files):\n",
    "        if i == 0:\n",
    "            tdt = lst_data[i].iloc[:,tgt_cols]\n",
    "        if i > 0:\n",
    "            tdt = lst_data[i].iloc[:,tgt_cols[0:6]]\n",
    "        tdt = tdt.iloc[::-1]\n",
    "        tdt.reset_index(drop=True, inplace=True)\n",
    "        first_column_name = tdt.columns[0]\n",
    "        tdt.rename(columns={first_column_name: 'key_date_hr'}, inplace=True)\n",
    "        #fn = f\"tdt data type {tdt['key_date_hr'].dtype}\"\n",
    "        #print(fn)\n",
    "        if tdt['key_date_hr'].dtype == \"datetime64[ns]\":\n",
    "            df00.append(tdt)\n",
    "            fn = f\"1. Done with setting up station -  {i}\"\n",
    "            #print(fn)\n",
    "    \n",
    "    \n",
    "    nstations_w_data = len(df00)\n",
    "    \n",
    "    #Take the list of station data, df00, and paste side by side\n",
    "    #to produce rs which is a large matrix of data for all stations \n",
    "    join_key = 'key_date_hr'\n",
    "    rs = reduce(lambda left, right: pd.merge(left, right, on=join_key, how='left'), df00)\n",
    "    \n",
    "    #rs has each row being a dayXhour for all stations for the stations full metrics\n",
    "    fn = f\"2. Shape of combined data including all stations\"\n",
    "    print(fn)\n",
    "    print(rs.shape)\n",
    "    \n",
    "\n",
    "    #2. Create output matrix and move data into appropriate cells\n",
    "    #Note: target cells get 9 elements, feature cells get 5 elements\n",
    "    \n",
    "    nmetrics_and_non_metrics = nmetrics + non_metrics\n",
    "    fn = f\"nmetrics_and_non_metrics {nmetrics_and_non_metrics}\"\n",
    "    print(fn)\n",
    "    #OLDndataelements = nmetrics_and_non_metrics + (nstations-1)*(nmetrics)\n",
    "    ndataelements = nmetrics_and_non_metrics + (nstations_w_data-1)*(nmetrics)\n",
    "    fn = f\"ndataelements - stations by metrics {ndataelements}\"\n",
    "    print(fn)\n",
    "    mt_cols = 1 + ndataelements*ndays_col*24\n",
    "    fn = f\"mt_cols 1 + #data elements*ndays_col*24 {mt_cols}\"\n",
    "    print(fn)\n",
    "    mt_rows = len(rs)\n",
    "    \n",
    "    #mt_rows = 170000  #keep here for development until done\n",
    "    mt_full_set = np.empty((mt_rows,mt_cols),np.float16)\n",
    "    \n",
    "    #Create matrix of data\n",
    "    for i in range(0,mt_rows):\n",
    "        mt_full_set[i,range(0,ndataelements)] = rs.iloc[i,range(1,ndataelements+1)].apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    fn = f\"3. Full mt setup with initial data for the dayXhour only\"\n",
    "    print(fn)  \n",
    "    print(mt_full_set.shape)\n",
    "    \n",
    "    #Add additional column sets\n",
    "    lpi = mt_rows - ndays_col*24 - 1  #number of rows in rs matrix - the data elements in a column... \n",
    "                                      #ensures all records have data\n",
    "    \n",
    "    #limit of rows in data set so all matrix rows have data\n",
    "    lpj = ndays_col*24 -1   #number of colum sets to fill, each with 104 columns\n",
    "        \n",
    "    \n",
    "    #The row I am operating on\n",
    "    for i in range(0,lpi):\n",
    "        #The columns I am going to fill\n",
    "        for j in range(0,lpj):\n",
    "        \n",
    "            row_get = i + j + 1              #For every new column in current row we have to go down one more row\n",
    "            col_get_start = 0                #data retrieved starting column 1\n",
    "            col_get_end = ndataelements - 1\n",
    "            \n",
    "            row_put = i\n",
    "            col_put_start = ndataelements*(row_get - row_put)\n",
    "            col_put_end = col_put_start + ndataelements - 1\n",
    "            \n",
    "            mt_full_set[row_put,col_put_start:col_put_end] = mt_full_set[row_get,col_get_start:col_get_end]\n",
    "    \n",
    "    fn = f\"4. mt_full_set with ndays of data added to the columns\"\n",
    "    print(fn)  \n",
    "    print(mt_full_set.shape)\n",
    "    \n",
    "    return nstations_w_data, mt_full_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_create_data_matrix_DAY_SUMMARY(lst_data,tgt_metric,nstations,nmetrics,non_metrics,nmonths_col,ndays_col,ndays_to_target):\n",
    "    #Create a matrix for precipitation data that feeds\n",
    "    #The S4 model\n",
    "    # 1. Get data list and select target metric\n",
    "    #Two choices now\n",
    "    #    - prec\n",
    "    #    - temp\n",
    "    # 2.  Create output and move data into approprate cells\n",
    "    #print(\"HEREJHH\")\n",
    "    # 1. get data list details and reduce to metrics\n",
    "    data_vectors_list = []\n",
    "    dt_rows = len(lst_data[0])\n",
    "    df00 = []\n",
    "\n",
    "    dt_rows_minus_1 = dt_rows - 1\n",
    "    \n",
    "    if tgt_metric == \"prec\": \n",
    "        tgt_cols = [13,10,2,3,4,5,6]\n",
    "        ncol_names = ['station','key_date_hr','precip','airtemp','maxRH','relhum','minRH']\n",
    "    if tgt_metric == \"temp\": \n",
    "        tgt_cols = [13,10,3,2,4,5,6]\n",
    "        ncol_names = ['station','key_date_hr','airtemp','precip','maxRH','relhum','minRH']\n",
    "        \n",
    "    #Reset the indices as all records are ordered by date and hour\n",
    "    #But stations have missing records so not everything aligns\n",
    "    #Need to use the target station (station 0) as the standard and left join th other fields\n",
    "    print(f\"number of stations in data list {len(lst_data)}\")\n",
    "    n_data_files = min(len(lst_data)-1,nstations) #TB DELETE\n",
    "    n_data_files = min(len(lst_data),nstations)\n",
    "    #print(f\"n_data_files HERE {n_data_files}\")\n",
    "    \n",
    "    for i in range(n_data_files):\n",
    "        \n",
    "        tdt = lst_data[i].iloc[:,tgt_cols]\n",
    "        tdt = tdt.iloc[::-1]  #Reverses row order\n",
    "        tdt.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        for j in range(len(tgt_cols)):\n",
    "            curr_column_name = tdt.columns[j]\n",
    "            tdt.rename(columns={curr_column_name: ncol_names[j]}, inplace=True)\n",
    "        \n",
    "        if tdt['key_date_hr'].dtype == \"datetime64[ns]\":\n",
    "            \n",
    "            tdt = tdt.set_index('key_date_hr')\n",
    "            tdt['precip'] = pd.to_numeric(tdt['precip'],errors='coerce')\n",
    "            tdt['precip'] = tdt['precip'].fillna(0)\n",
    "            tdt['precip_24hr_msum'] = tdt['precip'].rolling(window='24h').sum()\n",
    "            tdt['precip_24hr_mav'] = tdt['precip'].rolling(window='24h',min_periods=6).mean()\n",
    "            \n",
    "            tdt['airtemp'] = pd.to_numeric(tdt['airtemp'],errors='coerce')\n",
    "            tdt['airtemp_24hr_mav'] = tdt['airtemp'].rolling(window='24h',min_periods=6).mean()\n",
    "            \n",
    "            tdt['maxRH'] = pd.to_numeric(tdt['maxRH'],errors='coerce')\n",
    "            tdt['maxRH_24hr_mav'] = tdt['maxRH'].rolling(window='24h',min_periods=6).mean()\n",
    "    \n",
    "            tdt['relhum'] = pd.to_numeric(tdt['relhum'],errors='coerce')\n",
    "            tdt['relhum_24hr_mav'] = tdt['relhum'].rolling(window='24h',min_periods=6).mean()\n",
    "            \n",
    "            tdt['minRH'] = pd.to_numeric(tdt['minRH'],errors='coerce')\n",
    "            tdt['minRH_24hr_mav'] = tdt['minRH'].rolling(window='24h',min_periods=6).mean()        \n",
    "            \n",
    "            new_column_names = []\n",
    "            stri = str(i)\n",
    "            for n, col in enumerate(tdt.columns):\n",
    "                if i < 10: \n",
    "                    prefix = f\"st0{stri}_\"\n",
    "                    new_column_names.append(prefix + col)\n",
    "                \n",
    "                if i > 9: \n",
    "                    prefix = f\"st{stri}_\"\n",
    "                    new_column_names.append(prefix + col)\n",
    "\n",
    "            \n",
    "            #print(new_column_names)    \n",
    "            tdt.columns = new_column_names\n",
    "            df00.append(tdt)\n",
    "    \n",
    "    \n",
    "    df_valid_dates = pd.to_datetime(df00[0].index)\n",
    "    \n",
    "    print(f\"Number of valid rows: {len(df_valid_dates)}\")\n",
    "    \n",
    "    #Use a list comprehension to reindex each DataFrame, filling missing values with NaN.\n",
    "    df01 = [tdt.reindex(df_valid_dates) for tdt in df00]\n",
    "    df02 = [tdt.iloc[:,[0,6,7,8,9,10,11]] for tdt in df01]\n",
    "        \n",
    "    #Concatenate the reindexed DataFrames into a single DataFrame if needed.\n",
    "    rs = pd.concat(df02, axis=1)\n",
    "                \n",
    "    nstations_w_data = len(df02)\n",
    "    print(f\"Number of stations with data: {nstations_w_data}\")\n",
    "\n",
    "    fn = f\"2. Shape of combined data including all stations\"\n",
    "    print(fn)\n",
    "    print(rs.shape)\n",
    "    \n",
    "    #2. Create output matrix and move data into appropriate cells\n",
    "    #Note: target cells get 9 elements, feature cells get 5 elements\n",
    "    \n",
    "    nmetrics_and_non_metrics = nmetrics + non_metrics\n",
    "    fn = f\"nmetrics - {nmetrics_and_non_metrics}\"\n",
    "    print(fn)\n",
    "    \n",
    "    ndataelements = (nstations_w_data)*(nmetrics)\n",
    "    fn = f\"ndataelements - stations by metrics {ndataelements}\"\n",
    "    print(fn)\n",
    "    \n",
    "    rows_to_cols_sets = (ndays_col+ndays_to_target)*24\n",
    "    mt_cols = ndataelements*rows_to_cols_sets   #add two columns for the month and hour for the first set of data\n",
    "    fn = f\"mt_cols - ndataelements*(ndays_col+ndays_to_target)*24 {mt_cols}\"\n",
    "    print(fn)\n",
    "\n",
    "    #Identify the rows down to start\n",
    "\n",
    "    rows_to_include = len(df_valid_dates) - rows_to_cols_sets - 1\n",
    "    fn = f\"rows to include - len(df_valid_dates) - rows_to_cols_sets - 1 {rows_to_include}\"\n",
    "    print(fn)\n",
    "     \n",
    "    mt_full_set = np.empty((rows_to_include,mt_cols),np.float16)\n",
    "    fn = f\"mt_shape - {mt_full_set.shape}\"\n",
    "    print(fn)\n",
    "\n",
    "    ar_str_mon = np.array(rs.index.strftime('%m')).reshape(-1,1)\n",
    "    ar_str_mon = ar_str_mon[0:rows_to_include]\n",
    "    ar_str_hr = np.array(rs.index.strftime('%H')).reshape(-1,1)\n",
    "    ar_str_hr = ar_str_hr[0:rows_to_include]\n",
    "    rs.drop('st00_station', axis=1, inplace=True)\n",
    "    \n",
    "    #Create matrix of data\n",
    "    #rows_to_include = 1000  #number of col_rows to be copied each time from column A and placed in col B    \n",
    "    mt_loops = rows_to_cols_sets     #number of loops over which to copy each group of rows\n",
    "    \n",
    "    j = 0  #Loop across stations, set to operate only on the target station\n",
    "    for i in range(0,mt_loops):\n",
    "        mt_full_set[j:rows_to_include,range(i*nmetrics,i*nmetrics + nmetrics)] = rs.iloc[i:(i+rows_to_include),0:(nmetrics+1)].apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    #Add date month and hour info\n",
    "    mt_full_set_rtn = np.concatenate((ar_str_mon,ar_str_hr,mt_full_set),axis=1)\n",
    "    \n",
    "    fn = f\"3. Full mt setup with initial data for the dayXhour only\"\n",
    "    print(fn)  \n",
    "    print(mt_full_set_rtn.shape)\n",
    "    \n",
    "    return mt_full_set_rtn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_create_tgt_matrix_pr07(mtx_data,tgt_metric,tgt_mod,nstations,nmetrics,ndays_col):\n",
    "    \n",
    "    mtx01 = mtx_data #.iloc[0:1000,:]\n",
    "    nrows = mtx01.shape[0]\n",
    "    ncols = mtx01.shape[1]\n",
    "    \n",
    "       \n",
    "    if tgt_metric == \"prec\" and tgt_mod == \"prec_ex01\": \n",
    "        rtn_matrix = fn_prec_ex01(mtx01,nstations,nmetrics,ndays_col)\n",
    "    \n",
    "    \n",
    "    #return the matrix for analysis\n",
    "    return rtn_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_create_DAY_SUMMARY_tgt_matrix_pr07(mtx_data,tgt_metric,tgt_mod,nstations,nmetrics,ndays_col,ndays_to_target):\n",
    "    \n",
    "    mtx01 = mtx_data  #.iloc[0:100000,:]\n",
    "    nrows = mtx01.shape[0]\n",
    "    ncols = mtx01.shape[1]\n",
    "    \n",
    "    if tgt_metric == \"prec\" and tgt_mod == \"prec_ex02\": \n",
    "        rtn_matrix = fn_prec_ex02(mtx01,1,nmetrics,ndays_col,ndays_to_target)\n",
    "    \n",
    "    return rtn_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_prec_ex01(mtx01,nstations,nmetrics,dayscol):\n",
    "    \n",
    "    df = pd.DataFrame(mtx01).reset_index(drop=True)\n",
    "    ncols_orig = df.shape[1]\n",
    "    print(f\"Number of mxt columns {ncols_orig}\")\n",
    "    \n",
    "    #Data come in as follows\n",
    "    #target station \n",
    "    #      - number of metrics (nmetrics)\n",
    "    #      - date columns (4)\n",
    "    #feat station 1  - number of metrics (nmetrics)\n",
    "    #feat station ...  - number of metrics (nmetrics)\n",
    "    #feat station nstations  - number of metrics (nmetrics)\n",
    "    #total columns = (nmetrics + date columns) + (nstations+nmetrics)\n",
    "    \n",
    "    offset = 4  #additional fields for target station\n",
    "    data_columns_start = (nmetrics + offset) + (nstations - 1)*nmetrics\n",
    "    print(f\"data_columns_start {data_columns_start}\")\n",
    "    \n",
    "    tot_per = dayscol*24  #days by 24 hours\n",
    "    \n",
    "    for iper in range(1,tot_per,1):\n",
    "\n",
    "        vc_start1 = np.array([(iper*nstations*(nmetrics) + (iper*offset))])\n",
    "        vc_start2 = vc_start1 + nmetrics + offset   #first set has day/month/week/hr info\n",
    "        vc_end = vc_start2 + (nstations-1)*nmetrics\n",
    "        vc_s2_end = np.arange(vc_start2.item(),vc_end.item(),nmetrics)\n",
    "        vc_num = np.concatenate([vc_start1,vc_s2_end])\n",
    "        \n",
    "        nm1 = 'avg'+ str(iper)\n",
    "        pds1 = df.iloc[:,vc_num].mean(axis=1,skipna=True)\n",
    "        df[nm1] = pds1\n",
    "        \n",
    "        nm2 = 'cnt_ge_zero'+ str(iper)\n",
    "        pds2 = df.iloc[:,vc_num].apply(cnt_ge0,axis=1)\n",
    "        df[nm2] = pds2\n",
    "        \n",
    "        nm3 = 'cnt_gt_zero'+ str(iper)\n",
    "        pds3 = df.iloc[:,vc_num].apply(cnt_gt0,axis=1)\n",
    "        df[nm3] = pds3\n",
    "               \n",
    "    ncols_final = df.shape[1]\n",
    "    print(f\"Final shape {ncols_final}\")\n",
    "    new_cols = range(ncols_orig,ncols_final,1)\n",
    "    print(f\"New columns {new_cols}\")\n",
    "    old_cols = range(data_columns_start,ncols_orig-1,1)\n",
    "    print(f\"Old columns {old_cols}\")\n",
    "    vc_keep = [0]+list(new_cols) + list(old_cols)\n",
    "    print(f\"Final columns {ncols_final}\")\n",
    "    \n",
    "    df.rename(columns={'0': 'tgt_precip', '5': 'day_of_year'\n",
    "                       , '6': 'hour_of_day'\n",
    "                       , '7': 'month_of_year'\n",
    "                       , '8': 'week_of_year'}, inplace=True)\n",
    "    \n",
    "    df_out = pd.concat([df.iloc[:,[0,5,7,8,6]],df.iloc[:,new_cols],df.iloc[:,old_cols]],axis=1)\n",
    "    \n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_prec_ex02(mtx01,nstations,nmetrics,dayscol,ndays_to_target):\n",
    "    \n",
    "    df = pd.DataFrame(mtx01).reset_index(drop=True)\n",
    "    ncols_orig = df.shape[1]\n",
    "    print(f\"Number of mxt columns {ncols_orig}\")\n",
    "    \n",
    "    #Data come in as follows\n",
    "    #target station\n",
    "    #      - date columns (0,1) Month, Hour\n",
    "    #      - number of metrics (6 nmetrics)\n",
    "    #      - number of sets of day by hour (dayscol*24)\n",
    "    \n",
    "\n",
    "    data_columns_start = 2\n",
    "    tgt_column = 3   # precip sum   \n",
    "    tot_per = dayscol*24  #days by 24 hours\n",
    "    \n",
    "    cols = df.columns.tolist()\n",
    "    ncols = [cols[2],cols[0],cols[1]] + cols[3:]\n",
    "    #df_out = df.reindex(columns)\n",
    "    \n",
    "    \n",
    "    dt_start = 2 + ndays_to_target*nmetrics*24\n",
    "    \n",
    "    vc = [2,0,1] + list(range(dt_start,4754))\n",
    "    df_out = df.iloc[:,vc]\n",
    "    \n",
    "    df_out.rename(columns={'2': 'tgt_precip'\n",
    "                       , '1': 'hour_of_day'\n",
    "                       , '0': 'month_of_year'}, inplace=True)\n",
    "                       \n",
    "        \n",
    "    return df_out #new_order_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions to identify stations in network that have rain sensor and rain\n",
    "def cnt_ge0(row):\n",
    "        cnt_gt_0 = sum(row >= 0)\n",
    "        return cnt_gt_0\n",
    "\n",
    "def cnt_gt0(row):\n",
    "        cnt_gt_0 = sum(row > 0)\n",
    "        return cnt_gt_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_get_nearby_stations(stid_target, target_file,in_wd):\n",
    "\n",
    "    dt01 = target_file[target_file['tgt_stid']==stid_target]\n",
    "    dt01 = dt01[['feat_stid','dist_km']]\n",
    "    vc_stid = dt01['feat_stid'].to_numpy()\n",
    "    \n",
    "    vc_stid = np.insert(vc_stid,0,stid_target)\n",
    "    \n",
    "    #Need to create more combined data sets\n",
    "    lst_files = os.listdir(in_wd)\n",
    "    df_files = pd.DataFrame(lst_files,columns=[\"files\"])\n",
    "    df_files['stid'] = df_files['files'].str[:11]\n",
    "    df_files['istarget'] = df_files['stid'].str.contains(stid_target,case=False)\n",
    "    df_files['file_type'] = df_files['files'].str[23:31]\n",
    "    df_files['isintarget'] = df_files['stid'].isin(vc_stid)\n",
    "    \n",
    "    df_files = df_files[df_files['file_type']==\"combined\"]\n",
    "    df_files = df_files[df_files['isintarget']==True]\n",
    "    \n",
    "    mrg_df = pd.merge(df_files, dt01,how='left',left_on='stid',right_on='feat_stid')\n",
    "    mrg_df = mrg_df.drop(columns=['feat_stid','isintarget','file_type'])\n",
    "    mrg_df = mrg_df.sort_values(by='dist_km',na_position='first')\n",
    "    mrg_df = mrg_df.reset_index(drop=True)\n",
    "    mrg_df.columns = ['fnames','st_id','is_target_station','dist_km']\n",
    "    \n",
    "    return mrg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_create_data_sets(tgt_stid):\n",
    "    \n",
    "    print(tgt_stid)\n",
    "    \n",
    "    #Select the target station for which predictions will be made\n",
    "    dt_trg_set2 = ipd02 + \"Station_Pairs_LE_100km_Info.csv\"\n",
    "    dt_analysis_set2 = pd.read_csv(dt_trg_set2)\n",
    "\n",
    "    #Nearby stations within 100km\n",
    "    #dt_analysis_set_keys = fn_get_nearby_stations(tgt_stid,dt_analysis_set2,ipd03)\n",
    "    \n",
    "    dt_analysis_set_keys = dt_analysis_set2[dt_analysis_set2['tgt_stid'] == tgt_stid]    \n",
    "    print(f\"dt_analysis_set_keys{dt_analysis_set_keys}\")\n",
    "\n",
    "    dt_analysis_set_keys['fpn'] = ipd03 + dt_analysis_set_keys['feat_stid']+\"_model_data_combined.csv\"\n",
    "\n",
    "    dt_analysis_set_keys['file exists'] = dt_analysis_set_keys['fpn'].apply(lambda x: os.path.exists(x))\n",
    "\n",
    "    dt_analysis_set_keys = dt_analysis_set_keys[dt_analysis_set_keys['file exists'] == True]\n",
    "\n",
    "    nstations_in_100km = len(dt_analysis_set_keys) #number of stations within 100 km\n",
    "    num_st_keep = 20   #num of stations to keep for analysis\n",
    "    nstations = min(nstations_in_100km,num_st_keep)\n",
    "    \n",
    "    #Specify parameters governing the creation of data matrix for analysis\n",
    "    #All the available stations which have all three metrics needed for model\n",
    "    #vc_stid = dt_analysis_set_keys[['st_id']].drop_duplicates()\n",
    "\n",
    "    #Organize the data based on the nature of the data so they all conform to the same size matrices\n",
    "    #The metrics included are\n",
    "    #  AT - air temperature in Celcius\n",
    "    #  RH - relative humidity in Percent\n",
    "    #  PR - precipitation in MM\n",
    "\n",
    "    #Put them all in appropriate matrices and create list of metrics\n",
    "    #Data files are of two types\n",
    "    #    -  24 hour * 365 days * n years  (AT, PR)\n",
    "    #    -  365 days * n years (RH)\n",
    "    \n",
    "    #Parameters governing data matrix\n",
    "    stid_target = tgt_stid\n",
    "    stid_keys = dt_analysis_set_keys\n",
    "\n",
    "    #print(f\"st id keys{dt_analysis_set_keys}\")\n",
    "    \n",
    "    ipd = ipd02\n",
    "    nhours = 24\n",
    "    ndays = 365\n",
    "    nmonths_row = 240  #total months on rows of data\n",
    "    nmonths_col = 2 #governs the number of months predicting that are needed\n",
    "    ndays_col = 3  #governs the number of days predicting that are needed\n",
    "    nmetrics = 5  #number of metrics in the data file\n",
    "    tgt_metric =  \"prec\" #\"prec\" \"temp\"    the metric to be modeled\n",
    "    tgt_mod = \"prec_ex01\"  #specifies the metric specific approach for creating data set\n",
    "    \n",
    "    #1. Get the relevant station data and return in a list\n",
    "    lst_data = fn_select_data_vectors_pr07(stid_keys,nstations,nhours,nmonths_row,nmonths_col)\n",
    "    fn = f\"std id keys {stid_keys}\"\n",
    "    print(fn)\n",
    "    \n",
    "    #2. Construct a matrix from most recent to most distant data from the list data\n",
    "    nstations_w_data, mtx_data = fn_create_data_matrix_pr07(lst_data,tgt_metric,nstations,nmetrics,nmonths_col,ndays_col)\n",
    "    fn = f\"mtx data done\"\n",
    "    print(fn)\n",
    "    \n",
    "    #Save the mtx_data to a folder\n",
    "    #This lets us start at Step 3 for subsequent runs\n",
    "    print(ipd04)\n",
    "    rtn_mtx_pqt = pd.DataFrame(mtx_data)\n",
    "    print(rtn_mtx_pqt.shape)\n",
    "    fn = f\"{ipd04}{tgt_metric}_mtx_{stid_target}_stations_{nstations_w_data}_RowMn_{nmonths_row}_ColDy_{ndays_col}.parquet\"\n",
    "    print(fn)\n",
    "    rtn_mtx_pqt.to_parquet(fn, index=False)    \n",
    "    \n",
    "    #3.  Create a target variable, select feature variables, create data frame for input into s4 analysis\n",
    "    s4_data = fn_create_tgt_matrix_pr07(mtx_data,\"prec\",\"prec_ex01\",nstations_w_data,nmetrics,ndays_col)\n",
    "    \n",
    "    print(f\"Shape of s4_data {s4_data.shape}\")\n",
    "    fn = f\"s4 data done\"\n",
    "    print(fn)\n",
    "    \n",
    "    #4. Save the s4_data to a folder\n",
    "    #This lets us start at Step 4 for iterative runs\n",
    "\n",
    "    print(ipd04)\n",
    "    rtn_s4_data_pqt = pd.DataFrame(s4_data)\n",
    "    print(rtn_s4_data_pqt.shape)\n",
    "    fn = f\"{ipd04}{tgt_metric}_s4data_{stid_target}_stations_{nstations_w_data}_RowMn_{nmonths_row}_ColDy_{ndays_col}.parquet\"\n",
    "    print(fn)\n",
    "    rtn_s4_data_pqt.to_parquet(fn, index=False)    \n",
    "    \n",
    "    # Prep s4 data for modeling\n",
    "    # Example matrix (replace this with your actual matrix)\n",
    "    df = pd.DataFrame(s4_data).iloc[1:,] # Convert to DataFrame and drop the first row for binning\n",
    "    print(f\"All records at start {df.shape}\")\n",
    "    ##Only keep records w rain\n",
    "    #df = df[df.iloc[:,0]>0]\n",
    "    #print(df.shape)\n",
    "\n",
    "    #Drop rows where there has been no rain in total area for past 48 hours\n",
    "    ndays_check_for_rain = ndays_col\n",
    "    range_of_data = ndays_check_for_rain*ndays_col*(24) + ndays_col\n",
    "    vc_num = [x for x in range(5,range_of_data,3)]\n",
    "\n",
    "    nm1 = 'sum_avg'\n",
    "    pds1 = df.iloc[:,vc_num].sum(axis=1,skipna=True)\n",
    "    df_area_non_zero = df[pds1 > 0]\n",
    "    print(f\"Records with some rain in last {ndays_check_for_rain} days {df_area_non_zero.shape}\")\n",
    "\n",
    "    #Separate into dataframes with and withot rain in past n days\n",
    "    df_area_non_zero_yrain = df_area_non_zero[df_area_non_zero.iloc[:,0]>0]\n",
    "    df_area_non_zero_nrain = df_area_non_zero[df_area_non_zero.iloc[:,0]==0]\n",
    "\n",
    "    #sample non-rain to help balance outcomes if needed\n",
    "    df_sample_nrain = df_area_non_zero_nrain  #df_area_non_zero_nrain.sample(n=len(df_area_non_zero_yrain))\n",
    "    df_area_non_zero_ynrain = pd.concat((df_area_non_zero_yrain,df_sample_nrain),axis=0)\n",
    "\n",
    "    #Choose the df for analysis\n",
    "    dfs = df_area_non_zero_ynrain\n",
    "    print(f\"Full rain and no rain {dfs.shape}\")\n",
    "\n",
    "    del df, df_area_non_zero_yrain, df_area_non_zero_nrain, df_area_non_zero_ynrain\n",
    "    \n",
    "    # Drop rows where the first column has missing values\n",
    "    # df = df.dropna(subset=[0])\n",
    "    condition = np.isfinite(dfs.iloc[:,0])\n",
    "    df_subset = pd.DataFrame(dfs[condition])\n",
    "    del dfs\n",
    "    print(df_subset.shape)\n",
    "\n",
    "\n",
    "    bins = [-float('inf'),0.1,0.5,1,2,3,4,5,float('inf')]\n",
    "    labels = [0,0.1, 0.5,1,2,3,4,5]\n",
    "    print(bins, labels)\n",
    "\n",
    "    df_transformed = transform_matrix_precip(df_subset, bins, labels)\n",
    "\n",
    "    df_transformed.drop(columns=df_transformed.columns[1],inplace=True)\n",
    "    df_transformed.shape\n",
    "\n",
    "    df_transformed = df_transformed.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    print(\"Analysis Set:\", df_transformed.shape)\n",
    "\n",
    "    print(df_transformed['tgt_bin'].value_counts(dropna=False).sort_index())\n",
    "\n",
    "    train_data, val_data, test_data = split_time_series_data(df_transformed)\n",
    "    \n",
    "    train_data.to_parquet(f\"{ipd04}{stid_target}_train.parquet\", index=False)\n",
    "    val_data.to_parquet(f\"{ipd04}{stid_target}_validation.parquet\", index=False)\n",
    "    test_data.to_parquet(f\"{ipd04}{stid_target}_test.parquet\", index=False)\n",
    "\n",
    "    # code for saving to csv for testing and checking\n",
    "    # train_data.to_csv(f\"{ipd04}{stid_target}_train.csv\", index=False)\n",
    "    # val_data.to_csv(f\"{ipd04}{stid_target}_validation.csv\", index=False)\n",
    "    # test_data.to_csv(f\"{ipd04}{stid_target}_test.csv\", index=False)\n",
    "    \n",
    "    print(\"Datasets exported successfully.\") \n",
    "    \n",
    "    return 1 #mtx_data  #stid_keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_matrix_precip(df, bins, labels):\n",
    "    \"\"\"\n",
    "    Transforms the given matrix by binning the first column, dropping the first row and column,\n",
    "    and adding the binned column as the first column.\n",
    "\n",
    "    Parameters:\n",
    "        matrix (pd.DataFrame): The input matrix.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The transformed matrix.\n",
    "    \"\"\"\n",
    "    # Extract the first column (dependent variable)\n",
    "    firstcolumn = df.iloc[:, 0].astype('float32')\n",
    "    \n",
    "    # Bin the first column\n",
    "    binned_column = pd.cut(firstcolumn, bins=bins, labels=labels)\n",
    "    \n",
    "    # Add the binned column as the first column\n",
    "    df.insert(loc=0, column='tgt_bin', value=binned_column.iloc[0:].values)\n",
    "    \n",
    "    # Drop columns 3 the first first column\n",
    "    #df = df.drop(columns=df.columns[[3]])\n",
    "\n",
    "    # Return the final matrix\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_time_series_data(df, train_ratio=0.6, val_ratio=0.2, test_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Splits a time series dataset into training, validation, and test sets.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input time series dataset, ordered in descending dates.\n",
    "        train_ratio (float): The proportion of data to use for training.\n",
    "        val_ratio (float): The proportion of data to use for validation.\n",
    "        test_ratio (float): The proportion of data to use for testing.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing three DataFrames (train, validation, test).\n",
    "    \"\"\"\n",
    "    assert train_ratio + val_ratio + test_ratio == 1.0, \"Ratios must sum to 1.0\"\n",
    "\n",
    "    # Calculate split indices\n",
    "    n = len(df)\n",
    "    test_end = int(n * test_ratio)    \n",
    "    val_end = test_end + int(n * val_ratio)\n",
    "    \n",
    "\n",
    "    # Split the data\n",
    "    test_data = df.iloc[:test_end].sort_index(ascending=False)\n",
    "    val_data = df.iloc[test_end:val_end].sort_index(ascending=False)\n",
    "    train_data = df.iloc[val_end:].sort_index(ascending=False)\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DAY SUMMARY control\n",
    "def stid_loop(tgt_stid,part):\n",
    "\n",
    "    #Select the target station for which predictions will be made\n",
    "    #tgt_stid =  instid #\"72502014734\" #instid    # \"72502014734\"  large file \"74788012810\"\n",
    "    save_files = 1   #1 means save them and 0 means do not save them\n",
    "    run_files = 1    #1 means do it and 0 means do not do it\n",
    "    print(tgt_stid)\n",
    "    #Nearby stations within 100km\n",
    "    dt_trg_set2 = ipd02 + \"Station_Pairs_LE_100km_Info.csv\"\n",
    "    dt_analysis_set2 = pd.read_csv(dt_trg_set2)\n",
    "\n",
    "    dt_analysis_set_keys = fn_get_nearby_stations(tgt_stid,dt_analysis_set2,ipd03)\n",
    "    nstations_in_100km = len(dt_analysis_set_keys) #number of stations within 100 km\n",
    "    num_st_keep = 20   #num of stations to keep for analysis\n",
    "    nstations = min(nstations_in_100km,num_st_keep)\n",
    "    \n",
    "    #dt_analysis_set_keys\n",
    "\n",
    "    #Specify parameters governing the creation of data matrix for analysis\n",
    "    #All the available stations which have all three metrics needed for model\n",
    "    #vc_stid = dt_analysis_set_keys[['st_id']].drop_duplicates()\n",
    "\n",
    "    #Organize the data based on the nature of the data so they all conform to the same size matrices\n",
    "    #The metrics included are\n",
    "    #  AT - air temperature in Celcius\n",
    "    #  RH - relative humidity in Percent\n",
    "    #  PR - precipitation in MM\n",
    "\n",
    "    #Put them all in appropriate matrices and create list of metrics\n",
    "    #Data files are of two types\n",
    "    #    -  24 hour * 365 days * n years  (AT, PR)\n",
    "    #    -  365 days * n years (RH)\n",
    "\n",
    "    #Parameters governing data matrix\n",
    "    stid_target = tgt_stid\n",
    "    stid_keys = dt_analysis_set_keys\n",
    "\n",
    "    ipd = ipd02\n",
    "    nhours = 24\n",
    "    ndays = 365\n",
    "    nmonths_row = 180 #240  #total months on rows of data\n",
    "    nmonths_col = 2 #governs the number of months predicting that are needed\n",
    "    ndays_col = 3  #governs the number of days predicting that are needed\n",
    "    nmetrics = 5  #number of metrics in the data file\n",
    "    tgt_metric =  \"prec\" #\"prec\" \"temp\"    the metric to be modeled\n",
    "    tgt_mod = \"prec_ex01\"  #specifies the metric specific approach for creating data set\n",
    "\n",
    "    #2b. DAY SUMMARY ONE STATION Construct a matrix from most recent to most distant data from the list data by \n",
    "    nstations = 1\n",
    "    ndays_to_target = 3\n",
    "    ndays_col = 30\n",
    "    nmetrics = 6\n",
    "    non_metrics = 1\n",
    "\n",
    "    #Part 1\n",
    "    if part == 1:\n",
    "        #Get data for target station\n",
    "        lst_data = fn_select_data_vectors_pr07(stid_keys,nstations,nhours,nmonths_row,nmonths_col)\n",
    "        fn = f\"lst data done\"\n",
    "        print(fn)\n",
    "        \n",
    "        #Create matrix of data\n",
    "        #Creates a matrix of recent to distant day hours in each row\n",
    "        #Columns are\n",
    "        #Col 0  - month number as string\n",
    "        #Col 1  - day hour as string\n",
    "        #Col 2 - Precip sum last 24 hours\n",
    "        #Col 3 - Precip mean last 24 hours\n",
    "        #Col 4 - Airtemp mean last 24 hours\n",
    "        #Col 5 - Rel Hum max mean last 24 hours\n",
    "        #Col 6 - Rel Hum mean last 24 hours\n",
    "        #Col 7 - Rel Hum min mean last 24 hours\n",
    "        #Cols 8 to 13 are same as 2 to 7 but for previous hour\n",
    "        #Cols 14 to 19 are same as 2 to 7 but for 2 previous hours\n",
    "        mtx_data = fn_create_data_matrix_DAY_SUMMARY(lst_data,tgt_metric,nstations,nmetrics,non_metrics,nmonths_col,ndays_col,ndays_to_target)\n",
    "        fn = f\"mtx data done - DAY SUMMARY ONE STATION\"\n",
    "        print(fn)\n",
    "        print(mtx_data.shape)\n",
    "        \n",
    "        #Save the mtx_data to a folder\n",
    "        print(ipd04)        \n",
    "        rtn_mtx_pqt = pd.DataFrame(mtx_data)\n",
    "        print(rtn_mtx_pqt.shape)\n",
    "        fn = f\"{ipd04}{tgt_metric}_mtx_DAY_SUMMARY_{stid_target}_stations_{nstations}_RowMn_{nmonths_row}_ColDy_{ndays_col}.parquet\"\n",
    "        print(fn)\n",
    "        rtn_mtx_pqt.to_parquet(fn, index=False)\n",
    "        \n",
    "    #End part 1        \n",
    "\n",
    "    #Part 2\n",
    "    if part == 2:\n",
    "        #Get the mtx_data from a folder\n",
    "        #This lets us start at Step 3 for iterative runs\n",
    "        fn = f\"{ipd04}{tgt_metric}_mtx_DAY_SUMMARY_{stid_target}_stations_{nstations}_RowMn_{nmonths_row}_ColDy_{ndays_col}.parquet\"\n",
    "        rtn_mtx_data = pd.read_parquet(fn)\n",
    "        #rtn_mtx_data = rtn_mtx_data_all.head(80000)\n",
    "        print(rtn_mtx_data.shape)\n",
    "        \n",
    "        mtx_data = rtn_mtx_data.to_numpy()\n",
    "        del rtn_mtx_data\n",
    "        \n",
    "        #DAY SUMMARY Create a target variable, select feature variables, create data frame for input into s4 analysis\n",
    "        s4_data = fn_create_DAY_SUMMARY_tgt_matrix_pr07(mtx_data,\"prec\",\"prec_ex02\",1,nmetrics,ndays_col,ndays_to_target)\n",
    "        fn = f\"s4 matrix with target data done\"\n",
    "        print(ipd04)\n",
    "        print(fn)\n",
    "        \n",
    "        rtn_s4_data_pqt = pd.DataFrame(s4_data)\n",
    "        print(rtn_s4_data_pqt.shape)\n",
    "        fn = f\"{ipd04}{tgt_metric}_s4data_DAY_SUMMARY_{stid_target}_stations_{nstations}_RowMn_{nmonths_row}_ColDy_{ndays_col}.parquet\"\n",
    "\n",
    "        print(fn)\n",
    "        rtn_s4_data_pqt.to_parquet(fn, index=False)\n",
    "        \n",
    "    \n",
    "    if part == 3:\n",
    "        #This lets us start at Step 4 for iterative runs DAY SUMMARY\n",
    "        fn = f\"{ipd04}{tgt_metric}_s4data_DAY_SUMMARY_{stid_target}_stations_{nstations}_RowMn_{nmonths_row}_ColDy_{ndays_col}.parquet\"\n",
    "        print(fn)\n",
    "\n",
    "        # Read the Parquet file into a Pandas DataFrame\n",
    "        s4_data = pd.read_parquet(fn)\n",
    "        print(s4_data.shape)\n",
    "        \n",
    "        # Prep s4 data for modeling DAY SUMMARY\n",
    "        # Example matrix (replace this with your actual matrix)\n",
    "        df = pd.DataFrame(s4_data).iloc[1:,] # Convert to DataFrame and drop the first row for binning\n",
    "        print(f\"All records at start {df.shape}\")\n",
    "    \n",
    "        #Drop rows where there has been no rain in total area for past 48 hours\n",
    "        #Choose the df for analysis\n",
    "        dfs = df\n",
    "        #print(f\"Full rain and no rain {dfs.shape}\")\n",
    "\n",
    "        # Drop rows where the first column has missing values\n",
    "        # df = df.dropna(subset=[0])\n",
    "        condition = np.isfinite(dfs.iloc[:,0])\n",
    "        df_subset = pd.DataFrame(dfs[condition])\n",
    "        del dfs\n",
    "        print(f\"First missing values dropped {df_subset.shape}\")\n",
    "\n",
    "        bin_name = \"bin1_5_20\"\n",
    "        #bins = [-float('inf'),0.1,0.5,1,2,3,4,5,float('inf')]\n",
    "        #labels = [0,0.1, 0.5,1,2,3,4,5]\n",
    "        bins = [-float('inf'),1,5,20,float('inf')]\n",
    "        #labels = [0,1,5,20]\n",
    "        labels = [0,1,2,3]\n",
    "        print(bins, labels)\n",
    "\n",
    "        df_transformed = transform_matrix_precip(df_subset, bins, labels)\n",
    "\n",
    "        df_transformed.drop(columns=df_transformed.columns[1],inplace=True)\n",
    "\n",
    "        df_transformed = df_transformed.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "        print(\"Analysis Set:\", df_transformed.shape)\n",
    "        print(df_transformed['tgt_bin'].value_counts(dropna=False).sort_index())\n",
    "\n",
    "        #Convert day and mon from char to numeric\n",
    "        col_name_mon = 1\n",
    "        col_name_hour = 2\n",
    "        column_name_mn = df_transformed.columns[col_name_mon]\n",
    "        column_name_hr = df_transformed.columns[col_name_hour]\n",
    "        df_transformed[column_name_mn] = pd.to_numeric(df_transformed[column_name_mn], errors='coerce') \n",
    "        df_transformed[column_name_hr] = pd.to_numeric(df_transformed[column_name_hr], errors='coerce') \n",
    "        \n",
    "        df_transformed\n",
    "        \n",
    "        \n",
    "        #df_transformedn = df_transformed\n",
    "        train_data, val_data, test_data = split_time_series_data(df_transformed)\n",
    "        print(\"Training Set:\", train_data.shape)\n",
    "        print(train_data['tgt_bin'].value_counts(dropna=False).sort_index())\n",
    "        print(\"Validation Set:\", val_data.shape)\n",
    "        print(val_data['tgt_bin'].value_counts(dropna=False).sort_index())\n",
    "        print(\"Test Set:\", test_data.shape)\n",
    "        print(test_data['tgt_bin'].value_counts(dropna=False).sort_index())\n",
    "\n",
    "        # Define the station ID and output directory\n",
    "        # Export datasets to Parquet and CSV\n",
    "\n",
    "        train_data.to_parquet(f\"{ipd04}{stid_target}_DAYSUM_{bin_name}_train.parquet\", index=False)\n",
    "        val_data.to_parquet(f\"{ipd04}{stid_target}_DAYSUM_{bin_name}_validation.parquet\", index=False)\n",
    "        test_data.to_parquet(f\"{ipd04}{stid_target}_DAYSUM_{bin_name}_test.parquet\", index=False)\n",
    "\n",
    "        # code for saving to csv for testing and checking\n",
    "        #train_data.to_csv(f\"{ipd04}{stid_target}_DAYSUM_train.csv\", index=False)\n",
    "        # val_data.to_csv(f\"{ipd04}{stid_target}_validation.csv\", index=False)\n",
    "        # test_data.to_csv(f\"{ipd04}{stid_target}_test.csv\", index=False)\n",
    "        print(\"Datasets exported successfully.\")\n",
    "\n",
    "        #Code to run at cmd line\n",
    "        #python -m s4model --modelname 72406093721 --trainset C:\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72406093721_train.parquet --valset C:\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72406093721_validation.parquet --testset C:\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72406093721_test.parquet --tabulardata --dependent_variable 0 --epochs 30\n",
    "        #python -m s4model --modelname 72406093721 --modeltype classification --trainset ../data/weathermetrics/74486094789_DAYSUM_bin1_5_20_train.parquet --valset ../data/weathermetrics/74486094789_DAYSUM_bin1_5_20_validation.parquet --testset ../data/weathermetrics/74486094789_DAYSUM_bin1_5_20_test.parquet --tabulardata --dependent_variable tgt_bin --epochs 30\n",
    "        \n",
    "        \n",
    "        \n",
    "    return 1 #,train_data, val_data, test_data #mtx_data #tgt_stid #s4_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jj = stid_loop(\"70200026617\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lst_stx = [\"70200026617\",\"70219026615\",\"70316025624\",\"72202012839\",\"72206013889\",\"72208013880\",\"72214093805\",\"72217003813\",\"72218003820\",\"72223013894\",\"72241012917\",\"72250012919\",\"72251012924\",\"72255012912\",\"72312003870\",\"72417013729\",\"72438093819\",\"72515004725\",\"72519014771\",\"72605014745\",\"72635094860\",\"72638094814\",\"72639094849\",\"72654014936\",\"72659014929\",\"72712014607\",\"72734014847\",\"72792024227\"]\n",
    "lst_stx = [\n",
    "            # \"72206013889\",\n",
    "            \"72209864761\",\n",
    "            # \"99769899999\",\n",
    "            # \"72508654734\",\n",
    "            # \"99727199999\",\n",
    "            # \"74000154793\",\n",
    "           ]  \n",
    "run_part = 3\n",
    "\n",
    "def process_w_yield(my_list, run_part):\n",
    "    for value in my_list:\n",
    "        try:\n",
    "            result = stid_loop(value, run_part)\n",
    "            yield (result)\n",
    "        except Exception as e:\n",
    "            # yield the station id and exception so the caller can see failures\n",
    "            yield (value, e)\n",
    "\n",
    "for result in process_w_yield(lst_stx,run_part):\n",
    "    print(f\"Done with {result}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[3]['tgt_bin'].value_counts(dropna=False).sort_index()\n",
    "result[3]['tgt_bin'][0:3]\n",
    "type(result[3]['tgt_bin'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commands exported to run_models.bat\n"
     ]
    }
   ],
   "source": [
    "runlocation = \"D:/CodeLibrary/Python/weathermetrics/s4model/\"\n",
    "datalocation = \"D:/CodeLibrary/Python/weathermetrics/data/weathermetrics/\"\n",
    "loglocation =  \"D:/CodeLibrary/Python/weathermetrics/results/logs/\"\n",
    "with open(f\"{runlocation}/run_models.bat\", 'w') as f:\n",
    "    for x in lst_stx:\n",
    "        command = (\n",
    "            f\"python -m s4model --modelname {x} --modeltype classification \"\n",
    "            f\"--trainset {datalocation}{x}_DAYSUM_bin1_5_20_train.parquet \"\n",
    "            f\"--valset {datalocation}{x}_DAYSUM_bin1_5_20_validation.parquet \"\n",
    "            f\"--testset {datalocation}{x}_DAYSUM_bin1_5_20_test.parquet \"\n",
    "            f\"--tabulardata --dependent_variable tgt_bin --epochs 100 \"\n",
    "            f\"> {loglocation}{x}_runlog.txt 2>&1\"\n",
    "        )\n",
    "        f.write(command + '\\n')\n",
    "print(\"Commands exported to run_models.bat\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
