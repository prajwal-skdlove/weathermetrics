{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IV Load the data for a station and organize for precipitation model\n",
    "#Check for update\n",
    "import os as os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths\n",
    "#Need to set up based on who is running\n",
    "usr = \"JH\"\n",
    "if usr == \"PK\":\n",
    "    #ipd01 = \"\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\2024\\\\\"\n",
    "    ipd02 = \"D:\\\\CodeLibrary\\\\Python\\\\weathermetrics\\\\data\\\\weathermetrics\\\\\"\n",
    "    ipd03 = \"\"\n",
    "    ipd04 = \"\"\n",
    "    \n",
    "if usr == \"JH\":\n",
    "    #ipd01 = \"\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\2024\\\\\"\n",
    "    ipd02 = \"\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\\"\n",
    "    ipd03 = \"\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\Py_S4\\\\NCEI_data\\\\\"\n",
    "    ipd04 = \"\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\Py_S4\\\\NCEI_parquet_files\\\\\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_select_data_vectors_pr07(stid_keys,nstations,nhours,nmonths_row,nmonths_col):\n",
    "    #Get the matrix of data to input into marix\n",
    "    #The S4 model\n",
    "    # 1. Read in combined data files for multiple stations\n",
    "    # 2. Find the common dates that will be used to align the matrices\n",
    "    #    Keep only the values that fit the common starting date \n",
    "\n",
    "    # 1. Read in data files and get start date v_date... for each\n",
    "    # Check that data exist for all metrics\n",
    "\n",
    "    df_stid = stid_keys\n",
    "    df_stid2 = df_stid.iloc[:, [0]]\n",
    "    df_stid2 = df_stid2.drop_duplicates() # unique()\n",
    "    \n",
    "    n_rows_stid = min(len(df_stid2),nstations)\n",
    "    #print(f\"n rows stid {n_rows_stid}\")\n",
    "    #Matrix in which measures are stored\n",
    "    n_vmt_rows = nhours*30*max(nmonths_row,nmonths_col)\n",
    "    \n",
    "    data_list = []\n",
    "    date_max_list =[]\n",
    "    \n",
    "    #1. Read in data files\n",
    "    for i in range(0, n_rows_stid):\n",
    "    #for i in range(0, 28):\n",
    "        infile = ipd03 + str(df_stid2.iloc[i,0]) #+ \"_model_data_combined.csv\"\n",
    "        print(infile)\n",
    "        \n",
    "        #Eliminate fully blank lines\n",
    "        cleaned_lines = []\n",
    "        with open(infile, 'r', newline='') as infile:\n",
    "            reader = csv.reader(infile)\n",
    "            for row in reader:\n",
    "                # Example: Skip rows that are entirely empty or have only blank 'name'\n",
    "                if any(cell.strip() for cell in row) and (len(row) < 1 or row[0].strip() != ''): # Assuming 'name' is the first column\n",
    "                    cleaned_lines.append(row)\n",
    "        \n",
    "        df_metrics = pd.DataFrame(cleaned_lines[1:], columns=cleaned_lines[0]) # Assuming first row is header\n",
    "\n",
    "        df_metrics['str_hr'] = df_metrics['nHOUR']  #.apply(lambda x: f'{x:02d}')\n",
    "        df_metrics['date_hr'] = pd.to_datetime(df_metrics['Date_YYYYMMDD'] + ' ' + df_metrics['str_hr'])\n",
    "        df_metrics['month_number'] = df_metrics['date_hr'].dt.month\n",
    "        df_metrics['week_number'] = df_metrics['date_hr'].dt.isocalendar().week\n",
    "        df_metrics['stid'] = str(df_stid2.iloc[i,0])\n",
    "        data_list.append(df_metrics)\n",
    "\n",
    "    #2. Align dates across data files\n",
    "    for i in range(0, n_rows_stid):\n",
    "        df = data_list[i]\n",
    "        max_date = max(df['date_hr'])     \n",
    "        date_max_list.append(max_date)\n",
    "    \n",
    "    date_filter = min(date_max_list)\n",
    "\n",
    "    for i in range(0, n_rows_stid):\n",
    "        df = data_list[i]\n",
    "\n",
    "        filtered_df = df[df['date_hr'] < date_filter]\n",
    "        filtered_df = filtered_df.iloc[(len(filtered_df) - n_vmt_rows):len(filtered_df)]\n",
    "        \n",
    "        idx_col = filtered_df.columns\n",
    "        idx_col = str(df_stid2.iloc[i,0]) + '_' + idx_col\n",
    "        filtered_df.columns = idx_col\n",
    "\n",
    "        print(filtered_df.shape)\n",
    "        \n",
    "        data_list[i] = filtered_df\n",
    "        \n",
    "    return data_list[0:n_rows_stid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_create_data_matrix_pr07(lst_data,tgt_metric,nstations,nmetrics,nmonths_col,ndays_col):\n",
    "    #Create a matrix for precipitation data that feeds\n",
    "    #The S4 model\n",
    "    # 1. Get data list and select target metric\n",
    "    #Two choices now\n",
    "    #    - prec\n",
    "    #    - temp\n",
    "    # 2.  Create output and move data into approprate cells\n",
    "\n",
    "    # 1. get data list details and reduce to metrics\n",
    "    data_vectors_list = []\n",
    "    dt_rows = len(lst_data[0])\n",
    "    df00 = []\n",
    "\n",
    "    dt_rows_minus_1 = dt_rows - 1\n",
    "    \n",
    "    if tgt_metric == \"prec\": tgt_cols = [10,2,3,4,5,6,8,9,11,12]\n",
    "    if tgt_metric == \"temp\": tgt_cols = [10,3,2,4,5,6,8,9,11,12]\n",
    "    non_metrics = 4  #These are the non sensor data brought in to analysis (e.g., day of year, month, ...)\n",
    "    \n",
    "    #Reset the indices as all records are ordered by date and hour\n",
    "    #But stations have missing records so not everything aligns\n",
    "    #Need to use the target station (station 0) as the standard and left join th other fields\n",
    "    print(f\"number of stations in data list {len(lst_data)}\")\n",
    "    n_data_files = len(lst_data)-1\n",
    "    \n",
    "    for i in range(n_data_files):\n",
    "        if i == 0:\n",
    "            tdt = lst_data[i].iloc[:,tgt_cols]\n",
    "        if i > 0:\n",
    "            tdt = lst_data[i].iloc[:,tgt_cols[0:6]]\n",
    "        tdt = tdt.iloc[::-1]\n",
    "        tdt.reset_index(drop=True, inplace=True)\n",
    "        first_column_name = tdt.columns[0]\n",
    "        tdt.rename(columns={first_column_name: 'key_date_hr'}, inplace=True)\n",
    "        #fn = f\"tdt data type {tdt['key_date_hr'].dtype}\"\n",
    "        #print(fn)\n",
    "        if tdt['key_date_hr'].dtype == \"datetime64[ns]\":\n",
    "            df00.append(tdt)\n",
    "            fn = f\"1. Done with setting up station -  {i}\"\n",
    "            #print(fn)\n",
    "    \n",
    "    \n",
    "    nstations_w_data = len(df00)\n",
    "    \n",
    "    #Take the list of station data, df00, and paste side by side\n",
    "    #to produce rs which is a large matrix of data for all stations \n",
    "    join_key = 'key_date_hr'\n",
    "    rs = reduce(lambda left, right: pd.merge(left, right, on=join_key, how='left'), df00)\n",
    "    \n",
    "    #rs has each row being a dayXhour for all stations for the stations full metrics\n",
    "    fn = f\"2. Shape of combined data including all stations\"\n",
    "    print(fn)\n",
    "    print(rs.shape)\n",
    "    \n",
    "\n",
    "    #2. Create output matrix and move data into appropriate cells\n",
    "    #Note: target cells get 9 elements, feature cells get 5 elements\n",
    "    \n",
    "    nmetrics_and_non_metrics = nmetrics + non_metrics\n",
    "    fn = f\"nmetrics_and_non_metrics {nmetrics_and_non_metrics}\"\n",
    "    print(fn)\n",
    "    #OLDndataelements = nmetrics_and_non_metrics + (nstations-1)*(nmetrics)\n",
    "    ndataelements = nmetrics_and_non_metrics + (nstations_w_data-1)*(nmetrics)\n",
    "    fn = f\"ndataelements - stations by metrics {ndataelements}\"\n",
    "    print(fn)\n",
    "    mt_cols = 1 + ndataelements*ndays_col*24\n",
    "    fn = f\"mt_cols 1 + #data elements*ndays_col*24 {mt_cols}\"\n",
    "    print(fn)\n",
    "    mt_rows = len(rs)\n",
    "    \n",
    "    #mt_rows = 170000  #keep here for development until done\n",
    "    mt_full_set = np.empty((mt_rows,mt_cols),np.float16)\n",
    "    \n",
    "    #Create matrix of data\n",
    "    for i in range(0,mt_rows):\n",
    "        mt_full_set[i,range(0,ndataelements)] = rs.iloc[i,range(1,ndataelements+1)].apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    fn = f\"3. Full mt setup with initial data for the dayXhour only\"\n",
    "    print(fn)  \n",
    "    print(mt_full_set.shape)\n",
    "    \n",
    "    #Add additional column sets\n",
    "    lpi = mt_rows - ndays_col*24 - 1  #number of rows in rs matrix - the data elements in a column... \n",
    "                                      #ensures all records have data\n",
    "    \n",
    "    #limit of rows in data set so all matrix rows have data\n",
    "    lpj = ndays_col*24 -1   #number of colum sets to fill, each with 104 columns\n",
    "        \n",
    "    \n",
    "    #The row I am operating on\n",
    "    for i in range(0,lpi):\n",
    "        #The columns I am going to fill\n",
    "        for j in range(0,lpj):\n",
    "        \n",
    "            row_get = i + j + 1              #For every new column in current row we have to go down one more row\n",
    "            col_get_start = 0                #data retrieved starting column 1\n",
    "            col_get_end = ndataelements - 1\n",
    "            \n",
    "            row_put = i\n",
    "            col_put_start = ndataelements*(row_get - row_put)\n",
    "            col_put_end = col_put_start + ndataelements - 1\n",
    "            \n",
    "            mt_full_set[row_put,col_put_start:col_put_end] = mt_full_set[row_get,col_get_start:col_get_end]\n",
    "    \n",
    "    fn = f\"4. mt_full_set with ndays of data added to the columns\"\n",
    "    print(fn)  \n",
    "    print(mt_full_set.shape)\n",
    "    \n",
    "    return nstations_w_data, mt_full_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_create_data_matrix_DAY_SUMMARY(lst_data,tgt_metric,nstations,nmetrics,non_metrics,nmonths_col,ndays_col,ndays_to_target):\n",
    "    #Create a matrix for precipitation data that feeds\n",
    "    #The S4 model\n",
    "    # 1. Get data list and select target metric\n",
    "    #Two choices now\n",
    "    #    - prec\n",
    "    #    - temp\n",
    "    # 2.  Create output and move data into approprate cells\n",
    "\n",
    "    # 1. get data list details and reduce to metrics\n",
    "    data_vectors_list = []\n",
    "    dt_rows = len(lst_data[0])\n",
    "    df00 = []\n",
    "\n",
    "    dt_rows_minus_1 = dt_rows - 1\n",
    "    \n",
    "    if tgt_metric == \"prec\": \n",
    "        tgt_cols = [13,10,2,3,4,5,6]\n",
    "        ncol_names = ['station','key_date_hr','precip','airtemp','maxRH','relhum','minRH']\n",
    "    if tgt_metric == \"temp\": \n",
    "        tgt_cols = [13,10,3,2,4,5,6]\n",
    "        ncol_names = ['station','key_date_hr','airtemp','precip','maxRH','relhum','minRH']\n",
    "        \n",
    "    #Reset the indices as all records are ordered by date and hour\n",
    "    #But stations have missing records so not everything aligns\n",
    "    #Need to use the target station (station 0) as the standard and left join th other fields\n",
    "    print(f\"number of stations in data list {len(lst_data)}\")\n",
    "    n_data_files = min(len(lst_data)-1,nstations)\n",
    "    \n",
    "    for i in range(n_data_files):\n",
    "        \n",
    "        tdt = lst_data[i].iloc[:,tgt_cols]\n",
    "        tdt = tdt.iloc[::-1]  #Reverses row order\n",
    "        tdt.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        \n",
    "        for j in range(len(tgt_cols)):\n",
    "            curr_column_name = tdt.columns[j]\n",
    "            tdt.rename(columns={curr_column_name: ncol_names[j]}, inplace=True)\n",
    "        \n",
    "        if tdt['key_date_hr'].dtype == \"datetime64[ns]\":\n",
    "            \n",
    "            tdt = tdt.set_index('key_date_hr')\n",
    "            tdt['precip'] = pd.to_numeric(tdt['precip'],errors='coerce')\n",
    "            tdt['precip'] = tdt['precip'].fillna(0)\n",
    "            tdt['precip_24hr_msum'] = tdt['precip'].rolling(window='24h').sum()\n",
    "            tdt['precip_24hr_mav'] = tdt['precip'].rolling(window='24h',min_periods=6).mean()\n",
    "            \n",
    "            tdt['airtemp'] = pd.to_numeric(tdt['airtemp'],errors='coerce')\n",
    "            tdt['airtemp_24hr_mav'] = tdt['airtemp'].rolling(window='24h',min_periods=6).mean()\n",
    "            \n",
    "            tdt['maxRH'] = pd.to_numeric(tdt['maxRH'],errors='coerce')\n",
    "            tdt['maxRH_24hr_mav'] = tdt['maxRH'].rolling(window='24h',min_periods=6).mean()\n",
    "    \n",
    "            tdt['relhum'] = pd.to_numeric(tdt['relhum'],errors='coerce')\n",
    "            tdt['relhum_24hr_mav'] = tdt['relhum'].rolling(window='24h',min_periods=6).mean()\n",
    "            \n",
    "            tdt['minRH'] = pd.to_numeric(tdt['minRH'],errors='coerce')\n",
    "            tdt['minRH_24hr_mav'] = tdt['minRH'].rolling(window='24h',min_periods=6).mean()        \n",
    "            \n",
    "            new_column_names = []\n",
    "            stri = str(i)\n",
    "            for n, col in enumerate(tdt.columns):\n",
    "                if i < 10: \n",
    "                    prefix = f\"st0{stri}_\"\n",
    "                    new_column_names.append(prefix + col)\n",
    "                \n",
    "                if i > 9: \n",
    "                    prefix = f\"st{stri}_\"\n",
    "                    new_column_names.append(prefix + col)\n",
    "\n",
    "            \n",
    "            #print(new_column_names)    \n",
    "            tdt.columns = new_column_names\n",
    "            #print(tdt)\n",
    "            df00.append(tdt)\n",
    "\n",
    "    df_valid_dates = pd.to_datetime(df00[0].index)\n",
    "    print(f\"Number of valid rows: {len(df_valid_dates)}\")\n",
    "    \n",
    "    #Use a list comprehension to reindex each DataFrame, filling missing values with NaN.\n",
    "    df01 = [tdt.reindex(df_valid_dates) for tdt in df00]\n",
    "    df02 = [tdt.iloc[:,[0,6,7,8,9,10,11]] for tdt in df01]\n",
    "    \n",
    "    #Concatenate the reindexed DataFrames into a single DataFrame if needed.\n",
    "    rs = pd.concat(df02, axis=1)\n",
    "                \n",
    "    nstations_w_data = len(df02)\n",
    "    print(f\"Number of stations with data: {nstations_w_data}\")\n",
    "\n",
    "    fn = f\"2. Shape of combined data including all stations\"\n",
    "    print(fn)\n",
    "    print(rs.shape)\n",
    "    \n",
    "    #2. Create output matrix and move data into appropriate cells\n",
    "    #Note: target cells get 9 elements, feature cells get 5 elements\n",
    "    \n",
    "    nmetrics_and_non_metrics = nmetrics + non_metrics\n",
    "    fn = f\"nmetrics - {nmetrics_and_non_metrics}\"\n",
    "    print(fn)\n",
    "    \n",
    "    ndataelements = (nstations_w_data)*(nmetrics)\n",
    "    fn = f\"ndataelements - stations by metrics {ndataelements}\"\n",
    "    print(fn)\n",
    "    \n",
    "    rows_to_cols_sets = (ndays_col+ndays_to_target)*24\n",
    "    mt_cols = ndataelements*rows_to_cols_sets   #add two columns for the month and hour for the first set of data\n",
    "    fn = f\"mt_cols - ndataelements*(ndays_col+ndays_to_target)*24 {mt_cols}\"\n",
    "    print(fn)\n",
    "\n",
    "    #Identify the rows down to start\n",
    "\n",
    "    rows_to_include = len(df_valid_dates) - rows_to_cols_sets - 1\n",
    "    fn = f\"rows to include - len(df_valid_dates) - rows_to_cols_sets - 1 {rows_to_include}\"\n",
    "    print(fn)\n",
    "     \n",
    "    mt_full_set = np.empty((rows_to_include,mt_cols),np.float16)\n",
    "    fn = f\"mt_shape - {mt_full_set.shape}\"\n",
    "    print(fn)\n",
    "\n",
    "    ar_str_mon = np.array(rs.index.strftime('%m')).reshape(-1,1)\n",
    "    ar_str_mon = ar_str_mon[0:rows_to_include]\n",
    "    ar_str_hr = np.array(rs.index.strftime('%H')).reshape(-1,1)\n",
    "    ar_str_hr = ar_str_hr[0:rows_to_include]\n",
    "    rs.drop('st00_station', axis=1, inplace=True)\n",
    "    \n",
    "    #Create matrix of data\n",
    "    #rows_to_include = 1000  #number of col_rows to be copied each time from column A and placed in col B    \n",
    "    mt_loops = rows_to_cols_sets     #number of loops over which to copy each group of rows\n",
    "    \n",
    "    j = 0  #Loop across stations, set to operate only on the target station\n",
    "    for i in range(0,mt_loops):\n",
    "        mt_full_set[j:rows_to_include,range(i*nmetrics,i*nmetrics + nmetrics)] = rs.iloc[i:(i+rows_to_include),0:(nmetrics+1)].apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    #Add date month and hour info and delete data too close\n",
    "    mt_full_set_rtn = np.concatenate((ar_str_mon,ar_str_hr,mt_full_set),axis=1)\n",
    "    \n",
    "    fn = f\"3. Full mt setup with initial data for the dayXhour only\"\n",
    "    print(fn)  \n",
    "    print(mt_full_set_rtn.shape)\n",
    "    \n",
    "    return mt_full_set_rtn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_create_tgt_matrix_pr07(mtx_data,tgt_metric,tgt_mod,nstations,nmetrics,ndays_col):\n",
    "    \n",
    "    mtx01 = mtx_data #.iloc[0:1000,:]\n",
    "    nrows = mtx01.shape[0]\n",
    "    ncols = mtx01.shape[1]\n",
    "    \n",
    "       \n",
    "    if tgt_metric == \"prec\" and tgt_mod == \"prec_ex01\": \n",
    "        rtn_matrix = fn_prec_ex01(mtx01,nstations,nmetrics,ndays_col)\n",
    "    \n",
    "    \n",
    "    #return the matrix for analysis\n",
    "    return rtn_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_create_DAY_SUMMARY_tgt_matrix_pr07(mtx_data,tgt_metric,tgt_mod,nstations,nmetrics,ndays_col,ndays_to_target):\n",
    "    \n",
    "    mtx01 = mtx_data  #.iloc[0:100000,:]\n",
    "    nrows = mtx01.shape[0]\n",
    "    ncols = mtx01.shape[1]\n",
    "    \n",
    "    if tgt_metric == \"prec\" and tgt_mod == \"prec_ex02\": \n",
    "        rtn_matrix = fn_prec_ex02(mtx01,1,nmetrics,ndays_col,ndays_to_target)\n",
    "    \n",
    "    return rtn_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_prec_ex01(mtx01,nstations,nmetrics,dayscol):\n",
    "    \n",
    "    df = pd.DataFrame(mtx01).reset_index(drop=True)\n",
    "    ncols_orig = df.shape[1]\n",
    "    print(f\"Number of mxt columns {ncols_orig}\")\n",
    "    \n",
    "    #Data come in as follows\n",
    "    #target station \n",
    "    #      - number of metrics (nmetrics)\n",
    "    #      - date columns (4)\n",
    "    #feat station 1  - number of metrics (nmetrics)\n",
    "    #feat station ...  - number of metrics (nmetrics)\n",
    "    #feat station nstations  - number of metrics (nmetrics)\n",
    "    #total columns = (nmetrics + date columns) + (nstations+nmetrics)\n",
    "    \n",
    "    offset = 4  #additional fields for target station\n",
    "    data_columns_start = (nmetrics + offset) + (nstations - 1)*nmetrics\n",
    "    print(f\"data_columns_start {data_columns_start}\")\n",
    "    \n",
    "    tot_per = dayscol*24  #days by 24 hours\n",
    "    \n",
    "    for iper in range(1,tot_per,1):\n",
    "\n",
    "        vc_start1 = np.array([(iper*nstations*(nmetrics) + (iper*offset))])\n",
    "        vc_start2 = vc_start1 + nmetrics + offset   #first set has day/month/week/hr info\n",
    "        vc_end = vc_start2 + (nstations-1)*nmetrics\n",
    "        vc_s2_end = np.arange(vc_start2.item(),vc_end.item(),nmetrics)\n",
    "        vc_num = np.concatenate([vc_start1,vc_s2_end])\n",
    "        \n",
    "        nm1 = 'avg'+ str(iper)\n",
    "        pds1 = df.iloc[:,vc_num].mean(axis=1,skipna=True)\n",
    "        df[nm1] = pds1\n",
    "        \n",
    "        nm2 = 'cnt_ge_zero'+ str(iper)\n",
    "        pds2 = df.iloc[:,vc_num].apply(cnt_ge0,axis=1)\n",
    "        df[nm2] = pds2\n",
    "        \n",
    "        nm3 = 'cnt_gt_zero'+ str(iper)\n",
    "        pds3 = df.iloc[:,vc_num].apply(cnt_gt0,axis=1)\n",
    "        df[nm3] = pds3\n",
    "               \n",
    "    ncols_final = df.shape[1]\n",
    "    print(f\"Final shape {ncols_final}\")\n",
    "    new_cols = range(ncols_orig,ncols_final,1)\n",
    "    print(f\"New columns {new_cols}\")\n",
    "    old_cols = range(data_columns_start,ncols_orig-1,1)\n",
    "    print(f\"Old columns {old_cols}\")\n",
    "    vc_keep = [0]+list(new_cols) + list(old_cols)\n",
    "    print(f\"Final columns {ncols_final}\")\n",
    "    \n",
    "    df.rename(columns={'0': 'tgt_precip', '5': 'day_of_year'\n",
    "                       , '6': 'hour_of_day'\n",
    "                       , '7': 'month_of_year'\n",
    "                       , '8': 'week_of_year'}, inplace=True)\n",
    "    \n",
    "    df_out = pd.concat([df.iloc[:,[0,5,7,8,6]],df.iloc[:,new_cols],df.iloc[:,old_cols]],axis=1)\n",
    "    \n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_prec_ex02(mtx01,nstations,nmetrics,dayscol,ndays_to_target):\n",
    "    \n",
    "    df = pd.DataFrame(mtx01).reset_index(drop=True)\n",
    "    ncols_orig = df.shape[1]\n",
    "    print(f\"Number of mxt columns {ncols_orig}\")\n",
    "    \n",
    "    #Data come in as follows\n",
    "    #target station\n",
    "    #      - date columns (2) Month Hour\n",
    "    #      - number of metrics (nmetrics)\n",
    "    #      - number of sets of day by hour (dayscol*24)\n",
    "    \n",
    "\n",
    "    data_columns_start = 2\n",
    "    tgt_column = 3   # precip sum   \n",
    "    tot_per = dayscol*24  #days by 24 hours\n",
    "    \n",
    "    cols = df.columns.tolist()\n",
    "    ncols = [cols[2],cols[0],cols[1]] + cols[3:]\n",
    "    #df_out = df.reindex(columns)\n",
    "    \n",
    "    dt_start = 3 + ndays_to_target*24\n",
    "    vc = [2,0,1] + list(range(dt_start,4754))\n",
    "    df_out = df.iloc[:,vc]\n",
    "    \n",
    "    df_out.rename(columns={'2': 'tgt_precip'\n",
    "                       , '1': 'hour_of_day'\n",
    "                       , '0': 'month_of_year'}, inplace=True)\n",
    "                       \n",
    "        \n",
    "    return df_out #new_order_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions to identify stations in network that have rain sensor and rain\n",
    "def cnt_ge0(row):\n",
    "        cnt_gt_0 = sum(row >= 0)\n",
    "        return cnt_gt_0\n",
    "\n",
    "def cnt_gt0(row):\n",
    "        cnt_gt_0 = sum(row > 0)\n",
    "        return cnt_gt_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_get_nearby_stations(stid_target, target_file,in_wd):\n",
    "\n",
    "    dt01 = target_file[target_file['tgt_stid']==stid_target]\n",
    "    dt01 = dt01[['feat_stid','dist_km']]\n",
    "    vc_stid = dt01['feat_stid'].to_numpy()\n",
    "    \n",
    "    vc_stid = np.insert(vc_stid,0,stid_target)\n",
    "    \n",
    "    #Need to create more combined data sets\n",
    "    lst_files = os.listdir(in_wd)\n",
    "    df_files = pd.DataFrame(lst_files,columns=[\"files\"])\n",
    "    df_files['stid'] = df_files['files'].str[:11]\n",
    "    df_files['istarget'] = df_files['stid'].str.contains(stid_target,case=False)\n",
    "    df_files['file_type'] = df_files['files'].str[23:31]\n",
    "    df_files['isintarget'] = df_files['stid'].isin(vc_stid)\n",
    "    \n",
    "    df_files = df_files[df_files['file_type']==\"combined\"]\n",
    "    df_files = df_files[df_files['isintarget']==True]\n",
    "    \n",
    "    mrg_df = pd.merge(df_files, dt01,how='left',left_on='stid',right_on='feat_stid')\n",
    "    mrg_df = mrg_df.drop(columns=['feat_stid','isintarget','file_type'])\n",
    "    mrg_df = mrg_df.sort_values(by='dist_km',na_position='first')\n",
    "    mrg_df = mrg_df.reset_index(drop=True)\n",
    "    mrg_df.columns = ['fnames','st_id','is_target_station','dist_km']\n",
    "    \n",
    "    return mrg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_create_data_sets(tgt_stid):\n",
    "    \n",
    "    print(tgt_stid)\n",
    "    \n",
    "    #Select the target station for which predictions will be made\n",
    "    dt_trg_set2 = ipd02 + \"..\\\\Station_Pairs_LE_100km_Info.csv\"\n",
    "    dt_analysis_set2 = pd.read_csv(dt_trg_set2)\n",
    "\n",
    "    #Nearby stations within 100km\n",
    "    #dt_analysis_set_keys = fn_get_nearby_stations(tgt_stid,dt_analysis_set2,ipd03)\n",
    "    \n",
    "    dt_analysis_set_keys = dt_analysis_set2[dt_analysis_set2['tgt_stid'] == tgt_stid]    \n",
    "    print(f\"dt_analysis_set_keys{dt_analysis_set_keys}\")\n",
    "\n",
    "    dt_analysis_set_keys['fpn'] = ipd03 + dt_analysis_set_keys['feat_stid']+\"_model_data_combined.csv\"\n",
    "\n",
    "    dt_analysis_set_keys['file exists'] = dt_analysis_set_keys['fpn'].apply(lambda x: os.path.exists(x))\n",
    "\n",
    "    dt_analysis_set_keys = dt_analysis_set_keys[dt_analysis_set_keys['file exists'] == True]\n",
    "\n",
    "    nstations_in_100km = len(dt_analysis_set_keys) #number of stations within 100 km\n",
    "    num_st_keep = 20   #num of stations to keep for analysis\n",
    "    nstations = min(nstations_in_100km,num_st_keep)\n",
    "    \n",
    "    #Specify parameters governing the creation of data matrix for analysis\n",
    "    #All the available stations which have all three metrics needed for model\n",
    "    #vc_stid = dt_analysis_set_keys[['st_id']].drop_duplicates()\n",
    "\n",
    "    #Organize the data based on the nature of the data so they all conform to the same size matrices\n",
    "    #The metrics included are\n",
    "    #  AT - air temperature in Celcius\n",
    "    #  RH - relative humidity in Percent\n",
    "    #  PR - precipitation in MM\n",
    "\n",
    "    #Put them all in appropriate matrices and create list of metrics\n",
    "    #Data files are of two types\n",
    "    #    -  24 hour * 365 days * n years  (AT, PR)\n",
    "    #    -  365 days * n years (RH)\n",
    "    \n",
    "    #Parameters governing data matrix\n",
    "    stid_target = tgt_stid\n",
    "    stid_keys = dt_analysis_set_keys\n",
    "\n",
    "    #print(f\"st id keys{dt_analysis_set_keys}\")\n",
    "    \n",
    "    ipd = ipd02\n",
    "    nhours = 24\n",
    "    ndays = 365\n",
    "    nmonths_row = 240  #total months on rows of data\n",
    "    nmonths_col = 2 #governs the number of months predicting that are needed\n",
    "    ndays_col = 3  #governs the number of days predicting that are needed\n",
    "    nmetrics = 5  #number of metrics in the data file\n",
    "    tgt_metric =  \"prec\" #\"prec\" \"temp\"    the metric to be modeled\n",
    "    tgt_mod = \"prec_ex01\"  #specifies the metric specific approach for creating data set\n",
    "    \n",
    "    #1. Get the relevant station data and return in a list\n",
    "    lst_data = fn_select_data_vectors_pr07(stid_keys,nstations,nhours,nmonths_row,nmonths_col)\n",
    "    fn = f\"std id keys {stid_keys}\"\n",
    "    print(fn)\n",
    "    \n",
    "    #2. Construct a matrix from most recent to most distant data from the list data\n",
    "    nstations_w_data, mtx_data = fn_create_data_matrix_pr07(lst_data,tgt_metric,nstations,nmetrics,nmonths_col,ndays_col)\n",
    "    fn = f\"mtx data done\"\n",
    "    print(fn)\n",
    "    \n",
    "    #Save the mtx_data to a folder\n",
    "    #This lets us start at Step 3 for subsequent runs\n",
    "    print(ipd04)\n",
    "    rtn_mtx_pqt = pd.DataFrame(mtx_data)\n",
    "    print(rtn_mtx_pqt.shape)\n",
    "    fn = f\"{ipd04}{tgt_metric}_mtx_{stid_target}_stations_{nstations_w_data}_RowMn_{nmonths_row}_ColDy_{ndays_col}.parquet\"\n",
    "    print(fn)\n",
    "    rtn_mtx_pqt.to_parquet(fn, index=False)    \n",
    "    \n",
    "    #3.  Create a target variable, select feature variables, create data frame for input into s4 analysis\n",
    "    s4_data = fn_create_tgt_matrix_pr07(mtx_data,\"prec\",\"prec_ex01\",nstations_w_data,nmetrics,ndays_col)\n",
    "    \n",
    "    print(f\"Shape of s4_data {s4_data.shape}\")\n",
    "    fn = f\"s4 data done\"\n",
    "    print(fn)\n",
    "    \n",
    "    #4. Save the s4_data to a folder\n",
    "    #This lets us start at Step 4 for iterative runs\n",
    "\n",
    "    print(ipd04)\n",
    "    rtn_s4_data_pqt = pd.DataFrame(s4_data)\n",
    "    print(rtn_s4_data_pqt.shape)\n",
    "    fn = f\"{ipd04}{tgt_metric}_s4data_{stid_target}_stations_{nstations_w_data}_RowMn_{nmonths_row}_ColDy_{ndays_col}.parquet\"\n",
    "    print(fn)\n",
    "    rtn_s4_data_pqt.to_parquet(fn, index=False)    \n",
    "    \n",
    "    # Prep s4 data for modeling\n",
    "    # Example matrix (replace this with your actual matrix)\n",
    "    df = pd.DataFrame(s4_data).iloc[1:,] # Convert to DataFrame and drop the first row for binning\n",
    "    print(f\"All records at start {df.shape}\")\n",
    "    ##Only keep records w rain\n",
    "    #df = df[df.iloc[:,0]>0]\n",
    "    #print(df.shape)\n",
    "\n",
    "    #Drop rows where there has been no rain in total area for past 48 hours\n",
    "    ndays_check_for_rain = ndays_col\n",
    "    range_of_data = ndays_check_for_rain*ndays_col*(24) + ndays_col\n",
    "    vc_num = [x for x in range(5,range_of_data,3)]\n",
    "\n",
    "    nm1 = 'sum_avg'\n",
    "    pds1 = df.iloc[:,vc_num].sum(axis=1,skipna=True)\n",
    "    df_area_non_zero = df[pds1 > 0]\n",
    "    print(f\"Records with some rain in last {ndays_check_for_rain} days {df_area_non_zero.shape}\")\n",
    "\n",
    "    #Separate into dataframes with and withot rain in past n days\n",
    "    df_area_non_zero_yrain = df_area_non_zero[df_area_non_zero.iloc[:,0]>0]\n",
    "    df_area_non_zero_nrain = df_area_non_zero[df_area_non_zero.iloc[:,0]==0]\n",
    "\n",
    "    #sample non-rain to help balance outcomes if needed\n",
    "    df_sample_nrain = df_area_non_zero_nrain  #df_area_non_zero_nrain.sample(n=len(df_area_non_zero_yrain))\n",
    "    df_area_non_zero_ynrain = pd.concat((df_area_non_zero_yrain,df_sample_nrain),axis=0)\n",
    "\n",
    "    #Choose the df for analysis\n",
    "    dfs = df_area_non_zero_ynrain\n",
    "    print(f\"Full rain and no rain {dfs.shape}\")\n",
    "\n",
    "    del df, df_area_non_zero_yrain, df_area_non_zero_nrain, df_area_non_zero_ynrain\n",
    "    \n",
    "    # Drop rows where the first column has missing values\n",
    "    # df = df.dropna(subset=[0])\n",
    "    condition = np.isfinite(dfs.iloc[:,0])\n",
    "    df_subset = pd.DataFrame(dfs[condition])\n",
    "    del dfs\n",
    "    print(df_subset.shape)\n",
    "\n",
    "\n",
    "    bins = [-float('inf'),0.1,0.5,1,2,3,4,5,float('inf')]\n",
    "    labels = [0,0.1, 0.5,1,2,3,4,5]\n",
    "    print(bins, labels)\n",
    "\n",
    "    df_transformed = transform_matrix_precip(df_subset, bins, labels)\n",
    "\n",
    "    df_transformed.drop(columns=df_transformed.columns[1],inplace=True)\n",
    "    df_transformed.shape\n",
    "\n",
    "    df_transformed = df_transformed.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    print(\"Analysis Set:\", df_transformed.shape)\n",
    "\n",
    "    print(df_transformed['tgt_bin'].value_counts(dropna=False).sort_index())\n",
    "\n",
    "    train_data, val_data, test_data = split_time_series_data(df_transformed)\n",
    "    \n",
    "    train_data.to_parquet(f\"{ipd04}{stid_target}_train.parquet\", index=False)\n",
    "    val_data.to_parquet(f\"{ipd04}{stid_target}_validation.parquet\", index=False)\n",
    "    test_data.to_parquet(f\"{ipd04}{stid_target}_test.parquet\", index=False)\n",
    "\n",
    "    # code for saving to csv for testing and checking\n",
    "    # train_data.to_csv(f\"{ipd04}{stid_target}_train.csv\", index=False)\n",
    "    # val_data.to_csv(f\"{ipd04}{stid_target}_validation.csv\", index=False)\n",
    "    # test_data.to_csv(f\"{ipd04}{stid_target}_test.csv\", index=False)\n",
    "    \n",
    "    print(\"Datasets exported successfully.\") \n",
    "    \n",
    "    return 1 #mtx_data  #stid_keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETUP  Select a station using ID to get its related stations\n",
    "\n",
    "#Load df of target stations and those close by\n",
    "#74486094789 is JFK airport\n",
    "#74486454787 is Farmingdale\n",
    "#72502014734 is Newark\n",
    "#72427653859 is Dayton, OH\n",
    "#72551014939 is Lincoln, NE\n",
    "#72793024233 is Seattle, WA\n",
    "#72202012839 is Miami, FL\n",
    "#72202012839 is Miami, FL\n",
    "#72243012960 is Houston, TX\n",
    "#72572024127 is Salt Lake City, UT\n",
    "#72278023183 is Phoenix, AZ\n",
    "\n",
    "#76001399999 is Gen Abelardo Mexico\n",
    "#71413099999 is Spondin Canada\n",
    "#72507554768 is North Adams, US, ME\n",
    "#72614054742 is St Johnsbury, US, VT\n",
    "#72027312981 \n",
    "#72036363872 \n",
    "#74594693786 errors in data files\n",
    "#72027504872\n",
    "#72030553964\n",
    "#72037492825\n",
    "\n",
    "\n",
    "#Select the target station for which predictions will be made\n",
    "dt_trg_set2 = ipd02 + \"..\\\\Station_Pairs_LE_100km_Info.csv\"\n",
    "dt_analysis_set2 = pd.read_csv(dt_trg_set2)\n",
    "#Governs the target station in subsequent modules\n",
    "tgt_stid =  \"72037492825\"\n",
    "\n",
    "#Nearby stations within 100km\n",
    "dt_analysis_set_keys = fn_get_nearby_stations(tgt_stid,dt_analysis_set2,ipd03)\n",
    "\n",
    "nstations_in_100km = len(dt_analysis_set_keys) #number of stations within 100 km\n",
    "num_st_keep = 20   #num of stations to keep for analysis\n",
    "nstations = min(nstations_in_100km,num_st_keep)\n",
    "\n",
    "dt_analysis_set_keys\n",
    "\n",
    "#Specify parameters governing the creation of data matrix for analysis\n",
    "#All the available stations which have all three metrics needed for model\n",
    "#vc_stid = dt_analysis_set_keys[['st_id']].drop_duplicates()\n",
    "\n",
    "#Organize the data based on the nature of the data so they all conform to the same size matrices\n",
    "#The metrics included are\n",
    "#  AT - air temperature in Celcius\n",
    "#  RH - relative humidity in Percent\n",
    "#  PR - precipitation in MM\n",
    "\n",
    "#Put them all in appropriate matrices and create list of metrics\n",
    "#Data files are of two types\n",
    "#    -  24 hour * 365 days * n years  (AT, PR)\n",
    "#    -  365 days * n years (RH)\n",
    "\n",
    "#Parameters governing data matrix\n",
    "stid_target = tgt_stid\n",
    "stid_keys = dt_analysis_set_keys\n",
    "\n",
    "ipd = ipd02\n",
    "nhours = 24\n",
    "ndays = 365\n",
    "nmonths_row = 240  #total months on rows of data\n",
    "nmonths_col = 2 #governs the number of months predicting that are needed\n",
    "ndays_col = 3  #governs the number of days predicting that are needed\n",
    "nmetrics = 5  #number of metrics in the data file\n",
    "tgt_metric =  \"prec\" #\"prec\" \"temp\"    the metric to be modeled\n",
    "tgt_mod = \"prec_ex01\"  #specifies the metric specific approach for creating data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Get the relevant station data and return in a list a list\n",
    "lst_data = fn_select_data_vectors_pr07(stid_keys,nstations,nhours,nmonths_row,nmonths_col)\n",
    "fn = f\"lst data done\"\n",
    "print(fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#2. Construct a matrix from most recent to most distant data from the list data\n",
    "mtx_data = fn_create_data_matrix_pr07(lst_data,tgt_metric,nstations,nmetrics,nmonths_col,ndays_col)\n",
    "fn = f\"mtx data done\"\n",
    "print(fn)\n",
    "\n",
    "#Save the mtx_data to a folder\n",
    "#This lets us start at Step 3 for subsequent runs\n",
    "print(ipd04)\n",
    "rtn_mtx_pqt = pd.DataFrame(mtx_data)\n",
    "print(rtn_mtx_pqt.shape)\n",
    "fn = f\"{ipd04}{tgt_metric}_mtx_{stid_target}_stations_{nstations}_RowMn_{nmonths_row}_ColDy_{ndays_col}.parquet\"\n",
    "print(fn)\n",
    "rtn_mtx_pqt.to_parquet(fn, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.  Read in mtx_data file\n",
    "\n",
    "#Get the mtx_data from a folder\n",
    "#This lets us start at Step 3 for iterative runs\n",
    "fn = f\"{ipd04}{tgt_metric}_mtx_{stid_target}_stations_{nstations}_RowMn_{nmonths_row}_ColDy_{ndays_col}.parquet\"\n",
    "#fn = '\\\\\\\\CXA01\\\\Users\\\\jhugh\\\\Documents\\\\Py_S4\\\\Py_S4_v02_JHH\\\\NCEI_parquet_files\\\\prec_mtx_74486094789_stations_20_RowMn_240_ColDy_7.parquet'\n",
    "\n",
    "# Read the Parquet file into a Pandas DataFrame\n",
    "mtx_data = pd.read_parquet(fn)\n",
    "print(mtx_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.  Create a target variable, select feature variables, create data frame for input into s4 analysis\n",
    "\n",
    "s4_data = fn_create_tgt_matrix_pr07(mtx_data,\"prec\",\"prec_ex01\",nstations,nmetrics,ndays_col)\n",
    "\n",
    "print(f\"Shape of s4_data {s4_data.shape}\")\n",
    "fn = f\"s4 data done\"\n",
    "print(fn)\n",
    "s4_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Save the s4_data to a folder\n",
    "#This lets us start at Step 4 for iterative runs\n",
    "\n",
    "print(ipd04)\n",
    "rtn_s4_data_pqt = pd.DataFrame(s4_data)\n",
    "print(rtn_s4_data_pqt.shape)\n",
    "fn = f\"{ipd04}{tgt_metric}_s4data_{stid_target}_stations_{nstations}_RowMn_{nmonths_row}_ColDy_{ndays_col}.parquet\"\n",
    "print(fn)\n",
    "rtn_s4_data_pqt.to_parquet(fn, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This lets us start at Step 4 for iterative runs\n",
    "fn = f\"{ipd04}{tgt_metric}_s4data_{stid_target}_stations_{nstations}_RowMn_{nmonths_row}_ColDy_{ndays_col}.parquet\"\n",
    "print(fn)\n",
    "\n",
    "# Read the Parquet file into a Pandas DataFrame\n",
    "s4_data = pd.read_parquet(fn)\n",
    "print(s4_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_matrix_precip(df, bins, labels):\n",
    "    \"\"\"\n",
    "    Transforms the given matrix by binning the first column, dropping the first row and column,\n",
    "    and adding the binned column as the first column.\n",
    "\n",
    "    Parameters:\n",
    "        matrix (pd.DataFrame): The input matrix.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The transformed matrix.\n",
    "    \"\"\"\n",
    "    # Extract the first column (dependent variable)\n",
    "    firstcolumn = df.iloc[:, 0].astype('float32')\n",
    "    \n",
    "    # Bin the first column\n",
    "    binned_column = pd.cut(firstcolumn, bins=bins, labels=labels)\n",
    "    \n",
    "    # Add the binned column as the first column\n",
    "    df.insert(loc=0, column='tgt_bin', value=binned_column.iloc[0:].values)\n",
    "    \n",
    "    # Drop columns 3 the first first column\n",
    "    #df = df.drop(columns=df.columns[[3]])\n",
    "\n",
    "    # Return the final matrix\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep s4 data for modeling\n",
    "# Example matrix (replace this with your actual matrix)\n",
    "df = pd.DataFrame(s4_data).iloc[1:,] # Convert to DataFrame and drop the first row for binning\n",
    "print(f\"All records at start {df.shape}\")\n",
    "##Only keep records w rain\n",
    "#df = df[df.iloc[:,0]>0]\n",
    "#print(df.shape)\n",
    "\n",
    "#Drop rows where there has been no rain in total area for past 48 hours\n",
    "ndays_check_for_rain = ndays_col\n",
    "range_of_data = ndays_check_for_rain*ndays_col*(24) + ndays_col\n",
    "vc_num = [x for x in range(5,range_of_data,3)]\n",
    "\n",
    "nm1 = 'sum_avg'\n",
    "pds1 = df.iloc[:,vc_num].sum(axis=1,skipna=True)\n",
    "df_area_non_zero = df[pds1 > 0]\n",
    "print(f\"Records with some rain in last {ndays_check_for_rain} days {df_area_non_zero.shape}\")\n",
    "\n",
    "#Separate into dataframes with and withot rain in past n days\n",
    "df_area_non_zero_yrain = df_area_non_zero[df_area_non_zero.iloc[:,0]>0]\n",
    "df_area_non_zero_nrain = df_area_non_zero[df_area_non_zero.iloc[:,0]==0]\n",
    "\n",
    "#sample non-rain to help balance outcomes if needed\n",
    "df_sample_nrain = df_area_non_zero_nrain  #df_area_non_zero_nrain.sample(n=len(df_area_non_zero_yrain))\n",
    "df_area_non_zero_ynrain = pd.concat((df_area_non_zero_yrain,df_sample_nrain),axis=0)\n",
    "\n",
    "#Choose the df for analysis\n",
    "dfs = df_area_non_zero_ynrain\n",
    "print(f\"Full rain and no rain {dfs.shape}\")\n",
    "\n",
    "del df, df_area_non_zero_yrain, df_area_non_zero_nrain, df_area_non_zero_ynrain\n",
    "  \n",
    "# Drop rows where the first column has missing values\n",
    "# df = df.dropna(subset=[0])\n",
    "condition = np.isfinite(dfs.iloc[:,0])\n",
    "df_subset = pd.DataFrame(dfs[condition])\n",
    "del dfs\n",
    "print(df_subset.shape)\n",
    "\n",
    "\n",
    "bins = [-float('inf'),0.1,0.5,1,2,3,4,5,float('inf')]\n",
    "labels = [0,0.1, 0.5,1,2,3,4,5]\n",
    "print(bins, labels)\n",
    "\n",
    "df_transformed = transform_matrix_precip(df_subset, bins, labels)\n",
    "\n",
    "df_transformed.drop(columns=df_transformed.columns[1],inplace=True)\n",
    "df_transformed.shape\n",
    "\n",
    "df_transformed = df_transformed.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(\"Analysis Set:\", df_transformed.shape)\n",
    "\n",
    "print(df_transformed['tgt_bin'].value_counts(dropna=False).sort_index())\n",
    "\n",
    "df_transformed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#df_transformedn = df_transformed\\ntrain_data, val_data, test_data = split_time_series_data(df_transformed)\\nprint(\"Training Set:\", train_data.shape)\\nprint(train_data[\\'tgt_bin\\'].value_counts(dropna=False).sort_index())\\nprint(\"Validation Set:\", val_data.shape)\\nprint(val_data[\\'tgt_bin\\'].value_counts(dropna=False).sort_index())\\nprint(\"Test Set:\", test_data.shape)\\nprint(test_data[\\'tgt_bin\\'].value_counts(dropna=False).sort_index())\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_time_series_data(df, train_ratio=0.6, val_ratio=0.2, test_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Splits a time series dataset into training, validation, and test sets.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input time series dataset, ordered in descending dates.\n",
    "        train_ratio (float): The proportion of data to use for training.\n",
    "        val_ratio (float): The proportion of data to use for validation.\n",
    "        test_ratio (float): The proportion of data to use for testing.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing three DataFrames (train, validation, test).\n",
    "    \"\"\"\n",
    "    assert train_ratio + val_ratio + test_ratio == 1.0, \"Ratios must sum to 1.0\"\n",
    "\n",
    "    # Calculate split indices\n",
    "    n = len(df)\n",
    "    test_end = int(n * test_ratio)    \n",
    "    val_end = test_end + int(n * val_ratio)\n",
    "    \n",
    "\n",
    "    # Split the data\n",
    "    test_data = df.iloc[:test_end].sort_index(ascending=False)\n",
    "    val_data = df.iloc[test_end:val_end].sort_index(ascending=False)\n",
    "    train_data = df.iloc[val_end:].sort_index(ascending=False)\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "'''\n",
    "#df_transformedn = df_transformed\n",
    "train_data, val_data, test_data = split_time_series_data(df_transformed)\n",
    "print(\"Training Set:\", train_data.shape)\n",
    "print(train_data['tgt_bin'].value_counts(dropna=False).sort_index())\n",
    "print(\"Validation Set:\", val_data.shape)\n",
    "print(val_data['tgt_bin'].value_counts(dropna=False).sort_index())\n",
    "print(\"Test Set:\", test_data.shape)\n",
    "print(test_data['tgt_bin'].value_counts(dropna=False).sort_index())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the station ID and output directory\n",
    "# Export datasets to Parquet and CSV\n",
    "\n",
    "train_data.to_parquet(f\"{ipd04}{stid_target}_train.parquet\", index=False)\n",
    "val_data.to_parquet(f\"{ipd04}{stid_target}_validation.parquet\", index=False)\n",
    "test_data.to_parquet(f\"{ipd04}{stid_target}_test.parquet\", index=False)\n",
    "\n",
    "# code for saving to csv for testing and checking\n",
    "train_data.to_csv(f\"{ipd04}{stid_target}_train.csv\", index=False)\n",
    "# val_data.to_csv(f\"{ipd04}{stid_target}_validation.csv\", index=False)\n",
    "# test_data.to_csv(f\"{ipd04}{stid_target}_test.csv\", index=False)\n",
    "\n",
    "print(\"Datasets exported successfully.\")\n",
    "\n",
    "#python -m s4model --modelname 72406093721 --trainset C:\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72406093721_train.parquet --valset C:\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72406093721_validation.parquet --testset C:\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72406093721_test.parquet --tabulardata --dependent_variable 0 --epochs 30\n",
    "#python -m s4model --modelname 72406093721 --trainset C:\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72406093721_train.parquet --valset C:\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72406093721_validation.parquet --testset C:\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72406093721_test.parquet --tabulardata --dependent_variable 0 --epochs 30\n",
    "\n",
    "\n",
    "# python -m charts --df ../results/72508014740_Test_results_20250424_104121PM.csv --actual 0 --predicted Predicted \n",
    "# python -m charts --df ../results/72508014740_Test_results_20250427_100132PM.csv --actual 0 --predicted Predicted \n",
    "# python -m charts --df ../results/72508014740_Test_results_20250428_091259PM.csv --actual 0 --predicted Predicted \n",
    "# python -m charts --df ../results/72508014740_Test_results_20250428_094033PM.csv --actual 0 --predicted Predicted \n",
    "# python -m charts --df ../results/72508014740_Test_results_20250429_091538PM.csv --actual 0 --predicted Predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to run at cmd line\n",
    "#python -m s4model --modelname 72406093721 --trainset C:\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72406093721_train.parquet --valset C:\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72406093721_validation.parquet --testset C:\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72406093721_test.parquet --tabulardata --dependent_variable 0 --epochs 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DAY SUMMARY control\n",
    "def stid_loop(tgt_stid):\n",
    "\n",
    "    #Select the target station for which predictions will be made\n",
    "    #tgt_stid =  instid #\"72502014734\" #instid    # \"72502014734\"  large file \"74788012810\"\n",
    "    save_files = 1   #1 means save them and 0 means do not save them\n",
    "    run_files = 1    #1 means do it and 0 means do not do it\n",
    "    print(tgt_stid)\n",
    "    #Nearby stations within 100km\n",
    "    dt_trg_set2 = ipd02 + \"..\\\\Station_Pairs_LE_100km_Info.csv\"\n",
    "    dt_analysis_set2 = pd.read_csv(dt_trg_set2)\n",
    "\n",
    "    dt_analysis_set_keys = fn_get_nearby_stations(tgt_stid,dt_analysis_set2,ipd03)\n",
    "    nstations_in_100km = len(dt_analysis_set_keys) #number of stations within 100 km\n",
    "    num_st_keep = 20   #num of stations to keep for analysis\n",
    "    nstations = min(nstations_in_100km,num_st_keep)\n",
    "    \n",
    "    #dt_analysis_set_keys\n",
    "\n",
    "    #Specify parameters governing the creation of data matrix for analysis\n",
    "    #All the available stations which have all three metrics needed for model\n",
    "    #vc_stid = dt_analysis_set_keys[['st_id']].drop_duplicates()\n",
    "\n",
    "    #Organize the data based on the nature of the data so they all conform to the same size matrices\n",
    "    #The metrics included are\n",
    "    #  AT - air temperature in Celcius\n",
    "    #  RH - relative humidity in Percent\n",
    "    #  PR - precipitation in MM\n",
    "\n",
    "    #Put them all in appropriate matrices and create list of metrics\n",
    "    #Data files are of two types\n",
    "    #    -  24 hour * 365 days * n years  (AT, PR)\n",
    "    #    -  365 days * n years (RH)\n",
    "\n",
    "    #Parameters governing data matrix\n",
    "    stid_target = tgt_stid\n",
    "    stid_keys = dt_analysis_set_keys\n",
    "\n",
    "    ipd = ipd02\n",
    "    nhours = 24\n",
    "    ndays = 365\n",
    "    nmonths_row = 180 #240  #total months on rows of data\n",
    "    nmonths_col = 2 #governs the number of months predicting that are needed\n",
    "    ndays_col = 3  #governs the number of days predicting that are needed\n",
    "    nmetrics = 5  #number of metrics in the data file\n",
    "    tgt_metric =  \"prec\" #\"prec\" \"temp\"    the metric to be modeled\n",
    "    tgt_mod = \"prec_ex01\"  #specifies the metric specific approach for creating data set\n",
    "\n",
    "    #2b. DAY SUMMARY ONE STATION Construct a matrix from most recent to most distant data from the list data by \n",
    "    nstations = 1\n",
    "    ndays_to_target = 3\n",
    "    ndays_col = 30\n",
    "    ndays_to_target = 3\n",
    "    nmetrics = 6\n",
    "    non_metrics = 1\n",
    "\n",
    "    '''\n",
    "    #1. Get the relevant station data and return in a list a list\n",
    "    if run_files == 1:\n",
    "        lst_data = fn_select_data_vectors_pr07(stid_keys,nstations,nhours,nmonths_row,nmonths_col)\n",
    "        fn = f\"lst data done\"\n",
    "        print(fn)\n",
    "\n",
    "\n",
    "    \n",
    "    if run_files == 1:\n",
    "        mtx_data = fn_create_data_matrix_DAY_SUMMARY(lst_data,tgt_metric,nstations,nmetrics,non_metrics,nmonths_col,ndays_col,ndays_to_target)\n",
    "\n",
    "        fn = f\"mtx data done - DAY SUMMARY ONE STATION\"\n",
    "        print(fn)\n",
    "        print(mtx_data.shape)\n",
    "\n",
    "    if save_files == 0:\n",
    "        #Save the mtx_data to a folder\n",
    "        #This lets us start at Step 3 for subsequent runs\n",
    "        print(ipd04)\n",
    "        rtn_mtx_pqt = pd.DataFrame(mtx_data)\n",
    "        print(rtn_mtx_pqt.shape)\n",
    "        fn = f\"{ipd04}{tgt_metric}_mtx_DAY_SUMMARY_{stid_target}_stations_{nstations}_RowMn_{nmonths_row}_ColDy_{ndays_col}.parquet\"\n",
    "        print(fn)\n",
    "        rtn_mtx_pqt.to_parquet(fn, index=False)\n",
    "\n",
    "        #3b.  DAY SUMMARY Read in mtx_data file\n",
    "\n",
    "        #Get the mtx_data from a folder\n",
    "        #This lets us start at Step 3 for iterative runs\n",
    "        fn = f\"{ipd04}{tgt_metric}_mtx_DAY_SUMMARY_{stid_target}_stations_{nstations}_RowMn_{nmonths_row}_ColDy_{ndays_col}.parquet\"\n",
    "\n",
    "        # Read the Parquet file into a Pandas DataFrame\n",
    "        mtx_data = pd.read_parquet(fn)\n",
    "        print(mtx_data.shape)\n",
    "\n",
    "\n",
    "    #4b.  DAY SUMMARY Create a target variable, select feature variables, create data frame for input into s4 analysis\n",
    "\n",
    "    if run_files == 1:\n",
    "        s4_data = fn_create_DAY_SUMMARY_tgt_matrix_pr07(mtx_data,\"prec\",\"prec_ex02\",1,nmetrics,ndays_col,ndays_to_target)\n",
    "\n",
    "        #print(f\"Shape of s4_data {s4_data.shape}\")\n",
    "        fn = f\"s4 data done\"\n",
    "        print(fn)\n",
    "        s4_data\n",
    "    \n",
    "\n",
    "    #5b. Save the DAY SUMMARY s4_data to a folder\n",
    "    #This lets us start at Step 4 for iterative runs\n",
    "\n",
    "    if save_files == 1:\n",
    "        print(ipd04)\n",
    "        rtn_s4_data_pqt = pd.DataFrame(s4_data)\n",
    "        print(rtn_s4_data_pqt.shape)\n",
    "\n",
    "        fn = f\"{ipd04}{tgt_metric}_s4data_DAY_SUMMARY_{stid_target}_stations_{nstations}_RowMn_{nmonths_row}_ColDy_{ndays_col}.parquet\"\n",
    "\n",
    "        print(fn)\n",
    "        rtn_s4_data_pqt.to_parquet(fn, index=False)\n",
    "    '''\n",
    "    #This lets us start at Step 4 for iterative runs DAY SUMMARY\n",
    "    fn = f\"{ipd04}{tgt_metric}_s4data_DAY_SUMMARY_{stid_target}_stations_{nstations}_RowMn_{nmonths_row}_ColDy_{ndays_col}.parquet\"\n",
    "    print(fn)\n",
    "\n",
    "    # Read the Parquet file into a Pandas DataFrame\n",
    "    s4_data = pd.read_parquet(fn)\n",
    "    print(s4_data.shape)\n",
    "    \n",
    "    # Prep s4 data for modeling DAY SUMMARY\n",
    "    # Example matrix (replace this with your actual matrix)\n",
    "    df = pd.DataFrame(s4_data).iloc[1:,] # Convert to DataFrame and drop the first row for binning\n",
    "    print(f\"All records at start {df.shape}\")\n",
    "    \n",
    "    #Drop rows where there has been no rain in total area for past 48 hours\n",
    "\n",
    "    #Choose the df for analysis\n",
    "    dfs = df\n",
    "    print(f\"Full rain and no rain {dfs.shape}\")\n",
    "\n",
    "    # Drop rows where the first column has missing values\n",
    "    # df = df.dropna(subset=[0])\n",
    "    condition = np.isfinite(dfs.iloc[:,0])\n",
    "    df_subset = pd.DataFrame(dfs[condition])\n",
    "    del dfs\n",
    "    print(f\"first missing values dropped {df_subset.shape}\")\n",
    "\n",
    "\n",
    "    bins = [-float('inf'),0.1,0.5,1,2,3,4,5,float('inf')]\n",
    "    labels = [0,0.1, 0.5,1,2,3,4,5]\n",
    "    print(bins, labels)\n",
    "\n",
    "    df_transformed = transform_matrix_precip(df_subset, bins, labels)\n",
    "\n",
    "    df_transformed.drop(columns=df_transformed.columns[1],inplace=True)\n",
    "    df_transformed.shape\n",
    "\n",
    "    df_transformed = df_transformed.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    print(\"Analysis Set:\", df_transformed.shape)\n",
    "\n",
    "    print(df_transformed['tgt_bin'].value_counts(dropna=False).sort_index())\n",
    "\n",
    "    df_transformed\n",
    "\n",
    "    #df_transformedn = df_transformed\n",
    "    train_data, val_data, test_data = split_time_series_data(df_transformed)\n",
    "    print(\"Training Set:\", train_data.shape)\n",
    "    print(train_data['tgt_bin'].value_counts(dropna=False).sort_index())\n",
    "    print(\"Validation Set:\", val_data.shape)\n",
    "    print(val_data['tgt_bin'].value_counts(dropna=False).sort_index())\n",
    "    print(\"Test Set:\", test_data.shape)\n",
    "    print(test_data['tgt_bin'].value_counts(dropna=False).sort_index())\n",
    "\n",
    "    # Define the station ID and output directory\n",
    "    # Export datasets to Parquet and CSV\n",
    "\n",
    "    train_data.to_parquet(f\"{ipd04}{stid_target}_DAYSUM_train.parquet\", index=False)\n",
    "    val_data.to_parquet(f\"{ipd04}{stid_target}_DAYSUM_validation.parquet\", index=False)\n",
    "    test_data.to_parquet(f\"{ipd04}{stid_target}_DAYSUM_test.parquet\", index=False)\n",
    "\n",
    "    # code for saving to csv for testing and checking\n",
    "    #train_data.to_csv(f\"{ipd04}{stid_target}_DAYSUM_train.csv\", index=False)\n",
    "    # val_data.to_csv(f\"{ipd04}{stid_target}_validation.csv\", index=False)\n",
    "    # test_data.to_csv(f\"{ipd04}{stid_target}_test.csv\", index=False)\n",
    "\n",
    "    print(\"Datasets exported successfully.\")\n",
    "\n",
    "    #Code to run at cmd line\n",
    "    #python -m s4model --modelname 72406093721 --trainset C:\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72406093721_train.parquet --valset C:\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72406093721_validation.parquet --testset C:\\\\Users\\\\jhugh\\\\Documents\\\\HT\\\\NCEI_data\\\\metrics_csv\\\\72406093721_test.parquet --tabulardata --dependent_variable 0 --epochs 30\n",
    "    \n",
    "\n",
    "    return tgt_stid #s4_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70200026617\n",
      "\\\\CXA01\\Users\\jhugh\\Documents\\Py_S4\\NCEI_parquet_files\\prec_s4data_DAY_SUMMARY_70200026617_stations_1_RowMn_180_ColDy_30.parquet\n",
      "(128807, 4682)\n",
      "All records at start (128806, 4682)\n",
      "Full rain and no rain (128806, 4682)\n",
      "first missing values dropped (128806, 4682)\n",
      "[-inf, 0.1, 0.5, 1, 2, 3, 4, 5, inf] [0, 0.1, 0.5, 1, 2, 3, 4, 5]\n",
      "Analysis Set: (128806, 4682)\n",
      "tgt_bin\n",
      "0.0    79722\n",
      "0.1     8134\n",
      "0.5     5699\n",
      "1.0     6858\n",
      "2.0     5425\n",
      "3.0     3649\n",
      "4.0     2817\n",
      "5.0    16502\n",
      "Name: count, dtype: int64\n",
      "Training Set: (77284, 4682)\n",
      "tgt_bin\n",
      "0.0    47944\n",
      "0.1     4922\n",
      "0.5     3443\n",
      "1.0     4082\n",
      "2.0     3194\n",
      "3.0     2164\n",
      "4.0     1689\n",
      "5.0     9846\n",
      "Name: count, dtype: int64\n",
      "Validation Set: (25761, 4682)\n",
      "tgt_bin\n",
      "0.0    15878\n",
      "0.1     1628\n",
      "0.5     1147\n",
      "1.0     1375\n",
      "2.0     1096\n",
      "3.0      750\n",
      "4.0      570\n",
      "5.0     3317\n",
      "Name: count, dtype: int64\n",
      "Test Set: (25761, 4682)\n",
      "tgt_bin\n",
      "0.0    15900\n",
      "0.1     1584\n",
      "0.5     1109\n",
      "1.0     1401\n",
      "2.0     1135\n",
      "3.0      735\n",
      "4.0      558\n",
      "5.0     3339\n",
      "Name: count, dtype: int64\n",
      "Datasets exported successfully.\n",
      "Done with 70200026617\n",
      "70219026615\n",
      "\\\\CXA01\\Users\\jhugh\\Documents\\Py_S4\\NCEI_parquet_files\\prec_s4data_DAY_SUMMARY_70219026615_stations_1_RowMn_180_ColDy_30.parquet\n",
      "(128807, 4682)\n",
      "All records at start (128806, 4682)\n",
      "Full rain and no rain (128806, 4682)\n",
      "first missing values dropped (128806, 4682)\n",
      "[-inf, 0.1, 0.5, 1, 2, 3, 4, 5, inf] [0, 0.1, 0.5, 1, 2, 3, 4, 5]\n",
      "Analysis Set: (128806, 4682)\n",
      "tgt_bin\n",
      "0.0    72015\n",
      "0.1     9159\n",
      "0.5     7338\n",
      "1.0     9919\n",
      "2.0     5506\n",
      "3.0     4281\n",
      "4.0     3485\n",
      "5.0    17103\n",
      "Name: count, dtype: int64\n",
      "Training Set: (77284, 4682)\n",
      "tgt_bin\n",
      "0.0    43196\n",
      "0.1     5500\n",
      "0.5     4365\n",
      "1.0     5978\n",
      "2.0     3272\n",
      "3.0     2562\n",
      "4.0     2094\n",
      "5.0    10317\n",
      "Name: count, dtype: int64\n",
      "Validation Set: (25761, 4682)\n",
      "tgt_bin\n",
      "0.0    14356\n",
      "0.1     1856\n",
      "0.5     1489\n",
      "1.0     1967\n",
      "2.0     1137\n",
      "3.0      833\n",
      "4.0      691\n",
      "5.0     3432\n",
      "Name: count, dtype: int64\n",
      "Test Set: (25761, 4682)\n",
      "tgt_bin\n",
      "0.0    14463\n",
      "0.1     1803\n",
      "0.5     1484\n",
      "1.0     1974\n",
      "2.0     1097\n",
      "3.0      886\n",
      "4.0      700\n",
      "5.0     3354\n",
      "Name: count, dtype: int64\n",
      "Datasets exported successfully.\n",
      "Done with 70219026615\n",
      "70316025624\n",
      "\\\\CXA01\\Users\\jhugh\\Documents\\Py_S4\\NCEI_parquet_files\\prec_s4data_DAY_SUMMARY_70316025624_stations_1_RowMn_180_ColDy_30.parquet\n",
      "(128807, 4682)\n",
      "All records at start (128806, 4682)\n",
      "Full rain and no rain (128806, 4682)\n",
      "first missing values dropped (128806, 4682)\n",
      "[-inf, 0.1, 0.5, 1, 2, 3, 4, 5, inf] [0, 0.1, 0.5, 1, 2, 3, 4, 5]\n",
      "Analysis Set: (128806, 4682)\n",
      "tgt_bin\n",
      "0.0    40044\n",
      "0.1    10749\n",
      "0.5    10001\n",
      "1.0    13165\n",
      "2.0     8900\n",
      "3.0     7054\n",
      "4.0     5750\n",
      "5.0    33143\n",
      "Name: count, dtype: int64\n",
      "Training Set: (77284, 4682)\n",
      "tgt_bin\n",
      "0.0    24002\n",
      "0.1     6526\n",
      "0.5     6029\n",
      "1.0     7806\n",
      "2.0     5384\n",
      "3.0     4232\n",
      "4.0     3478\n",
      "5.0    19827\n",
      "Name: count, dtype: int64\n",
      "Validation Set: (25761, 4682)\n",
      "tgt_bin\n",
      "0.0    8040\n",
      "0.1    2092\n",
      "0.5    1974\n",
      "1.0    2650\n",
      "2.0    1745\n",
      "3.0    1436\n",
      "4.0    1105\n",
      "5.0    6719\n",
      "Name: count, dtype: int64\n",
      "Test Set: (25761, 4682)\n",
      "tgt_bin\n",
      "0.0    8002\n",
      "0.1    2131\n",
      "0.5    1998\n",
      "1.0    2709\n",
      "2.0    1771\n",
      "3.0    1386\n",
      "4.0    1167\n",
      "5.0    6597\n",
      "Name: count, dtype: int64\n",
      "Datasets exported successfully.\n",
      "Done with 70316025624\n"
     ]
    }
   ],
   "source": [
    "#lst_stx = [\"70200026617\",\"70219026615\",\"70316025624\",]  #not run all the way to train\n",
    "\n",
    "lst_stx = [\"72202012839\",\"72206013889\",\"72208013880\",\"72214093805\",\"72217003813\",\"72218003820\",\"72223013894\",\"72241012917\",\"72250012919\",\"72251012924\",\"72255012912\",\"72312003870\",\"72417013729\",\"72438093819\",\"72515004725\",\"72519014771\",\"72605014745\",\"72635094860\",\"72638094814\",\"72639094849\",\"72654014936\",\"72659014929\",\"72712014607\",\"72734014847\",\"72792024227\"]\n",
    "\n",
    "def process_w_yield(my_list):\n",
    "    for value in my_list:\n",
    "        yield stid_loop(value)\n",
    "\n",
    "for result in process_w_yield(lst_stx):\n",
    "    print(f\"Done with {result}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s4_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
