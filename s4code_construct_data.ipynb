{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_make_matrix_pr(stid,ipd,nhours,ndays,nyears_row,nyears_col):\n",
    "\n",
    "    #Create a matrix for precipitation data that feeds\n",
    "    #The S4 model\n",
    "    # 1. Read in all data files\n",
    "    # 2. Find the common starting date that will be used to align the matrices\n",
    "    # 3. Keep only the values that fit the common starting date\n",
    "    # 4. Combine the data into one large matrix for the StationID \n",
    "\n",
    "    # 1. Read in data files and get start date v_date... for each\n",
    "    # Check that data exist for all metrics\n",
    "\n",
    "    #Air temperature (one variable recorded once an hour a day)\n",
    "    df_at = pd.read_csv(ipd + stid + \"_model_data_airtemp.csv\")\n",
    "    v_date_at = datetime.strptime(df_at.iloc[0,2], \"%Y-%m-%d\").date()\n",
    "    #Relative Hunidity (3 variables recorded once a day)\n",
    "    df_rh = pd.read_csv(ipd + stid + \"_model_data_relhum.csv\")\n",
    "    v_date_rh = datetime.strptime(df_rh.iloc[0,4], \"%Y-%m-%d\").date()\n",
    "    #Precipitation (one variable recorded once an hour a day)\n",
    "    df_pr = pd.read_csv(ipd + stid + \"_model_data_precip.csv\")\n",
    "    v_date_pr = datetime.strptime(df_pr.iloc[0,2], \"%Y-%m-%d\").date()\n",
    "\n",
    "\n",
    "    # 2. Find the offsets needed to all datasets start at same starting date\n",
    "    v_date_minimum = min(v_date_at,v_date_rh,v_date_pr)\n",
    "    v_date_minimum\n",
    "\n",
    "    #Identify offsets to use so all datadata vectors are appropriately aligned \n",
    "    offset_at = -(v_date_minimum - v_date_at).days\n",
    "    offset_rh = -(v_date_minimum - v_date_rh).days\n",
    "    offset_pr = -(v_date_minimum - v_date_pr).days\n",
    "\n",
    "    # 3. Get vectors of data for each measure\n",
    "    #These vectors all start on the same date\n",
    "    vc_at1 = df_at.iloc[offset_at:(len(df_at.iloc[:,1])-offset_at),1]\n",
    "    vc_rh1 = df_rh.iloc[offset_rh:(len(df_rh.iloc[:,1])-offset_rh),1]\n",
    "    vc_rh2 = df_rh.iloc[offset_rh:(len(df_rh.iloc[:,2])-offset_rh),2]\n",
    "    vc_rh3 = df_rh.iloc[offset_rh:(len(df_rh.iloc[:,3])-offset_rh),3]\n",
    "    vc_pr1 = df_pr.iloc[offset_pr:(len(df_pr.iloc[:,1])-offset_pr),1]\n",
    "\n",
    "\n",
    "    # 4. Interleave vectors so we get a complete vector of all metrics\n",
    "    #Start with the once a day vectors and loop over the number of days as defined by once a day vector\n",
    "    # These set how many years of data are needed : nyears_row + nyears_col\n",
    "    nyears_data_limit = nyears_row + nyears_col + 1\n",
    "    nrows = nhours*ndays*nyears_row\n",
    "    ncols = nhours*ndays*nyears_col \n",
    "    \n",
    "    # set the number of metrics created each day\n",
    "    # Metrics measured at hourly intervals are put into day metrics\n",
    "    # And ordered from most recent hour to most distant hour from left to right\n",
    "    # Day metrics are also ordered most recent day to most distant day left to right\n",
    "    ndaily_metrics = 2*nhours + 3*1\n",
    "\n",
    "    #Set the total number of days of data needed for the model\n",
    "    num_days = nyears_data_limit*ndays\n",
    "    #print(f\"Number of days: {num_days}\")\n",
    "\n",
    "    is_first = 1\n",
    "\n",
    "    #Construct the vector that holds all data values (valid and missing)\n",
    "    for i_day in range(num_days):\n",
    "        i_day_hour_end = (i_day + 1)*nhours - 1\n",
    "        i_day_hour_start = i_day_hour_end - (nhours -1)\n",
    "        va = vc_pr1[(i_day_hour_start + offset_pr):(i_day_hour_end + offset_pr)]\n",
    "        vb = vc_rh1[(i_day + offset_rh):(i_day + offset_rh)]\n",
    "        vc = vc_rh2[(i_day + offset_rh):(i_day + offset_rh)]\n",
    "        vd = vc_rh3[(i_day + offset_rh):(i_day + offset_rh)]\n",
    "        ve = vc_at1[(i_day_hour_start + offset_pr):(i_day_hour_end + offset_pr)]\n",
    "\n",
    "        #vmt_full set = np.concatenate((vmt_one_day,va,vb,vc,vd,ve)) \n",
    "        #print(i_day)\n",
    "        if is_first == 0:\n",
    "            vmt_full_set = np.concatenate((vmt_full_set,va,vb,vc,vd,ve))\n",
    "            #print(len(vmt_full_set))\n",
    "        else:\n",
    "            is_first = 0\n",
    "            vmt_full_set = np.concatenate((va,vb,vc,vd,ve))\n",
    "            #print(len(vmt_full_set))\n",
    "\n",
    "    #print(f\"Length of data vector: {len(vmt_full_set)}\")\n",
    "\n",
    "\n",
    "    #Organize the data vector into an array which will be returned to the function call\n",
    "    #Create an empty matrix in which data are organized\n",
    "    nrows = nyears_row*ndays\n",
    "    ncols = nyears_col*ndays*ndaily_metrics\n",
    "    nrow_days =  nyears_row*ndays\n",
    "    ncol_days =  nyears_col*ndays\n",
    "    data_limit = len(vmt_full_set)\n",
    "    #print(f\"Number of columns in matrix: {ncols}\")\n",
    "    #print(f\"Limit of data: {data_limit}\")\n",
    "\n",
    "    #Make template matrix to house data\n",
    "    template_matrix = np.zeros((nrows, ncols), dtype=np.float16)\n",
    "\n",
    "    # Fill the matrix\n",
    "    for i in range(nrow_days):\n",
    "        vc_start = i*(ndaily_metrics)\n",
    "        vc_end = ncols + vc_start\n",
    "        \n",
    "        if vc_end < data_limit:\n",
    "            template_matrix[i] = vmt_full_set[(vc_start):(vc_end)]\n",
    "\n",
    "    #   print(vc_start)\n",
    "    #   print(vc_end)\n",
    "    #   print(len(vmt_full_set[(vc_start):(vc_end)]))\n",
    "        \n",
    "    return template_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data for a station and organize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "# Path to data files\n",
    "ipd = \"C:\\\\Users\\\\jhugh\\\\Documents\\\\Py_S4\\\\Py_S4_v02_JHH\\\\NCEI_data\\\\\"\n",
    "\n",
    "#Organize the data based on the nature of the data so they all conform to the same size matrices\n",
    "#The metrics included are\n",
    "#  AT - air temperature in Celcius\n",
    "#  RH - relative humidity in Percent\n",
    "#  PR - precipitation in MM\n",
    "\n",
    "#Identify which stations are in folder and have all three metrics\n",
    "#List only files\n",
    "files = [entry.name for entry in os.scandir(ipd) if entry.is_file()]\n",
    "\n",
    "# Unique_stations\n",
    "df_files = pd.DataFrame(files,columns=['File_name'])\n",
    "df_files['stid'] = df_files['File_name'].str[:11]\n",
    "df_files['mttype'] = df_files['File_name'].str[23:29]\n",
    "df_files['has_airtem'] = np.where(df_files['mttype'] == 'airtem',1,0)\n",
    "df_files['has_precip'] = np.where(df_files['mttype'] == 'precip',1,0)\n",
    "df_files['has_relhum'] = np.where(df_files['mttype'] == 'relhum',1,0)\n",
    "df_files['has_all3'] = df_files[['has_airtem','has_precip','has_relhum']].sum(axis=1) \n",
    "\n",
    "#All the available stations which have all three metrics needed for model\n",
    "vc_stid = df_files[['stid','has_all3']].drop_duplicates()\n",
    "#vc_stid <- vc_stid[1]\n",
    "\n",
    "#print(df_files)\n",
    "print(vc_stid)\n",
    "\n",
    "#Put them all in appropriate matrices and create list of metrics\n",
    "#Data files are of two types\n",
    "#    -  24 hour * 365 days * n years  (AT, PR)\n",
    "#    -  365 days * n years (RH)\n",
    "\n",
    "#Parameters governing data matrix\n",
    "nhours = 24\n",
    "ndays = 365\n",
    "nyears_row = 6\n",
    "nyears_col = 3\n",
    "\n",
    "def get_matrix(in_stid):\n",
    "    return fn_make_matrix_pr(in_stid,ipd,nhours,ndays,nyears_row,nyears_col)\n",
    "\n",
    "#lst_matrix = list(map(get_matrix,vc_stid.iloc[1,0]))\n",
    "\n",
    "#print(lst_matrix)\n",
    "#print(jj.shape)\n",
    "\n",
    "jj <- fn_make_matrix_pr(\"72381523161\",ipd,nhours,ndays,nyears_row,nyears_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_make_matrix(vc_measure,offset_measure,nhours,ndays,nyears_col,nyears_row):\n",
    "    #the offset from the start of the vector to align the data points\n",
    "    vector = vc_measure\n",
    "\n",
    "    nrows = nhours*ndays*nyears_row   #the number of values put in each row of matrix days*hours*years\n",
    "    ncols = nhours*ndays*nyears_col   #the number of values put in each row of matrix days*hours*years\n",
    "    \n",
    "    # Create an empty matrix to store the results\n",
    "    matrix = np.zeros((nrows, ncols))\n",
    "\n",
    "    # Fill the matrix\n",
    "    for i in range(nrows):\n",
    "        matrix[i] = vector[(offset_measure + i):(offset_measure + i + ncols)]\n",
    "     \n",
    "    return matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_first_non_nan(vector):\n",
    "    return next((i for i, x in enumerate(vector) if not math.isnan(x)), None)\n",
    "\n",
    "first_number_at1 = find_first_non_nan(vc_at1)\n",
    "first_number_rh1 = find_first_non_nan(vc_rh1)\n",
    "first_number_rh2 = find_first_non_nan(vc_rh2)\n",
    "first_number_rh3 = find_first_non_nan(vc_rh3)\n",
    "first_number_pr1 = find_first_non_nan(vc_pr1)\n",
    "\n",
    "#Number of missing data points\n",
    "print(f\"AT1 Number of missing cells: {np.count_nonzero(np.isnan(vc_at1))}\")\n",
    "print(f\"AT1 Vector Length: {len(vc_at1)}\")\n",
    "\n",
    "print(f\"RH1 Number of missing cells: {np.count_nonzero(np.isnan(vc_rh1))}\")\n",
    "print(f\"RH1 Vector Length: {len(vc_rh1)}\")\n",
    "\n",
    "print(f\"PR1 Number of missing cells: {np.count_nonzero(np.isnan(vc_pr1))}\")\n",
    "print(f\"PR1 Vector Length: {len(vc_pr1)}\")\n",
    "\n",
    "#Once a day metrics\n",
    "v_data_first_once_day = max(\n",
    "                   first_number_rh1,\n",
    "                   first_number_rh2,\n",
    "                   first_number_rh3)\n",
    "print(v_data_first_once_day)\n",
    "\n",
    "#24 times a day metrics\n",
    "v_data_first_24h_day = max(first_number_at1,\n",
    "                   first_number_pr1)\n",
    "print(v_data_first_24h_day)\n",
    "\n",
    "\n",
    "#These vectors all start with a numeric value so missing values can be filled\n",
    "\n",
    "\n",
    "#print(first_number_at1)\n",
    "#print(first_number_rh1)\n",
    "#print(first_number_rh2)\n",
    "#print(first_number_rh3)\n",
    "#print(first_number_pr1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s4_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
